{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from adahessian import Adahessian, get_params_grad\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from fastai.vision import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the data-loader class for the GEO-dataset (can be changed to wrok for both the training and validation sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88807, 943)\n",
      "(88807, 4760)\n"
     ]
    }
   ],
   "source": [
    "def GEOdataLoader(path, bSize= 16):\n",
    "    \"\"\"\n",
    "    Specify the path to the parent folder containing the .npy files\n",
    "    \"\"\"\n",
    "    X_tr= np.load(os.path.join(path, \"X_tr.npy\"))\n",
    "    Y_tr= np.load(os.path.join(path, \"Y_tr.npy\"))\n",
    "    X_va= np.load(os.path.join(path, \"X_va.npy\"))\n",
    "    Y_va= np.load(os.path.join(path, \"Y_va.npy\"))\n",
    "    print(X_tr.shape)\n",
    "    print(Y_tr.shape)\n",
    "    print(X_va.shape)\n",
    "    print(Y_va.shape)\n",
    "    test_data= torch.utils.data.TensorDataset(torch.from_numpy(X_tr).float(), torch.from_numpy(Y_tr).float())\n",
    "    val_data= torch.utils.data.TensorDataset(torch.from_numpy(X_va).float(), torch.from_numpy(Y_va).float())\n",
    "    trainLoader= torch.utils.data.DataLoader(test_data, batch_size=bSize, shuffle=True) \n",
    "    valLoader= torch.utils.data.DataLoader(val_data, batch_size=bSize, shuffle=True) \n",
    "    return (trainLoader, valLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardised class for all network architectures\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, size1, size2, drop):\n",
    "        super(Network, self).__init__()\n",
    "        self.net= nn.Sequential(\n",
    "            nn.Linear(943, size1),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(p= drop),\n",
    "            nn.Linear(size1, size2),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(p= drop),\n",
    "            nn.Linear(size2, 4760) \n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.net(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network class for the wide-medium architecture\n",
    "class wideM(nn.Module):\n",
    "    def __init__(self, size1, drop):\n",
    "        super(wideM, self).__init__()\n",
    "        self.net= nn.Sequential(\n",
    "            nn.Linear(943, size1),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(p= drop),\n",
    "            nn.Linear(size1, 4760) \n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.net(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining all the criterions to be used in the following experiments:\n",
    "\n",
    "\n",
    "def tiltedLC(x, y, tau, h):\n",
    "    e= y-x # errors\n",
    "    ind= (torch.sign(e)+1)/2 # the division in the log-cosh is only about the origin\n",
    "    quantFactor= (1-tau)*(1-ind) + tau*ind\n",
    "    loss= quantFactor*torch.log(torch.cosh(e/h))\n",
    "    loss= torch.mean(loss)*h\n",
    "    return loss\n",
    "\n",
    "def check_loss(x, y, tau): # the x,*args way to pass arguments to this function is an idiom for the scipy.optimize() library y = args[0][0]\n",
    "    e = y-x\n",
    "    # below I(e<0)\n",
    "    ind = (np.sign(-e)+1)/2\n",
    "    loss = np.mean(e*(tau-ind))\n",
    "    return loss\n",
    "\n",
    "\n",
    "class TiltedLC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TiltedLC, self).__init__()\n",
    "    def forward(self, x, y, tau, h):\n",
    "        return tiltedLC(x, y, tau, h)\n",
    "\n",
    "class CheckLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CheckLoss, self).__init__()\n",
    "    def forward(self, x, y, tau):\n",
    "        return check_loss(x, y, tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global initialisations:\n",
    "h= 0.4 # smoothing parameter for the log-cosh \n",
    "device= ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "trainLoader, valLoader= GEOdataLoader(\"/home/aryamanj/Downloads/LGdata\")\n",
    "criterion= TiltedLC()\n",
    "criterion1= CheckLoss()\n",
    "criterion2= nn.L1Loss()\n",
    "N_EPOCHS= 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiations and training loops start for constant LR's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating objects for all constantLR tests:\n",
    "size1,size2= 300,300\n",
    "model_CLR_S= Network(size1, size2, 0.1).to(device)\n",
    "optimizer_CLR_S= optim.SGD(model_CLR_S.parameters(), lr= 0.1)\n",
    "lossList_CLR_S= []\n",
    "\n",
    "size1, size2= 1000, 1000\n",
    "model_CLR_M= Network(size1, size2, 0.1).to(device)\n",
    "optimizer_CLR_M= optim.SGD(model_CLR_M.parameters(), lr= 0.1)\n",
    "lossList_CLR_M= []\n",
    "\n",
    "size1, size2= 3000, 3000\n",
    "model_CLR_L= Network(size1, size2, 0.1).to(device)\n",
    "optimizer_CLR_L= optim.SGD(model_CLR_L.parameters(), lr= 0.1)\n",
    "lossList_CLR_L= []\n",
    "\n",
    "size1= 2000\n",
    "model_CLR_WM= wideM(size1, 0.1).to(device)\n",
    "optimizer_CLR_WM= optim.SGD(model_CLR_WM.parameters(), lr= 0.1)\n",
    "lossList_CLR_WM= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.13386895350536082\n",
      "Epoch: 1 Loss: 0.1336473765160732\n",
      "Epoch: 2 Loss: 0.13338832228092692\n",
      "Epoch: 3 Loss: 0.13315786012597522\n",
      "Epoch: 4 Loss: 0.13294416327660974\n",
      "Epoch: 5 Loss: 0.13271940638865035\n",
      "Epoch: 6 Loss: 0.13252280629437113\n",
      "Epoch: 7 Loss: 0.132285691769547\n",
      "Epoch: 8 Loss: 0.13209931786490697\n",
      "Epoch: 9 Loss: 0.13190648027893187\n",
      "Epoch: 10 Loss: 0.1317071426512851\n",
      "Epoch: 11 Loss: 0.1315119389885504\n",
      "Epoch: 12 Loss: 0.13133860254095517\n",
      "Epoch: 13 Loss: 0.1311479967038517\n",
      "Epoch: 14 Loss: 0.13099940435115634\n",
      "Epoch: 15 Loss: 0.13081443068052795\n",
      "Epoch: 16 Loss: 0.13064387180562106\n",
      "Epoch: 17 Loss: 0.13048930255314656\n",
      "Epoch: 18 Loss: 0.1303272681222523\n",
      "Epoch: 19 Loss: 0.13016274789124432\n",
      "Epoch: 20 Loss: 0.13001288522865037\n",
      "Epoch: 21 Loss: 0.12986963122030826\n",
      "Epoch: 22 Loss: 0.12971601760848064\n",
      "Epoch: 23 Loss: 0.12958107533819546\n",
      "Epoch: 24 Loss: 0.12943074332526344\n",
      "Epoch: 25 Loss: 0.12929609544216847\n",
      "Epoch: 26 Loss: 0.1291717951525076\n",
      "Epoch: 27 Loss: 0.1290396464190984\n",
      "Epoch: 28 Loss: 0.12892029407464198\n",
      "Epoch: 29 Loss: 0.12878950116704593\n",
      "Epoch: 30 Loss: 0.12865826072397157\n",
      "Epoch: 31 Loss: 0.1285403784659076\n",
      "Epoch: 32 Loss: 0.12841507644830277\n",
      "Epoch: 33 Loss: 0.12828562130849225\n",
      "Epoch: 34 Loss: 0.12818571276207097\n",
      "Epoch: 35 Loss: 0.1280845994375736\n",
      "Epoch: 36 Loss: 0.12796142431309235\n",
      "Epoch: 37 Loss: 0.12784762141769415\n",
      "Epoch: 38 Loss: 0.12774189869507308\n",
      "Epoch: 39 Loss: 0.1276477078619121\n",
      "Epoch: 40 Loss: 0.1275422168092628\n",
      "Epoch: 41 Loss: 0.1274579522902741\n",
      "Epoch: 42 Loss: 0.12736099419256006\n",
      "Epoch: 43 Loss: 0.12724866811739735\n",
      "Epoch: 44 Loss: 0.12715884541293734\n",
      "Epoch: 45 Loss: 0.12706557028982107\n",
      "Epoch: 46 Loss: 0.12695406854201\n",
      "Epoch: 47 Loss: 0.12688325016675953\n",
      "Epoch: 48 Loss: 0.12677709694526965\n",
      "Epoch: 49 Loss: 0.1267034133298615\n",
      "Epoch: 50 Loss: 0.12660725659571173\n",
      "Epoch: 51 Loss: 0.12654760225981856\n",
      "Epoch: 52 Loss: 0.12644886105468822\n",
      "Epoch: 53 Loss: 0.12636660214448445\n",
      "Epoch: 54 Loss: 0.12628847005195049\n",
      "Epoch: 55 Loss: 0.1262053702999033\n",
      "Epoch: 56 Loss: 0.12612022165564069\n",
      "Epoch: 57 Loss: 0.12604008630854957\n",
      "Epoch: 58 Loss: 0.12596162222023805\n",
      "Epoch: 59 Loss: 0.1259063170597674\n",
      "Epoch: 60 Loss: 0.1258336877505042\n",
      "Epoch: 61 Loss: 0.12574110096654384\n",
      "Epoch: 62 Loss: 0.12567060053015675\n",
      "Epoch: 63 Loss: 0.12561325134833295\n",
      "Epoch: 64 Loss: 0.1255379473490879\n",
      "Epoch: 65 Loss: 0.1254603491454828\n",
      "Epoch: 66 Loss: 0.12540896231796136\n",
      "Epoch: 67 Loss: 0.12535125148165358\n",
      "Epoch: 68 Loss: 0.12528894031784424\n",
      "Epoch: 69 Loss: 0.12521795415621667\n",
      "Epoch: 70 Loss: 0.12515213213682475\n",
      "Epoch: 71 Loss: 0.12509078083892403\n",
      "Epoch: 72 Loss: 0.1250319187999996\n",
      "Epoch: 73 Loss: 0.1249564815627371\n",
      "Epoch: 74 Loss: 0.12491157998578696\n",
      "Epoch: 75 Loss: 0.12485291488325032\n",
      "Epoch: 76 Loss: 0.12477348451731635\n",
      "Epoch: 77 Loss: 0.12473652243662516\n",
      "Epoch: 78 Loss: 0.12466312464111196\n",
      "Epoch: 79 Loss: 0.12461384633863709\n",
      "Epoch: 80 Loss: 0.124551809892002\n",
      "Epoch: 81 Loss: 0.12449328698401751\n",
      "Epoch: 82 Loss: 0.12443680445866807\n",
      "Epoch: 83 Loss: 0.12438827594821034\n",
      "Epoch: 84 Loss: 0.1243404454797093\n",
      "Epoch: 85 Loss: 0.12429856520110563\n",
      "Epoch: 86 Loss: 0.1242277859041571\n",
      "Epoch: 87 Loss: 0.12418888994212238\n",
      "Epoch: 88 Loss: 0.12412925309918846\n",
      "Epoch: 89 Loss: 0.12408628999526555\n",
      "Epoch: 90 Loss: 0.12404020144995903\n",
      "Epoch: 91 Loss: 0.12399606416621008\n",
      "Epoch: 92 Loss: 0.12393397954514898\n",
      "Epoch: 93 Loss: 0.12389642811158695\n",
      "Epoch: 94 Loss: 0.12386900447731812\n",
      "Epoch: 95 Loss: 0.12380590498544057\n",
      "Epoch: 96 Loss: 0.12376444034078404\n",
      "Epoch: 97 Loss: 0.12372604888412378\n",
      "Epoch: 98 Loss: 0.12366758317018367\n",
      "Epoch: 99 Loss: 0.1236159127884095\n",
      "Epoch: 100 Loss: 0.12358118991253934\n",
      "Epoch: 101 Loss: 0.12353399250313433\n",
      "Epoch: 102 Loss: 0.12349228695547704\n",
      "Epoch: 103 Loss: 0.12344692482231673\n",
      "Epoch: 104 Loss: 0.1234134847200619\n",
      "Epoch: 105 Loss: 0.12337608334771621\n",
      "Epoch: 106 Loss: 0.12331415203332342\n",
      "Epoch: 107 Loss: 0.12327466546211387\n",
      "Epoch: 108 Loss: 0.12324609871604081\n",
      "Epoch: 109 Loss: 0.12319865056527153\n",
      "Epoch: 110 Loss: 0.12316919753297431\n",
      "Epoch: 111 Loss: 0.12312057258910562\n",
      "Epoch: 112 Loss: 0.12307250419058814\n",
      "Epoch: 113 Loss: 0.12305240517365054\n",
      "Epoch: 114 Loss: 0.12300834844636822\n",
      "Epoch: 115 Loss: 0.12299130023615658\n",
      "Epoch: 116 Loss: 0.12293166130475151\n",
      "Epoch: 117 Loss: 0.12289702761304247\n",
      "Epoch: 118 Loss: 0.12287251639475674\n",
      "Epoch: 119 Loss: 0.1228352589484078\n",
      "Epoch: 120 Loss: 0.12277930635248212\n",
      "Epoch: 121 Loss: 0.12275920436390239\n",
      "Epoch: 122 Loss: 0.1227196941148524\n",
      "Epoch: 123 Loss: 0.12268330697443351\n",
      "Epoch: 124 Loss: 0.12265620184812304\n",
      "Epoch: 125 Loss: 0.12262933085741513\n",
      "Epoch: 126 Loss: 0.12258335005091324\n",
      "Epoch: 127 Loss: 0.12255564973448187\n",
      "Epoch: 128 Loss: 0.1225151404580046\n",
      "Epoch: 129 Loss: 0.12250069860477014\n",
      "Epoch: 130 Loss: 0.12245359875622837\n",
      "Epoch: 131 Loss: 0.12241944332844157\n",
      "Epoch: 132 Loss: 0.1224168238849645\n",
      "Epoch: 133 Loss: 0.12237351348278351\n",
      "Epoch: 134 Loss: 0.12234767201306089\n",
      "Epoch: 135 Loss: 0.12230693785548533\n",
      "Epoch: 136 Loss: 0.12228639534847562\n",
      "Epoch: 137 Loss: 0.12223935367781458\n",
      "Epoch: 138 Loss: 0.12221868876572493\n",
      "Epoch: 139 Loss: 0.12218255681473808\n",
      "Epoch: 140 Loss: 0.12215392817824494\n",
      "Epoch: 141 Loss: 0.12214132911760385\n",
      "Epoch: 142 Loss: 0.12211973860476\n",
      "Epoch: 143 Loss: 0.1220773966706802\n",
      "Epoch: 144 Loss: 0.12204055483009209\n",
      "Epoch: 145 Loss: 0.12200808158931893\n",
      "Epoch: 146 Loss: 0.1219959259006359\n",
      "Epoch: 147 Loss: 0.12194850982991108\n",
      "Epoch: 148 Loss: 0.12193881787280525\n",
      "Epoch: 149 Loss: 0.1219045766083169\n",
      "Epoch: 150 Loss: 0.12188784023530418\n",
      "Epoch: 151 Loss: 0.12184353298946364\n",
      "Epoch: 152 Loss: 0.12183178104681704\n",
      "Epoch: 153 Loss: 0.12181574956637592\n",
      "Epoch: 154 Loss: 0.1217608582842481\n",
      "Epoch: 155 Loss: 0.12174979193729014\n",
      "Epoch: 156 Loss: 0.12172335283329112\n",
      "Epoch: 157 Loss: 0.12170438008616796\n",
      "Epoch: 158 Loss: 0.1216563155491746\n",
      "Epoch: 159 Loss: 0.12164335209326665\n",
      "Epoch: 160 Loss: 0.12160388445520247\n",
      "Epoch: 161 Loss: 0.12160966410317564\n",
      "Epoch: 162 Loss: 0.12157947216011816\n",
      "Epoch: 163 Loss: 0.12155145434238228\n",
      "Epoch: 164 Loss: 0.12153139365345396\n",
      "Epoch: 165 Loss: 0.12149565981526865\n",
      "Epoch: 166 Loss: 0.12148673693900966\n",
      "Epoch: 167 Loss: 0.12145613987683992\n",
      "Epoch: 168 Loss: 0.12143458031658824\n",
      "Epoch: 169 Loss: 0.12141598942145165\n",
      "Epoch: 170 Loss: 0.12139006917905301\n",
      "Epoch: 171 Loss: 0.12137399213074822\n",
      "Epoch: 172 Loss: 0.12135472267710438\n",
      "Epoch: 173 Loss: 0.1213247511241269\n",
      "Epoch: 174 Loss: 0.12130073028450677\n",
      "Epoch: 175 Loss: 0.12129530291034604\n",
      "Epoch: 176 Loss: 0.1212669713065294\n",
      "Epoch: 177 Loss: 0.1212385103919875\n",
      "Epoch: 178 Loss: 0.12121590746053595\n",
      "Epoch: 179 Loss: 0.12120733301938279\n",
      "Epoch: 180 Loss: 0.12116806531216599\n",
      "Epoch: 181 Loss: 0.12114917429996555\n",
      "Epoch: 182 Loss: 0.12115028298018193\n",
      "Epoch: 183 Loss: 0.12112304199452745\n",
      "Epoch: 184 Loss: 0.12108883486499959\n",
      "Epoch: 185 Loss: 0.12109308705804928\n",
      "Epoch: 186 Loss: 0.1210736382066119\n",
      "Epoch: 187 Loss: 0.12103888246819171\n",
      "Epoch: 188 Loss: 0.12102321112151619\n",
      "Epoch: 189 Loss: 0.12100082579909563\n",
      "Epoch: 190 Loss: 0.1209772897693321\n",
      "Epoch: 191 Loss: 0.12095811981066909\n",
      "Epoch: 192 Loss: 0.12093995894999832\n",
      "Epoch: 193 Loss: 0.12091219657939783\n",
      "Epoch: 194 Loss: 0.12090872014198104\n",
      "Epoch: 195 Loss: 0.12088939146085208\n",
      "Epoch: 196 Loss: 0.12086642701584806\n",
      "Epoch: 197 Loss: 0.12083806329708961\n",
      "Epoch: 198 Loss: 0.12083248097474372\n",
      "Epoch: 199 Loss: 0.12083153427452509\n",
      "Epoch: 200 Loss: 0.12079554446037558\n",
      "Epoch: 201 Loss: 0.1207813120260365\n",
      "Epoch: 202 Loss: 0.12075210753254495\n",
      "Epoch: 203 Loss: 0.12073737001031438\n",
      "Epoch: 204 Loss: 0.12071228157940307\n",
      "Epoch: 205 Loss: 0.1207150386780946\n",
      "Epoch: 206 Loss: 0.1207004577294596\n",
      "Epoch: 207 Loss: 0.1206877322787316\n",
      "Epoch: 208 Loss: 0.12066236882596548\n",
      "Epoch: 209 Loss: 0.12064408178996061\n",
      "Epoch: 210 Loss: 0.12062406456834496\n",
      "Epoch: 211 Loss: 0.12062227897382259\n",
      "Epoch: 212 Loss: 0.12060831758304236\n",
      "Epoch: 213 Loss: 0.12056954934793085\n",
      "Epoch: 214 Loss: 0.1205735861788009\n",
      "Epoch: 215 Loss: 0.12055881212577371\n",
      "Epoch: 216 Loss: 0.12052094901205075\n",
      "Epoch: 217 Loss: 0.12051563091888833\n",
      "Epoch: 218 Loss: 0.12049755601437537\n",
      "Epoch: 219 Loss: 0.12049113167038782\n",
      "Epoch: 220 Loss: 0.12046010272822022\n",
      "Epoch: 221 Loss: 0.12045773685279139\n",
      "Epoch: 222 Loss: 0.12044594947995635\n",
      "Epoch: 223 Loss: 0.12042654958276615\n",
      "Epoch: 224 Loss: 0.12040301075853156\n",
      "Epoch: 225 Loss: 0.12039849563823737\n",
      "Epoch: 226 Loss: 0.12034839484723188\n",
      "Epoch: 227 Loss: 0.12037190698171775\n",
      "Epoch: 228 Loss: 0.12033830234927544\n",
      "Epoch: 229 Loss: 0.12030919316174253\n",
      "Epoch: 230 Loss: 0.12031388217947372\n",
      "Epoch: 231 Loss: 0.12030642594282444\n",
      "Epoch: 232 Loss: 0.12029553275240625\n",
      "Epoch: 233 Loss: 0.12028076326287646\n",
      "Epoch: 234 Loss: 0.12026498759733495\n",
      "Epoch: 235 Loss: 0.12025254364325923\n",
      "Epoch: 236 Loss: 0.12022585703797908\n",
      "Epoch: 237 Loss: 0.1202196231971902\n",
      "Epoch: 238 Loss: 0.12020071917902218\n",
      "Epoch: 239 Loss: 0.12020450376257384\n",
      "Epoch: 240 Loss: 0.12017612951153185\n",
      "Epoch: 241 Loss: 0.12017844174573324\n",
      "Epoch: 242 Loss: 0.12014210337433723\n",
      "Epoch: 243 Loss: 0.12012477094413912\n",
      "Epoch: 244 Loss: 0.12014092271031829\n",
      "Epoch: 245 Loss: 0.12012407798242879\n",
      "Epoch: 246 Loss: 0.12007383241983732\n",
      "Epoch: 247 Loss: 0.12010518583874942\n",
      "Epoch: 248 Loss: 0.12007088761107597\n",
      "Epoch: 249 Loss: 0.12006982360884126\n",
      "Epoch: 250 Loss: 0.12005957569024345\n",
      "Epoch: 251 Loss: 0.12003668895623122\n",
      "Epoch: 252 Loss: 0.12003178067050352\n",
      "Epoch: 253 Loss: 0.12003097850430057\n",
      "Epoch: 254 Loss: 0.12001411588213247\n",
      "Epoch: 255 Loss: 0.1199966806927107\n",
      "Epoch: 256 Loss: 0.11998142174258358\n",
      "Epoch: 257 Loss: 0.11996233846352694\n",
      "Epoch: 258 Loss: 0.11996528720494928\n",
      "Epoch: 259 Loss: 0.11993699921743008\n",
      "Epoch: 260 Loss: 0.11994037779601409\n",
      "Epoch: 261 Loss: 0.11993076220635272\n",
      "Epoch: 262 Loss: 0.11991698582646783\n",
      "Epoch: 263 Loss: 0.11989984898776798\n",
      "Epoch: 264 Loss: 0.11988698181897063\n",
      "Epoch: 265 Loss: 0.11987573830362183\n",
      "Epoch: 266 Loss: 0.11986659993767373\n",
      "Epoch: 267 Loss: 0.11985831751069213\n",
      "Epoch: 268 Loss: 0.11982860848498203\n",
      "Epoch: 269 Loss: 0.11982915115595895\n",
      "Epoch: 270 Loss: 0.11980108868037445\n",
      "Epoch: 271 Loss: 0.11980163041986103\n",
      "Epoch: 272 Loss: 0.11978794128196645\n",
      "Epoch: 273 Loss: 0.11978840127042382\n",
      "Epoch: 274 Loss: 0.11978952632956925\n",
      "Epoch: 275 Loss: 0.11976674956913318\n",
      "Epoch: 276 Loss: 0.11975658499610421\n",
      "Epoch: 277 Loss: 0.11971657100002565\n",
      "Epoch: 278 Loss: 0.11972680420134438\n",
      "Epoch: 279 Loss: 0.11972062643520082\n",
      "Epoch: 280 Loss: 0.11969671917566199\n",
      "Epoch: 281 Loss: 0.11969748229980254\n",
      "Epoch: 282 Loss: 0.11969430710824411\n",
      "Epoch: 283 Loss: 0.11968343626151448\n",
      "Epoch: 284 Loss: 0.11965410687583693\n",
      "Epoch: 285 Loss: 0.1196632699517812\n",
      "Epoch: 286 Loss: 0.11967558883113101\n",
      "Epoch: 287 Loss: 0.11966490977074816\n",
      "Epoch: 288 Loss: 0.11962027383498126\n",
      "Epoch: 289 Loss: 0.11963124344921654\n",
      "Epoch: 290 Loss: 0.11959496232018431\n",
      "Epoch: 291 Loss: 0.1195953628570083\n",
      "Epoch: 292 Loss: 0.11958963228676517\n",
      "Epoch: 293 Loss: 0.1195836989187245\n",
      "Epoch: 294 Loss: 0.11959069724733433\n",
      "Epoch: 295 Loss: 0.11956562075936414\n",
      "Epoch: 296 Loss: 0.11957720767118075\n",
      "Epoch: 297 Loss: 0.11954125117593103\n",
      "Epoch: 298 Loss: 0.11953692950900235\n",
      "Epoch: 299 Loss: 0.11952008858223774\n",
      "Epoch: 300 Loss: 0.11951810497085247\n",
      "Epoch: 301 Loss: 0.11950991178977341\n",
      "Epoch: 302 Loss: 0.11951374240022289\n",
      "Epoch: 303 Loss: 0.11948920126284292\n",
      "Epoch: 304 Loss: 0.11948466358276513\n",
      "Epoch: 305 Loss: 0.11947403622412334\n",
      "Epoch: 306 Loss: 0.11946251977404868\n",
      "Epoch: 307 Loss: 0.11944297945133267\n",
      "Epoch: 308 Loss: 0.11946573521833853\n",
      "Epoch: 309 Loss: 0.11944512382936229\n",
      "Epoch: 310 Loss: 0.11942107687311503\n",
      "Epoch: 311 Loss: 0.11940393915527082\n",
      "Epoch: 312 Loss: 0.11942224239571807\n",
      "Epoch: 313 Loss: 0.11939115963380974\n",
      "Epoch: 314 Loss: 0.11940035169601569\n",
      "Epoch: 315 Loss: 0.11940960674924993\n",
      "Epoch: 316 Loss: 0.11938498496144898\n",
      "Epoch: 317 Loss: 0.1193668465853983\n",
      "Epoch: 318 Loss: 0.11936382083227304\n",
      "Epoch: 319 Loss: 0.11934830470490812\n",
      "Epoch: 320 Loss: 0.11932308191523812\n",
      "Epoch: 321 Loss: 0.11931997130790545\n",
      "Epoch: 322 Loss: 0.11932911192607373\n",
      "Epoch: 323 Loss: 0.11931610835970811\n",
      "Epoch: 324 Loss: 0.11931512357441934\n",
      "Epoch: 325 Loss: 0.11929414229116189\n",
      "Epoch: 326 Loss: 0.11927470314252529\n",
      "Epoch: 327 Loss: 0.11929267430532045\n",
      "Epoch: 328 Loss: 0.11926561002768046\n",
      "Epoch: 329 Loss: 0.11926198039499838\n",
      "Epoch: 330 Loss: 0.11925898564278467\n",
      "Epoch: 331 Loss: 0.11924774299181597\n",
      "Epoch: 332 Loss: 0.11923428302336504\n",
      "Epoch: 333 Loss: 0.11926352229875996\n",
      "Epoch: 334 Loss: 0.11923024804071142\n",
      "Epoch: 335 Loss: 0.11923614189020687\n",
      "Epoch: 336 Loss: 0.11920710117124132\n",
      "Epoch: 337 Loss: 0.1192028300007535\n",
      "Epoch: 338 Loss: 0.11920045076233055\n",
      "Epoch: 339 Loss: 0.11917770396125486\n",
      "Epoch: 340 Loss: 0.1191838189390431\n",
      "Epoch: 341 Loss: 0.11918174977013495\n",
      "Epoch: 342 Loss: 0.11916645033910153\n",
      "Epoch: 343 Loss: 0.11915709427822133\n",
      "Epoch: 344 Loss: 0.11913053491130174\n",
      "Epoch: 345 Loss: 0.11915267017151382\n",
      "Epoch: 346 Loss: 0.11914993754122155\n",
      "Epoch: 347 Loss: 0.11911134161303397\n",
      "Epoch: 348 Loss: 0.11911788857953867\n",
      "Epoch: 349 Loss: 0.11913218321971605\n",
      "Epoch: 350 Loss: 0.11911420781555272\n",
      "Epoch: 351 Loss: 0.11909601779816022\n",
      "Epoch: 352 Loss: 0.11908999541507628\n",
      "Epoch: 353 Loss: 0.11907931442728689\n",
      "Epoch: 354 Loss: 0.11906367172863737\n",
      "Epoch: 355 Loss: 0.11906977441853778\n",
      "Epoch: 356 Loss: 0.11906368060061318\n",
      "Epoch: 357 Loss: 0.11905287677823821\n",
      "Epoch: 358 Loss: 0.11904013081593162\n",
      "Epoch: 359 Loss: 0.11903579100104758\n",
      "Epoch: 360 Loss: 0.11903203648154831\n",
      "Epoch: 361 Loss: 0.11900124161726848\n",
      "Epoch: 362 Loss: 0.11902241069112805\n",
      "Epoch: 363 Loss: 0.1190030771847717\n",
      "Epoch: 364 Loss: 0.11901369633717143\n",
      "Epoch: 365 Loss: 0.11900354682234167\n",
      "Epoch: 366 Loss: 0.1189919888914179\n",
      "Epoch: 367 Loss: 0.11896753576511178\n",
      "Epoch: 368 Loss: 0.11897183635405131\n",
      "Epoch: 369 Loss: 0.11897363002327965\n",
      "Epoch: 370 Loss: 0.11895008463431986\n",
      "Epoch: 371 Loss: 0.11894957677477482\n",
      "Epoch: 372 Loss: 0.11894717640870113\n",
      "Epoch: 373 Loss: 0.11893498134777324\n",
      "Epoch: 374 Loss: 0.11894562300224172\n",
      "Epoch: 375 Loss: 0.11893734574543423\n",
      "Epoch: 376 Loss: 0.1188980336863916\n",
      "Epoch: 377 Loss: 0.11891792388708393\n",
      "Epoch: 378 Loss: 0.11892006160240735\n",
      "Epoch: 379 Loss: 0.11890313632342005\n",
      "Epoch: 380 Loss: 0.1188917621631941\n",
      "Epoch: 381 Loss: 0.11889794853958348\n",
      "Epoch: 382 Loss: 0.118885130192155\n",
      "Epoch: 383 Loss: 0.11886141254056404\n",
      "Epoch: 384 Loss: 0.11888045912910165\n",
      "Epoch: 385 Loss: 0.11885356401225679\n",
      "Epoch: 386 Loss: 0.1188649034791649\n",
      "Epoch: 387 Loss: 0.11884296114483174\n",
      "Epoch: 388 Loss: 0.11885846016176584\n",
      "Epoch: 389 Loss: 0.11883431178984223\n",
      "Epoch: 390 Loss: 0.11882442300775162\n",
      "Epoch: 391 Loss: 0.11881684802621577\n",
      "Epoch: 392 Loss: 0.11883307211711759\n",
      "Epoch: 393 Loss: 0.11879104823196199\n",
      "Epoch: 394 Loss: 0.11878171023716262\n",
      "Epoch: 395 Loss: 0.11880839827054901\n",
      "Epoch: 396 Loss: 0.11879095621709028\n",
      "Epoch: 397 Loss: 0.11879577940969806\n",
      "Epoch: 398 Loss: 0.11878105683694808\n",
      "Epoch: 399 Loss: 0.11877748145888119\n",
      "Epoch: 400 Loss: 0.11875992317717046\n",
      "Epoch: 401 Loss: 0.11874777606786253\n",
      "Epoch: 402 Loss: 0.11874944633475254\n",
      "Epoch: 403 Loss: 0.11875410818356777\n",
      "Epoch: 404 Loss: 0.118725268274594\n",
      "Epoch: 405 Loss: 0.11872571465309151\n",
      "Epoch: 406 Loss: 0.11872353863928731\n",
      "Epoch: 407 Loss: 0.11872446919143619\n",
      "Epoch: 408 Loss: 0.1187290318056584\n",
      "Epoch: 409 Loss: 0.11871776002883869\n",
      "Epoch: 410 Loss: 0.1187204403319223\n",
      "Epoch: 411 Loss: 0.11869762907413035\n",
      "Epoch: 412 Loss: 0.11868737438410773\n",
      "Epoch: 413 Loss: 0.11868476709161357\n",
      "Epoch: 414 Loss: 0.11868549955442045\n",
      "Epoch: 415 Loss: 0.11866864784974876\n",
      "Epoch: 416 Loss: 0.11867207241764886\n",
      "Epoch: 417 Loss: 0.11866474334017003\n",
      "Epoch: 418 Loss: 0.11864827726445011\n",
      "Epoch: 419 Loss: 0.11865761688331766\n",
      "Epoch: 420 Loss: 0.11865507071216007\n",
      "Epoch: 421 Loss: 0.11864277004787588\n",
      "Epoch: 422 Loss: 0.11864038332397499\n",
      "Epoch: 423 Loss: 0.1186412798834371\n",
      "Epoch: 424 Loss: 0.11862458813337566\n",
      "Epoch: 425 Loss: 0.11861486211915721\n",
      "Epoch: 426 Loss: 0.11861965812808478\n",
      "Epoch: 427 Loss: 0.11859894067134928\n",
      "Epoch: 428 Loss: 0.1186079680355921\n",
      "Epoch: 429 Loss: 0.11859648068855097\n",
      "Epoch: 430 Loss: 0.1185904385133104\n",
      "Epoch: 431 Loss: 0.11860487860020293\n",
      "Epoch: 432 Loss: 0.1185739589054785\n",
      "Epoch: 433 Loss: 0.1185843970091724\n",
      "Epoch: 434 Loss: 0.1185666104975616\n",
      "Epoch: 435 Loss: 0.118569565442656\n",
      "Epoch: 436 Loss: 0.1185643137913312\n",
      "Epoch: 437 Loss: 0.11855925940120784\n",
      "Epoch: 438 Loss: 0.11855736499814337\n",
      "Epoch: 439 Loss: 0.11854975768154408\n",
      "Epoch: 440 Loss: 0.11853262607207536\n",
      "Epoch: 441 Loss: 0.11851989505790715\n",
      "Epoch: 442 Loss: 0.11853438381181496\n",
      "Epoch: 443 Loss: 0.11852713759536894\n",
      "Epoch: 444 Loss: 0.1185191316170062\n",
      "Epoch: 445 Loss: 0.11849339789682585\n",
      "Epoch: 446 Loss: 0.11851469095017196\n",
      "Epoch: 447 Loss: 0.11848705147187187\n",
      "Epoch: 448 Loss: 0.11849455633392808\n",
      "Epoch: 449 Loss: 0.1184996066236148\n",
      "Epoch: 450 Loss: 0.11849277394056491\n",
      "Epoch: 451 Loss: 0.11847465968528238\n",
      "Epoch: 452 Loss: 0.11848716130048986\n",
      "Epoch: 453 Loss: 0.11848177299792005\n",
      "Epoch: 454 Loss: 0.1184585044011836\n",
      "Epoch: 455 Loss: 0.11847034341670268\n",
      "Epoch: 456 Loss: 0.11846344652524447\n",
      "Epoch: 457 Loss: 0.11843944174645077\n",
      "Epoch: 458 Loss: 0.11845368700287531\n",
      "Epoch: 459 Loss: 0.118439949659684\n",
      "Epoch: 460 Loss: 0.11843094205404268\n",
      "Epoch: 461 Loss: 0.11843352568480287\n",
      "Epoch: 462 Loss: 0.11841422453766963\n",
      "Epoch: 463 Loss: 0.11841938249368192\n",
      "Epoch: 464 Loss: 0.11839643647849635\n",
      "Epoch: 465 Loss: 0.11841371202738386\n",
      "Epoch: 466 Loss: 0.11840926550316652\n",
      "Epoch: 467 Loss: 0.11839894926767051\n",
      "Epoch: 468 Loss: 0.1183953223261097\n",
      "Epoch: 469 Loss: 0.11838531114430798\n",
      "Epoch: 470 Loss: 0.11837961835579665\n",
      "Epoch: 471 Loss: 0.11836153427865394\n",
      "Epoch: 472 Loss: 0.1183806160960492\n",
      "Epoch: 473 Loss: 0.11834598619338521\n",
      "Epoch: 474 Loss: 0.11835547814179576\n",
      "Epoch: 475 Loss: 0.11834186379157022\n",
      "Epoch: 476 Loss: 0.11836563319590603\n",
      "Epoch: 477 Loss: 0.11834013565416444\n",
      "Epoch: 478 Loss: 0.11832511408911849\n",
      "Epoch: 479 Loss: 0.11833418758360895\n",
      "Epoch: 480 Loss: 0.11833028935420795\n",
      "Epoch: 481 Loss: 0.11831906619339971\n",
      "Epoch: 482 Loss: 0.11831637355276667\n",
      "Epoch: 483 Loss: 0.11830548170186919\n",
      "Epoch: 484 Loss: 0.11831878916897488\n",
      "Epoch: 485 Loss: 0.11831898258475866\n",
      "Epoch: 486 Loss: 0.11829020335016713\n",
      "Epoch: 487 Loss: 0.11829379859018532\n",
      "Epoch: 488 Loss: 0.11827644776909271\n",
      "Epoch: 489 Loss: 0.11828684112903685\n",
      "Epoch: 490 Loss: 0.11828986691840167\n",
      "Epoch: 491 Loss: 0.1182818853628032\n",
      "Epoch: 492 Loss: 0.11828221945242073\n",
      "Epoch: 493 Loss: 0.11827120673058376\n",
      "Epoch: 494 Loss: 0.11828012045300387\n",
      "Epoch: 495 Loss: 0.11826498762586486\n",
      "Epoch: 496 Loss: 0.11824551863866634\n",
      "Epoch: 497 Loss: 0.1182544103140252\n",
      "Epoch: 498 Loss: 0.11825186179669306\n",
      "Epoch: 499 Loss: 0.11824022409986149\n"
     ]
    }
   ],
   "source": [
    "def trainConstantLR(model, optimizer, criterion, tau, N_EPOCHS, lossList):\n",
    "    \"\"\"\n",
    "    The fundamental loop used for both constantLR training\n",
    "    \"\"\"\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        # training loop\n",
    "        epoch_loss = 0.0\n",
    "        model.train()\n",
    "        for inputs, labels in trainLoader: \n",
    "            inputs= inputs.to(device) \n",
    "            labels= labels.to(device)\n",
    "            optimizer.zero_grad() \n",
    "            outputs= model(inputs) \n",
    "            loss= criterion(outputs, labels, tau, h) \n",
    "            loss.backward() \n",
    "            optimizer.step() \n",
    "            epoch_loss+= loss.item() \n",
    "        lossList.append((epoch_loss/len(trainLoader)))\n",
    "    \n",
    "        # validation loop\n",
    "        val_loss= 0.0\n",
    "        model.eval()\n",
    "        for inputs, labels in valLoader:\n",
    "            inputs= inputs.to(device)\n",
    "            labels= labels.to(device)\n",
    "            outputs= model(inputs)\n",
    "            loss= criterion(outputs, labels, tau, h)\n",
    "            val_loss+= loss.item()\n",
    "        print(\"Epoch: {} Train Loss: {} Val Loss: {} LR: {}\".format(epoch,epoch_loss/len(trainLoader),\n",
    "            val_loss/len(valLoader),optimizer.param_groups[0]['lr']))\n",
    "\n",
    "def trainadaH(model, optimizer, criterion, tau, N_EPOCHS, lossList):\n",
    "    \"\"\"\n",
    "    The fundamental loop used for adaH training\n",
    "    \"\"\"\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        epoch_loss= 0.0\n",
    "        for inputs, labels in trainLoader: \n",
    "            inputs= inputs.to(device) \n",
    "            labels= labels.to(device)\n",
    "            optimizer.zero_grad() \n",
    "            outputs= model(inputs) \n",
    "            loss= criterion(outputs, labels, tau, h) \n",
    "            loss.backward(create_graph=True)\n",
    "            _, gradsH = get_params_grad(model)\n",
    "            optimizer.step(gradsH)\n",
    "            epoch_loss+= loss.item() \n",
    "        lossList.append((epoch_loss/len(trainLoader)))\n",
    "        \n",
    "        # validation loop\n",
    "        val_loss= 0.0\n",
    "        model.eval()\n",
    "        for inputs, labels in valLoader:\n",
    "            inputs= inputs.to(device)\n",
    "            labels= labels.to(device)\n",
    "            outputs= model(inputs)\n",
    "            loss= criterion(outputs, labels, tau, h)\n",
    "            val_loss+= loss.item()\n",
    "        print(\"Epoch: {} Train Loss: {} Val Loss: {} LR: {}\".format(epoch,epoch_loss/len(trainLoader),\n",
    "            val_loss/len(valLoader),optimizer.param_groups[0]['lr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calls to training loops themselves:\n",
    "trainConstantLR(model_CLR_S, optimizer_CLR_S, criterion, tau, N_EPOCHS, lossList_CLR_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainConstantLR(model_CLR_M, optimizer_CLR_M, criterion, tau, N_EPOCHS, lossList_CLR_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainConstantLR(model_CLR_L, optimizer_CLR_L, criterion, tau, N_EPOCHS, lossList_CLR_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainConstantLR(model_CLR_WM, optimizer_CLR_WM, criterion, tau, N_EPOCHS, lossList_CLR_WM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the EPOCH-losses\n",
    "A= np.asarray(ls_a)\n",
    "B= np.asarray(ls_b)\n",
    "C= np.asarray(ls_c)\n",
    "D= np.asarray(ls_d)\n",
    "\n",
    "np.save(\"/home/aryamanj/Downloads/ls_a.npy\", A)\n",
    "np.save(\"/home/aryamanj/Downloads/ls_b.npy\", B)\n",
    "np.save(\"/home/aryamanj/Downloads/ls_c.npy\", C)\n",
    "np.save(\"/home/aryamanj/Downloads/ls_d.npy\", D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiations and training loops using adaH as the optimizer algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating objects for all adaH tests:\n",
    "size1,size2= 300,300\n",
    "model_adaH_S= Network(size1, size2, 0.1).to(device)\n",
    "optimizer_adaH_S= optim.SGD(model_adaH_S.parameters(), lr= 0.1)\n",
    "lossList_adaH_S= []\n",
    "\n",
    "size1, size2= 1000, 1000\n",
    "model_adaH_M= Network(size1, size2, 0.1).to(device)\n",
    "optimizer_adaH_M= optim.SGD(model_adaH_M.parameters(), lr= 0.1)\n",
    "lossList_adaH_M= []\n",
    "\n",
    "size1, size2= 3000, 3000\n",
    "model_adaH_L= Network(size1, size2, 0.1).to(device)\n",
    "optimizer_adaH_L= optim.SGD(model_adaH_L.parameters(), lr= 0.1)\n",
    "lossList_adaH_L= []\n",
    "\n",
    "size1= 2000\n",
    "model_adaH_WM= wideM(size1, 0.1).to(device)\n",
    "optimizer_adaH_WM= optim.SGD(model_adaH_WM.parameters(), lr= 0.1)\n",
    "lossList_adaH_WM= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainadaH(model_adaH_S, optimizer_adaH_S, criterion, tau, N_EPOCHS, lossList_adaH_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainadaH(model_adaH_M, optimizer_adaH_M, criterion, tau, N_EPOCHS, lossList_adaH_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainadaH(model_adaH_L, optimizer_adaH_L, criterion, tau, N_EPOCHS, lossList_adaH_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainadaH(model_adaH_WM, optimizer_adaH_WM, criterion, tau, N_EPOCHS, lossList_adaH_WM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "E= np.asarray(ls_e)\n",
    "F= np.asarray(ls_f)\n",
    "G= np.asarray(ls_g)\n",
    "\n",
    "np.save(\"/home/aryamanj/Downloads/ls_e.npy\", E)\n",
    "np.save(\"/home/aryamanj/Downloads/ls_f.npy\", F)\n",
    "np.save(\"/home/aryamanj/Downloads/ls_g.npy\", G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I see the following steps to incorporating LALR into the code:\n",
    "1. Be able to compute the $K_z$: \n",
    "    * This is defined to be the largest activation in the penultimate layer\n",
    "    * Hence, our network needs a method that allows us to obtain it's penultimate output and then simply take a supremum over it \n",
    "2. For now, a possible LC of the log-cosh seems to be just the same as for the check, namely, $\\frac{K_z}{m}\\times \\max(\\tau, 1-\\tau)$ -- these kinds of LC's are all expressable in the form: $C.\\frac{K_z}{m}$ (where C is some constant which we know)\n",
    "\n",
    "The next expression that we want to try out, is: $\\frac{1}{m}\\tanh(g(0)-y_k).K_z: \\text{ } z_k^{[L]}=0$, or, exploiting the continuity of the regression problem, we can also write: $\\frac{1}{m}\\tanh(g(0)-y_k).K_z,\\text{ where: } k= \\argmin_j z_j^{[L]}$\n",
    "\n",
    "Hence, for implementing the above expression in addition to the penultimate function, we also need access to the set of pre-activation values, and be able to compute the $\\argmin$ from among them, since in this network, we do not have any activation function acting on the final layer, we can just compute the minimum feature in the final layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new network class for LALR training, that supports returning penultimate activations\n",
    "class LALRnetwork(nn.Module):\n",
    "    def __init__(self, size1, size2, drop):\n",
    "        super(LALRnetwork, self).__init__()\n",
    "        self.l1= nn.Linear(943, size1)\n",
    "        self.l2= nn.Dropout(p= drop)\n",
    "        self.l3= nn.Linear(size1, size2)\n",
    "        self.l4= nn.Dropout(p= drop)\n",
    "        self.l5= nn.Linear(size2, 4760)\n",
    "        # creating separate heads for all the different quantiles\n",
    "        self.fc1= create_head(4760, 4760, ps=drop)\n",
    "        self.fc2= create_head(4760, 4760, ps=drop)\n",
    "        self.fc3= create_head(4760, 4760, ps=drop)\n",
    "        self.fc4= create_head(4760, 4760, ps=drop)\n",
    "        self.fc5= create_head(4760, 4760, ps=drop)\n",
    "        self.fc6= create_head(4760, 4760, ps=drop)\n",
    "        self.fc7= create_head(4760, 4760, ps=drop)\n",
    "        self.fc8= create_head(4760, 4760, ps=drop)\n",
    "        self.fc9= create_head(4760, 4760, ps=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x= F.tanh(self.l2(self.l1(x)))\n",
    "        x= self.l5(F.tanh(self.l4(self.l3(x))))\n",
    "        q1= self.fc1(x)\n",
    "        q2= self.fc2(x)\n",
    "        q3= self.fc3(x)\n",
    "        q4= self.fc4(x)\n",
    "        q5= self.fc5(x)\n",
    "        q6= self.fc6(x)\n",
    "        q7= self.fc7(x)\n",
    "        q8= self.fc8(x)\n",
    "        q9= self.fc9(x)\n",
    "        return [q1,q2,q3,q4,q5,q6,q7,q8,q9]\n",
    "    \n",
    "    def penU(self, x):\n",
    "        x= F.tanh(self.l2(self.l1(x)))\n",
    "        x= F.tanh(self.l4(self.l3(x)))\n",
    "        return x\n",
    "\n",
    "class LALRwideM(nn.Module):\n",
    "    def __init__(self, size1, drop):\n",
    "        super(LALRwideM, self).__init__()\n",
    "        self.l1= nn.Linear(943, size1)\n",
    "        self.l2= nn.Dropout(p= drop)\n",
    "        self.l3= nn.Linear(size1, 4760)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x= F.tanh(self.l1(x))\n",
    "        return self.l3(self.l2(x))\n",
    "        \n",
    "    def penU(self, x):\n",
    "        x= F.tanh(self.l1(x))\n",
    "        return self.l2(x)\n",
    "\n",
    "\n",
    "def computeLR(model, bSize= 16):\n",
    "    \"\"\"\n",
    "    Takes in a network of the LALRnetwork class(during some arbitrary EPOCH of training) and the current input, and returns Kz for the EPOCH\n",
    "    \"\"\"\n",
    "    Kz = 0.0\n",
    "    z_k= 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i,j in enumerate(trainLoader):\n",
    "            inputs,labels= j[0],j[1]\n",
    "            inputs= inputs.to(device)\n",
    "            labels= labels.to(device)\n",
    "            op1= model.penU(inputs)\n",
    "            op2= model(inputs)\n",
    "            # activ1= np.linalg.norm(op1.detach().cpu().numpy())\n",
    "            # activ2= np.linalg.norm(op2.detach().cpu().numpy())\n",
    "            activ1= torch.max(op1)\n",
    "            activ2, arg2= torch.min(op2)\n",
    "            if activ1 > Kz:\n",
    "                Kz= activ1\n",
    "            if activ2 < z_k:\n",
    "                z_k= activ2\n",
    "                argMin= arg2\n",
    "    LR= (1/bSize)*np.tanh(-op2[argMin])*Kz\n",
    "    if LR==0:\n",
    "        return 0.1\n",
    "    return 1/LR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing training loop for networks with LALR\n",
    "\n",
    "def train_adaptive_lr(model,optimizer, criterion, epochs, ls_list):\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        lr_val= computeLR(model, bSize=16)\n",
    "        optimizer.param_groups[0]['lr']= lr_val\n",
    "        for inputs, labels in trainLoader: \n",
    "            inputs= inputs.to(device) \n",
    "            labels= labels.to(device)\n",
    "            optimizer.zero_grad() \n",
    "            outputs= model(inputs) \n",
    "            loss= criterion(outputs, labels, tau, h) \n",
    "            loss.backward() \n",
    "            optimizer.step() \n",
    "            epoch_loss+= loss.item() \n",
    "        ls_list.append((epoch_loss/len(trainLoader)))\n",
    "        print(\"Epoch: {} Loss: {} LR: {}\".format(epoch,\n",
    "            epoch_loss/len(trainLoader), optimizer.param_groups[0]['lr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating LALR models\n",
    "model_lr_1= LALRnetwork(size1=300, size2=300, drop=0.1).to(device)\n",
    "optimizer_lr_1= optim.SGD(model_lr_1.parameters(), lr= 0.1)\n",
    "loss_list_lr_1= []\n",
    "\n",
    "model_lr_2= LALRnetwork(size1=1000, size2=1000, drop=0.1).to(device)\n",
    "optimizer_lr_2= optim.SGD(model_lr_2.parameters(), lr= 0.1)\n",
    "loss_list_lr_2= []\n",
    "\n",
    "\n",
    "model_lr_3= LALRwideM(size1=2000,drop=0.1).to(device)\n",
    "optimizer_lr_3= optim.SGD(model_lr_3.parameters(), lr= 0.1)\n",
    "loss_list_lr_3= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aryamanj/miniconda3/lib/python3.9/site-packages/torch/nn/functional.py:1698: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.17367235173047973 LR: 1.6327996884124587\n",
      "Epoch: 1 Loss: 0.1438976589505081 LR: 0.6847130045882568\n",
      "Epoch: 2 Loss: 0.13761624940702066 LR: 0.700449054833888\n",
      "Epoch: 3 Loss: 0.13314414638680308 LR: 0.6964594313141763\n",
      "Epoch: 4 Loss: 0.12983695297783676 LR: 0.705482966403094\n",
      "Epoch: 5 Loss: 0.12724609879722812 LR: 0.7041584560673653\n",
      "Epoch: 6 Loss: 0.12514590478828203 LR: 0.7123995500692348\n",
      "Epoch: 7 Loss: 0.12340140272917736 LR: 0.6974392913040496\n",
      "Epoch: 8 Loss: 0.1219351899809632 LR: 0.712972342876764\n",
      "Epoch: 9 Loss: 0.12065037253890418 LR: 0.7236611147731506\n",
      "Epoch: 10 Loss: 0.11950898959016697 LR: 0.7302756085689559\n",
      "Epoch: 11 Loss: 0.1184839633505015 LR: 0.7299473932986149\n",
      "Epoch: 12 Loss: 0.11757076266697346 LR: 0.7263564012705294\n",
      "Epoch: 13 Loss: 0.1167418597649613 LR: 0.732737926467025\n",
      "Epoch: 14 Loss: 0.11598094526448136 LR: 0.7012487894345484\n",
      "Epoch: 15 Loss: 0.11533269464534761 LR: 0.7363674365192464\n",
      "Epoch: 16 Loss: 0.11469170159771515 LR: 0.7397070626675529\n",
      "Epoch: 17 Loss: 0.11410318868554534 LR: 0.7334684281690219\n",
      "Epoch: 18 Loss: 0.11356624557029833 LR: 0.7440128512961985\n",
      "Epoch: 19 Loss: 0.11306077293035598 LR: 0.744059772372175\n",
      "Epoch: 20 Loss: 0.11260021141044214 LR: 0.7475943341661317\n",
      "Epoch: 21 Loss: 0.11214357273462892 LR: 0.749508516910744\n",
      "Epoch: 22 Loss: 0.11172820298983328 LR: 0.747732274913065\n",
      "Epoch: 23 Loss: 0.11134262989123991 LR: 0.7610397655281957\n",
      "Epoch: 24 Loss: 0.11098140527258862 LR: 0.7652260788171346\n",
      "Epoch: 25 Loss: 0.11061760405901637 LR: 0.759226861143055\n",
      "Epoch: 26 Loss: 0.11027903178644018 LR: 0.7579772041415013\n",
      "Epoch: 27 Loss: 0.10997111599545417 LR: 0.7666058245574081\n",
      "Epoch: 28 Loss: 0.10967278403352991 LR: 0.7657530541409696\n",
      "Epoch: 29 Loss: 0.10938815743139124 LR: 0.7662903456241489\n",
      "Epoch: 30 Loss: 0.10911533403742359 LR: 0.763337417274708\n",
      "Epoch: 31 Loss: 0.10886421942336133 LR: 0.7721111715321117\n",
      "Epoch: 32 Loss: 0.10862539775590385 LR: 0.7768549563745322\n",
      "Epoch: 33 Loss: 0.10839203350786861 LR: 0.776897764960079\n",
      "Epoch: 34 Loss: 0.1081564007667545 LR: 0.7729322313428209\n",
      "Epoch: 35 Loss: 0.10795071635310058 LR: 0.7804234709209129\n",
      "Epoch: 36 Loss: 0.10772997692530702 LR: 0.7721248877632875\n",
      "Epoch: 37 Loss: 0.107546626449093 LR: 0.7907495419207335\n",
      "Epoch: 38 Loss: 0.10734554331187965 LR: 0.7829852499929529\n",
      "Epoch: 39 Loss: 0.10714715281664154 LR: 0.765841070463775\n",
      "Epoch: 40 Loss: 0.10699589232124095 LR: 0.7923574969740901\n",
      "Epoch: 41 Loss: 0.10682871379126001 LR: 0.7902626500217335\n",
      "Epoch: 42 Loss: 0.1066435672701146 LR: 0.7702187508780023\n",
      "Epoch: 43 Loss: 0.10648972275202993 LR: 0.7772450125297121\n",
      "Epoch: 44 Loss: 0.10634638314187216 LR: 0.7880915484059167\n",
      "Epoch: 45 Loss: 0.10619643629751556 LR: 0.7874785343634648\n",
      "Epoch: 46 Loss: 0.10604826828833637 LR: 0.7844503802831073\n",
      "Epoch: 47 Loss: 0.10590458459450176 LR: 0.7744957140844413\n",
      "Epoch: 48 Loss: 0.10578270807762959 LR: 0.7833526651530541\n",
      "Epoch: 49 Loss: 0.10566831300133435 LR: 0.7994382609265798\n",
      "Epoch: 50 Loss: 0.10553885351277927 LR: 0.7988817989194164\n",
      "Epoch: 51 Loss: 0.10540973256056727 LR: 0.7891983622216262\n",
      "Epoch: 52 Loss: 0.10529435565317671 LR: 0.7978321993644626\n",
      "Epoch: 53 Loss: 0.10517756235162495 LR: 0.796042924746562\n",
      "Epoch: 54 Loss: 0.10507647668594843 LR: 0.8027222445503436\n",
      "Epoch: 55 Loss: 0.10495024563959753 LR: 0.7933510875489508\n",
      "Epoch: 56 Loss: 0.10485167025182938 LR: 0.7992280405431156\n",
      "Epoch: 57 Loss: 0.10475611087277738 LR: 0.8065425590063593\n",
      "Epoch: 58 Loss: 0.10464962724468081 LR: 0.7985297760147342\n",
      "Epoch: 59 Loss: 0.1045597738708339 LR: 0.8080718873690313\n",
      "Epoch: 60 Loss: 0.10446189499893525 LR: 0.8061107762703108\n",
      "Epoch: 61 Loss: 0.10436075880863155 LR: 0.7953284194296748\n",
      "Epoch: 62 Loss: 0.10427601143269469 LR: 0.8054295149511179\n",
      "Epoch: 63 Loss: 0.10419437884371724 LR: 0.8099416706607246\n",
      "Epoch: 64 Loss: 0.10411083131459485 LR: 0.8105082488454862\n",
      "Epoch: 65 Loss: 0.1040394047647312 LR: 0.8164951207051094\n",
      "Epoch: 66 Loss: 0.1039356339881923 LR: 0.8078449658717944\n",
      "Epoch: 67 Loss: 0.10385792704047093 LR: 0.8052510697985348\n",
      "Epoch: 68 Loss: 0.10375656423896654 LR: 0.7883881881919513\n",
      "Epoch: 69 Loss: 0.10371634402515179 LR: 0.8114984517542159\n",
      "Epoch: 70 Loss: 0.10363423731295489 LR: 0.8106730495294223\n",
      "Epoch: 71 Loss: 0.10356669476914461 LR: 0.8196272627273866\n",
      "Epoch: 72 Loss: 0.10348857417662409 LR: 0.8121804292451674\n",
      "Epoch: 73 Loss: 0.10339302890817446 LR: 0.7884971973625797\n",
      "Epoch: 74 Loss: 0.10335820252815682 LR: 0.8168163162131448\n",
      "Epoch: 75 Loss: 0.10329065832940765 LR: 0.8178037438619201\n",
      "Epoch: 76 Loss: 0.10322207112051178 LR: 0.8164509363443149\n",
      "Epoch: 77 Loss: 0.10314964905577165 LR: 0.8092386203037801\n",
      "Epoch: 78 Loss: 0.10309575027258541 LR: 0.8116483413985928\n",
      "Epoch: 79 Loss: 0.10303920519645096 LR: 0.8224552844412504\n",
      "Epoch: 80 Loss: 0.10294533653009949 LR: 0.7913914772711728\n",
      "Epoch: 81 Loss: 0.1029103693212234 LR: 0.8125211397097994\n",
      "Epoch: 82 Loss: 0.10286024170289317 LR: 0.8145183489779497\n",
      "Epoch: 83 Loss: 0.10280309808515167 LR: 0.8156356562517958\n",
      "Epoch: 84 Loss: 0.10274729104063314 LR: 0.8122166028936944\n",
      "Epoch: 85 Loss: 0.10270071441183175 LR: 0.8212380176273385\n",
      "Epoch: 86 Loss: 0.1026451225858769 LR: 0.8206292893922178\n",
      "Epoch: 87 Loss: 0.10258387363916946 LR: 0.8122206922914631\n",
      "Epoch: 88 Loss: 0.10256142386607256 LR: 0.8312580309233718\n",
      "Epoch: 89 Loss: 0.10248705420466592 LR: 0.8148794676817533\n",
      "Epoch: 90 Loss: 0.10246290382883008 LR: 0.8314577496345786\n",
      "Epoch: 91 Loss: 0.1024071855832598 LR: 0.8264902249096424\n",
      "Epoch: 92 Loss: 0.10233687919215069 LR: 0.8095220219290453\n",
      "Epoch: 93 Loss: 0.10230185120889806 LR: 0.8190506672269812\n",
      "Epoch: 94 Loss: 0.10227189558386868 LR: 0.8295264704387411\n",
      "Epoch: 95 Loss: 0.10222939459579826 LR: 0.8298473302378881\n",
      "Epoch: 96 Loss: 0.10216973885461632 LR: 0.8175331584685053\n",
      "Epoch: 97 Loss: 0.10214010999076453 LR: 0.8271093135835365\n",
      "Epoch: 98 Loss: 0.10210693932679124 LR: 0.8354279430392574\n",
      "Epoch: 99 Loss: 0.1020694917101105 LR: 0.8286878282207619\n"
     ]
    }
   ],
   "source": [
    "train_adaptive_lr(model_lr_1,optimizer_lr_1, criterion, 100,loss_list_lr_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.2674305171842855 LR: 0.9126519997249622\n",
      "Epoch: 1 Loss: 0.2674271901262449 LR: 0.9352691898299412\n",
      "Epoch: 2 Loss: 0.2674272424051341 LR: 0.9262805662578283\n",
      "Epoch: 3 Loss: 0.26743391728781296 LR: 0.9057811818054404\n",
      "Epoch: 4 Loss: 0.26742825945837606 LR: 0.9015939795771736\n",
      "Epoch: 5 Loss: 0.2674365390304874 LR: 0.9329796987076587\n",
      "Epoch: 6 Loss: 0.26742701939506974 LR: 0.8969772091488466\n",
      "Epoch: 7 Loss: 0.26742735472622703 LR: 0.9006793795554802\n",
      "Epoch: 8 Loss: 0.2674325669892005 LR: 0.9256528839203082\n",
      "Epoch: 9 Loss: 0.2674266588331407 LR: 0.9313245655132905\n",
      "Epoch: 10 Loss: 0.2674270817217065 LR: 0.9233213428398747\n",
      "Epoch: 11 Loss: 0.26742708029628465 LR: 0.9172196045963499\n",
      "Epoch: 12 Loss: 0.2674366643145973 LR: 0.933775326875597\n",
      "Epoch: 13 Loss: 0.2674257676250504 LR: 0.9363839735456096\n",
      "Epoch: 14 Loss: 0.2674297019825865 LR: 0.9179157425802812\n",
      "Epoch: 15 Loss: 0.26742823457120884 LR: 0.9263275156422387\n",
      "Epoch: 16 Loss: 0.2674285500592057 LR: 0.9302631551223366\n",
      "Epoch: 17 Loss: 0.2674252700535075 LR: 0.9302649088876666\n",
      "Epoch: 18 Loss: 0.2674284869004018 LR: 0.9292588191491872\n",
      "Epoch: 19 Loss: 0.2674257776486382 LR: 0.9146231514377722\n",
      "Epoch: 20 Loss: 0.2674255152878042 LR: 0.9304671520540226\n",
      "Epoch: 21 Loss: 0.2674320996655921 LR: 0.9160243058626767\n",
      "Epoch: 22 Loss: 0.2674271296357447 LR: 0.9092890564349217\n",
      "Epoch: 23 Loss: 0.2674273956688519 LR: 0.9353330109485571\n",
      "Epoch: 24 Loss: 0.2674289621242294 LR: 0.917846944906149\n",
      "Epoch: 25 Loss: 0.26742864034934377 LR: 0.9203229059206989\n",
      "Epoch: 26 Loss: 0.26742932551552684 LR: 0.9198533341608636\n",
      "Epoch: 27 Loss: 0.2674243747167943 LR: 0.9181145605282913\n",
      "Epoch: 28 Loss: 0.2674272444613924 LR: 0.9351052970633602\n",
      "Epoch: 29 Loss: 0.2674255984776773 LR: 0.9150963814204827\n",
      "Epoch: 30 Loss: 0.2674270477719703 LR: 0.9248264725509723\n",
      "Epoch: 31 Loss: 0.26742655585379543 LR: 0.916788160344551\n",
      "Epoch: 32 Loss: 0.2674272219391905 LR: 0.9246489939384106\n",
      "Epoch: 33 Loss: 0.26742848651116224 LR: 0.922674117919061\n",
      "Epoch: 34 Loss: 0.2674287374820436 LR: 0.9193966354363354\n",
      "Epoch: 35 Loss: 0.2674280727308004 LR: 0.9241975003966225\n",
      "Epoch: 36 Loss: 0.26742756219624053 LR: 0.9249608756014484\n",
      "Epoch: 37 Loss: 0.2674303680303998 LR: 0.9320305850090168\n",
      "Epoch: 38 Loss: 0.2674268018504646 LR: 0.9102551122995222\n",
      "Epoch: 39 Loss: 0.2674321161827683 LR: 0.9210670471301146\n",
      "Epoch: 40 Loss: 0.26742505687106904 LR: 0.9310064151326188\n",
      "Epoch: 41 Loss: 0.2674264618672244 LR: 0.919850913369439\n",
      "Epoch: 42 Loss: 0.26742482533531775 LR: 0.9168459767773233\n",
      "Epoch: 43 Loss: 0.26743140710922775 LR: 0.9427250623858273\n",
      "Epoch: 44 Loss: 0.26742894351052887 LR: 0.9087855797405339\n",
      "Epoch: 45 Loss: 0.2674253039415023 LR: 0.9293298530672919\n",
      "Epoch: 46 Loss: 0.26743046337796683 LR: 0.9303395016168302\n",
      "Epoch: 47 Loss: 0.26742607700332954 LR: 0.9315028578230797\n",
      "Epoch: 48 Loss: 0.2674340775766323 LR: 0.9242748912502149\n",
      "Epoch: 49 Loss: 0.26742722583158535 LR: 0.9350602678907325\n",
      "Epoch: 50 Loss: 0.26742906035216857 LR: 0.9304349523591494\n",
      "Epoch: 51 Loss: 0.2674290556517663 LR: 0.9144031161576226\n",
      "Epoch: 52 Loss: 0.2674247364571794 LR: 0.9319054040152569\n",
      "Epoch: 53 Loss: 0.2674288374494806 LR: 0.9335064000868895\n",
      "Epoch: 54 Loss: 0.26743106956074764 LR: 0.9326257838997631\n",
      "Epoch: 55 Loss: 0.26742816297798794 LR: 0.9231942229926934\n",
      "Epoch: 56 Loss: 0.2674334114214425 LR: 0.9205355970049426\n",
      "Epoch: 57 Loss: 0.2674310066623315 LR: 0.9025408319558067\n",
      "Epoch: 58 Loss: 0.2674251854570035 LR: 0.9298458438858975\n",
      "Epoch: 59 Loss: 0.2674258352668194 LR: 0.9306388181610521\n",
      "Epoch: 60 Loss: 0.26743265228633556 LR: 0.9248348333615387\n",
      "Epoch: 61 Loss: 0.26742637802772407 LR: 0.9040364002388173\n",
      "Epoch: 62 Loss: 0.26742524819972385 LR: 0.9208938389992889\n",
      "Epoch: 63 Loss: 0.2674278120450384 LR: 0.9331106696052109\n",
      "Epoch: 64 Loss: 0.2674274791379036 LR: 0.9075343803264961\n",
      "Epoch: 65 Loss: 0.2674291567251803 LR: 0.9224947252917888\n",
      "Epoch: 66 Loss: 0.2674254804897944 LR: 0.9269891997230727\n",
      "Epoch: 67 Loss: 0.26742847503799294 LR: 0.9175657349696059\n",
      "Epoch: 68 Loss: 0.2674225730265482 LR: 0.9151124536926988\n",
      "Epoch: 69 Loss: 0.26742656853226493 LR: 0.9218308290502479\n",
      "Epoch: 70 Loss: 0.26742501033413324 LR: 0.9287673356229752\n",
      "Epoch: 71 Loss: 0.26743441762966713 LR: 0.9288067215576603\n",
      "Epoch: 72 Loss: 0.26742846371783496 LR: 0.922071987849933\n",
      "Epoch: 73 Loss: 0.2674276065722261 LR: 0.9313424537140917\n",
      "Epoch: 74 Loss: 0.26743049134146823 LR: 0.9368280995896258\n",
      "Epoch: 75 Loss: 0.26742351213257026 LR: 0.9328862150362618\n",
      "Epoch: 76 Loss: 0.2674265931805197 LR: 0.9312975794123991\n",
      "Epoch: 77 Loss: 0.26742905531621497 LR: 0.9188873418202662\n",
      "Epoch: 78 Loss: 0.2674273998565318 LR: 0.9283556800946438\n",
      "Epoch: 79 Loss: 0.267423969695663 LR: 0.9069450761738643\n",
      "Epoch: 80 Loss: 0.26743538625346414 LR: 0.9308025948291827\n",
      "Epoch: 81 Loss: 0.267428580266874 LR: 0.9243247948573009\n",
      "Epoch: 82 Loss: 0.2674270049583115 LR: 0.9221599712862748\n",
      "Epoch: 83 Loss: 0.26742609605727335 LR: 0.9217534417930388\n",
      "Epoch: 84 Loss: 0.267425992272605 LR: 0.9190530495548207\n",
      "Epoch: 85 Loss: 0.26743040517995303 LR: 0.9244741304864357\n",
      "Epoch: 86 Loss: 0.26742789717037124 LR: 0.9305684098472916\n",
      "Epoch: 87 Loss: 0.26742440099448606 LR: 0.9314232176718639\n",
      "Epoch: 88 Loss: 0.2674379538193713 LR: 0.905556972709713\n",
      "Epoch: 89 Loss: 0.2674299242893354 LR: 0.9104015167917727\n",
      "Epoch: 90 Loss: 0.26742971289202966 LR: 0.9278428807322747\n",
      "Epoch: 91 Loss: 0.2674268161959529 LR: 0.9248780672816564\n",
      "Epoch: 92 Loss: 0.2674273014514216 LR: 0.8836886460033211\n",
      "Epoch: 93 Loss: 0.26742495942966205 LR: 0.9151036687782997\n",
      "Epoch: 94 Loss: 0.2674271784812733 LR: 0.89297965329182\n",
      "Epoch: 95 Loss: 0.267429036850157 LR: 0.9227994703418148\n",
      "Epoch: 96 Loss: 0.26742699925393976 LR: 0.9174033723175989\n",
      "Epoch: 97 Loss: 0.26742773199860975 LR: 0.937891234863233\n",
      "Epoch: 98 Loss: 0.2674281036310466 LR: 0.9343874319073299\n",
      "Epoch: 99 Loss: 0.26742752436216277 LR: 0.9194028829981322\n"
     ]
    }
   ],
   "source": [
    "train_adaptive_lr(model_lr_2,optimizer_lr_2, criterion, 100,loss_list_lr_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.2766984735812352 LR: 0.3429063828778339\n",
      "Epoch: 1 Loss: 0.2766971208586003 LR: 0.34261955077124656\n",
      "Epoch: 2 Loss: 0.2766984212916083 LR: 0.3458309937426772\n",
      "Epoch: 3 Loss: 0.27669959257495574 LR: 0.34909062822853176\n",
      "Epoch: 4 Loss: 0.2767024405946218 LR: 0.33833348713457956\n",
      "Epoch: 5 Loss: 0.27669661724713446 LR: 0.34781446297450036\n",
      "Epoch: 6 Loss: 0.2766980450151418 LR: 0.343522984846034\n",
      "Epoch: 7 Loss: 0.2767026562950371 LR: 0.3470735479450581\n",
      "Epoch: 8 Loss: 0.27669903356800035 LR: 0.34617273691945266\n",
      "Epoch: 9 Loss: 0.2766997841774204 LR: 0.34503833975043874\n",
      "Epoch: 10 Loss: 0.27669807655427764 LR: 0.34747707182064125\n",
      "Epoch: 11 Loss: 0.276699979640067 LR: 0.34331283018648623\n",
      "Epoch: 12 Loss: 0.2767020834821599 LR: 0.3523759372066897\n",
      "Epoch: 13 Loss: 0.2766986167005667 LR: 0.34504600362198007\n",
      "Epoch: 14 Loss: 0.27669969829239943 LR: 0.3453722606796819\n",
      "Epoch: 15 Loss: 0.27670550601393484 LR: 0.3423404908108156\n",
      "Epoch: 16 Loss: 0.27669683603730594 LR: 0.34144203460606926\n",
      "Epoch: 17 Loss: 0.2767089406711908 LR: 0.34693830115618696\n",
      "Epoch: 18 Loss: 0.2766980921184881 LR: 0.3415465497707757\n",
      "Epoch: 19 Loss: 0.2766956705147029 LR: 0.346299381445478\n",
      "Epoch: 20 Loss: 0.27669967639298076 LR: 0.3460894438570812\n",
      "Epoch: 21 Loss: 0.2766990542540656 LR: 0.3382657082144704\n",
      "Epoch: 22 Loss: 0.2766989765940776 LR: 0.34417078167384857\n",
      "Epoch: 23 Loss: 0.2767003901051822 LR: 0.34754965833803925\n",
      "Epoch: 24 Loss: 0.27669936868977363 LR: 0.34929311162961824\n",
      "Epoch: 25 Loss: 0.2767059764944096 LR: 0.3463128773640911\n",
      "Epoch: 26 Loss: 0.27670241245931815 LR: 0.33708452661902766\n",
      "Epoch: 27 Loss: 0.27669926989005506 LR: 0.34101307853789653\n",
      "Epoch: 28 Loss: 0.27669997264986274 LR: 0.33871914812413345\n",
      "Epoch: 29 Loss: 0.2767014329448689 LR: 0.3413631970570342\n",
      "Epoch: 30 Loss: 0.2766976038779553 LR: 0.3417814486425032\n",
      "Epoch: 31 Loss: 0.276699513725774 LR: 0.345415436546208\n",
      "Epoch: 32 Loss: 0.27670059739802466 LR: 0.3488299988689213\n",
      "Epoch: 33 Loss: 0.27669971329556825 LR: 0.34067593523955075\n",
      "Epoch: 34 Loss: 0.27669861959972974 LR: 0.3420509499911109\n",
      "Epoch: 35 Loss: 0.2766989916133528 LR: 0.34267357519726405\n",
      "Epoch: 36 Loss: 0.27669773275112164 LR: 0.3428669149887379\n",
      "Epoch: 37 Loss: 0.2766994043253194 LR: 0.34552829085870407\n",
      "Epoch: 38 Loss: 0.27669905974368453 LR: 0.346116204095065\n",
      "Epoch: 39 Loss: 0.2766970653476811 LR: 0.3409679193904254\n",
      "Epoch: 40 Loss: 0.27670068908137174 LR: 0.34391008411657265\n",
      "Epoch: 41 Loss: 0.27669779036393405 LR: 0.3475704234864339\n",
      "Epoch: 42 Loss: 0.27669844915847097 LR: 0.3464745956457892\n",
      "Epoch: 43 Loss: 0.27670057579657553 LR: 0.3373907651884622\n",
      "Epoch: 44 Loss: 0.27670110896339806 LR: 0.3385757648823898\n",
      "Epoch: 45 Loss: 0.276703542985265 LR: 0.35250769531389525\n",
      "Epoch: 46 Loss: 0.27669864040390907 LR: 0.3363826393585037\n",
      "Epoch: 47 Loss: 0.27670084758774266 LR: 0.34316019780761564\n",
      "Epoch: 48 Loss: 0.276703399672656 LR: 0.34649274223014953\n",
      "Epoch: 49 Loss: 0.27670283503917675 LR: 0.33030275873376425\n",
      "Epoch: 50 Loss: 0.2767003461533335 LR: 0.34682627346882006\n",
      "Epoch: 51 Loss: 0.2767004039808987 LR: 0.3425976939144961\n",
      "Epoch: 52 Loss: 0.27670094969465464 LR: 0.33869658264251723\n",
      "Epoch: 53 Loss: 0.2766999429495479 LR: 0.34410468096941227\n",
      "Epoch: 54 Loss: 0.2767005684063942 LR: 0.34413102220509806\n",
      "Epoch: 55 Loss: 0.276705799556878 LR: 0.33877703891396954\n",
      "Epoch: 56 Loss: 0.2766992309553691 LR: 0.3381242349627042\n",
      "Epoch: 57 Loss: 0.27669738229331275 LR: 0.3420164198385154\n",
      "Epoch: 58 Loss: 0.27670340979288255 LR: 0.3418386358220273\n",
      "Epoch: 59 Loss: 0.27670256444257224 LR: 0.34120452085186914\n",
      "Epoch: 60 Loss: 0.2766975783787425 LR: 0.3459933467513588\n",
      "Epoch: 61 Loss: 0.27669693405854556 LR: 0.3440839326746936\n",
      "Epoch: 62 Loss: 0.27670653880050516 LR: 0.34083763757965607\n",
      "Epoch: 63 Loss: 0.27669760801731585 LR: 0.34478548112789786\n",
      "Epoch: 64 Loss: 0.27670190110601317 LR: 0.34404281045706064\n",
      "Epoch: 65 Loss: 0.2767001122767765 LR: 0.3436232039124\n",
      "Epoch: 66 Loss: 0.27669635502052103 LR: 0.33874447974953137\n",
      "Epoch: 67 Loss: 0.276702396419967 LR: 0.3419600095585326\n",
      "Epoch: 68 Loss: 0.27669842018026247 LR: 0.34225925491627457\n",
      "Epoch: 69 Loss: 0.27670699227524115 LR: 0.34892762031972563\n",
      "Epoch: 70 Loss: 0.2766982350740706 LR: 0.33930089666342705\n",
      "Epoch: 71 Loss: 0.27670276495727886 LR: 0.3474322278764914\n",
      "Epoch: 72 Loss: 0.27669825571986967 LR: 0.34106859438043613\n",
      "Epoch: 73 Loss: 0.27670036241280627 LR: 0.3383420842281661\n",
      "Epoch: 74 Loss: 0.2766983417337423 LR: 0.3412069357119661\n",
      "Epoch: 75 Loss: 0.2767007280348486 LR: 0.3426014438044072\n",
      "Epoch: 76 Loss: 0.2767083336696649 LR: 0.3451790118061759\n",
      "Epoch: 77 Loss: 0.2767023747728829 LR: 0.3416079707961829\n",
      "Epoch: 78 Loss: 0.276701822868877 LR: 0.34351474139817567\n",
      "Epoch: 79 Loss: 0.27669875401888766 LR: 0.3456091211782225\n",
      "Epoch: 80 Loss: 0.276704011634972 LR: 0.34868311831796284\n",
      "Epoch: 81 Loss: 0.2766970124862749 LR: 0.341248437786921\n",
      "Epoch: 82 Loss: 0.2767005232680361 LR: 0.3415194625521413\n",
      "Epoch: 83 Loss: 0.27669781767512375 LR: 0.34626287339544093\n",
      "Epoch: 84 Loss: 0.27669912456145396 LR: 0.34619819557476206\n",
      "Epoch: 85 Loss: 0.2766973197009194 LR: 0.3420210215967277\n",
      "Epoch: 86 Loss: 0.2766999359835034 LR: 0.34210194910570557\n",
      "Epoch: 87 Loss: 0.27669851299374626 LR: 0.349031366691623\n",
      "Epoch: 88 Loss: 0.2767002250971697 LR: 0.3374317512492477\n",
      "Epoch: 89 Loss: 0.276697743295485 LR: 0.3455506086613224\n",
      "Epoch: 90 Loss: 0.27670512975625916 LR: 0.34438727606699154\n",
      "Epoch: 91 Loss: 0.2766988968832002 LR: 0.3464817223899525\n",
      "Epoch: 92 Loss: 0.2767022252190201 LR: 0.3482609140414203\n",
      "Epoch: 93 Loss: 0.2766983105623707 LR: 0.34624249284451025\n",
      "Epoch: 94 Loss: 0.27669737519304766 LR: 0.34241091899809367\n",
      "Epoch: 95 Loss: 0.2767041954445938 LR: 0.34484759071448823\n",
      "Epoch: 96 Loss: 0.2767018317301152 LR: 0.34510888860858707\n",
      "Epoch: 97 Loss: 0.27670086156546675 LR: 0.3381603010359903\n",
      "Epoch: 98 Loss: 0.2766972678971707 LR: 0.3494675255801436\n",
      "Epoch: 99 Loss: 0.2767017121664835 LR: 0.3405348998210657\n"
     ]
    }
   ],
   "source": [
    "train_adaptive_lr(model_lr_3, optimizer_lr_3, criterion, 100, loss_list_lr_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "H= np.asarray(ls_h)\n",
    "I= np.asarray(ls_i)\n",
    "J= np.asarray(ls_j)\n",
    "\n",
    "np.save(\"/home/aryamanj/Downloads/ls_h.npy\", H)\n",
    "np.save(\"/home/aryamanj/Downloads/ls_i.npy\", I)\n",
    "np.save(\"/home/aryamanj/Downloads/ls_j.npy\", J)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f631849613479c9658f644ddf8baf1a688d5be660ada6c6034305de03a8a6229"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
