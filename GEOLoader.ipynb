{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from adahessian import Adahessian, get_params_grad\n",
    "import torch.optim.lr_scheduler as lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the data-loader class for the GEO-dataset (can be changed to wrok for both the training and validation sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88807, 943)\n",
      "(88807, 4760)\n"
     ]
    }
   ],
   "source": [
    "def GEOdataLoader(path, bSize= 16):\n",
    "    \"\"\"\n",
    "    Specify the path to the parent folder containing the .npy files\n",
    "    \"\"\"\n",
    "    X_tr= np.load(os.path.join(path, \"X_tr.npy\"))\n",
    "    Y_tr= np.load(os.path.join(path, \"Y_tr.npy\"))\n",
    "    print(X_tr.shape)\n",
    "    print(Y_tr.shape)\n",
    "    test_data= torch.utils.data.TensorDataset(torch.from_numpy(X_tr).float(), torch.from_numpy(Y_tr).float())\n",
    "    trainLoader= torch.utils.data.DataLoader(test_data, batch_size=bSize, shuffle=True) \n",
    "    return trainLoader \n",
    "\n",
    "# loading the data\n",
    "trainLoader= GEOdataLoader(\"/home/aryamanj/Downloads/LGdata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardised class for all network architectures\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, size1, size2, drop):\n",
    "        super(Network, self).__init__()\n",
    "        self.net= nn.Sequential(\n",
    "            nn.Linear(943, size1),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(p= drop),\n",
    "            nn.Linear(size1, size2),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(p= drop),\n",
    "            nn.Linear(size2, 4760) \n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.net(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network class for the wide-medium architecture\n",
    "class wideM(nn.Module):\n",
    "    def __init__(self, size1, drop):\n",
    "        super(wideM, self).__init__()\n",
    "        self.net= nn.Sequential(\n",
    "            nn.Linear(943, size1),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(p= drop),\n",
    "            nn.Linear(size1, 4760) \n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.net(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the tiltedLC and it's corresponding torch class\n",
    "\n",
    "def tiltedLC(x, y, tau, h):\n",
    "    e= y-x # errors\n",
    "    ind= (torch.sign(e)+1)/2 # the division in the log-cosh is only about the origin\n",
    "    quantFactor= (1-tau)*(1-ind) + tau*ind\n",
    "    loss= quantFactor*torch.log(torch.cosh(e/h))\n",
    "    loss= torch.mean(loss)*h\n",
    "    return loss\n",
    "\n",
    "class TiltedLC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TiltedLC, self).__init__()\n",
    "    def forward(self, x, y, tau, h):\n",
    "        return tiltedLC(x, y, tau, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiations and training loops start for constant LR's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating all the loss functions, optimizer algos and models for training\n",
    "# the small architecture\n",
    "size1= 300\n",
    "size2= 300\n",
    "tau= 0.5\n",
    "h= 0.4\n",
    "device= ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion1= TiltedLC()\n",
    "model1= Network(size1, size2, 0.1).to(device)\n",
    "optimizer1= optim.SGD(model1.parameters(), lr= 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.13386895350536082\n",
      "Epoch: 1 Loss: 0.1336473765160732\n",
      "Epoch: 2 Loss: 0.13338832228092692\n",
      "Epoch: 3 Loss: 0.13315786012597522\n",
      "Epoch: 4 Loss: 0.13294416327660974\n",
      "Epoch: 5 Loss: 0.13271940638865035\n",
      "Epoch: 6 Loss: 0.13252280629437113\n",
      "Epoch: 7 Loss: 0.132285691769547\n",
      "Epoch: 8 Loss: 0.13209931786490697\n",
      "Epoch: 9 Loss: 0.13190648027893187\n",
      "Epoch: 10 Loss: 0.1317071426512851\n",
      "Epoch: 11 Loss: 0.1315119389885504\n",
      "Epoch: 12 Loss: 0.13133860254095517\n",
      "Epoch: 13 Loss: 0.1311479967038517\n",
      "Epoch: 14 Loss: 0.13099940435115634\n",
      "Epoch: 15 Loss: 0.13081443068052795\n",
      "Epoch: 16 Loss: 0.13064387180562106\n",
      "Epoch: 17 Loss: 0.13048930255314656\n",
      "Epoch: 18 Loss: 0.1303272681222523\n",
      "Epoch: 19 Loss: 0.13016274789124432\n",
      "Epoch: 20 Loss: 0.13001288522865037\n",
      "Epoch: 21 Loss: 0.12986963122030826\n",
      "Epoch: 22 Loss: 0.12971601760848064\n",
      "Epoch: 23 Loss: 0.12958107533819546\n",
      "Epoch: 24 Loss: 0.12943074332526344\n",
      "Epoch: 25 Loss: 0.12929609544216847\n",
      "Epoch: 26 Loss: 0.1291717951525076\n",
      "Epoch: 27 Loss: 0.1290396464190984\n",
      "Epoch: 28 Loss: 0.12892029407464198\n",
      "Epoch: 29 Loss: 0.12878950116704593\n",
      "Epoch: 30 Loss: 0.12865826072397157\n",
      "Epoch: 31 Loss: 0.1285403784659076\n",
      "Epoch: 32 Loss: 0.12841507644830277\n",
      "Epoch: 33 Loss: 0.12828562130849225\n",
      "Epoch: 34 Loss: 0.12818571276207097\n",
      "Epoch: 35 Loss: 0.1280845994375736\n",
      "Epoch: 36 Loss: 0.12796142431309235\n",
      "Epoch: 37 Loss: 0.12784762141769415\n",
      "Epoch: 38 Loss: 0.12774189869507308\n",
      "Epoch: 39 Loss: 0.1276477078619121\n",
      "Epoch: 40 Loss: 0.1275422168092628\n",
      "Epoch: 41 Loss: 0.1274579522902741\n",
      "Epoch: 42 Loss: 0.12736099419256006\n",
      "Epoch: 43 Loss: 0.12724866811739735\n",
      "Epoch: 44 Loss: 0.12715884541293734\n",
      "Epoch: 45 Loss: 0.12706557028982107\n",
      "Epoch: 46 Loss: 0.12695406854201\n",
      "Epoch: 47 Loss: 0.12688325016675953\n",
      "Epoch: 48 Loss: 0.12677709694526965\n",
      "Epoch: 49 Loss: 0.1267034133298615\n",
      "Epoch: 50 Loss: 0.12660725659571173\n",
      "Epoch: 51 Loss: 0.12654760225981856\n",
      "Epoch: 52 Loss: 0.12644886105468822\n",
      "Epoch: 53 Loss: 0.12636660214448445\n",
      "Epoch: 54 Loss: 0.12628847005195049\n",
      "Epoch: 55 Loss: 0.1262053702999033\n",
      "Epoch: 56 Loss: 0.12612022165564069\n",
      "Epoch: 57 Loss: 0.12604008630854957\n",
      "Epoch: 58 Loss: 0.12596162222023805\n",
      "Epoch: 59 Loss: 0.1259063170597674\n",
      "Epoch: 60 Loss: 0.1258336877505042\n",
      "Epoch: 61 Loss: 0.12574110096654384\n",
      "Epoch: 62 Loss: 0.12567060053015675\n",
      "Epoch: 63 Loss: 0.12561325134833295\n",
      "Epoch: 64 Loss: 0.1255379473490879\n",
      "Epoch: 65 Loss: 0.1254603491454828\n",
      "Epoch: 66 Loss: 0.12540896231796136\n",
      "Epoch: 67 Loss: 0.12535125148165358\n",
      "Epoch: 68 Loss: 0.12528894031784424\n",
      "Epoch: 69 Loss: 0.12521795415621667\n",
      "Epoch: 70 Loss: 0.12515213213682475\n",
      "Epoch: 71 Loss: 0.12509078083892403\n",
      "Epoch: 72 Loss: 0.1250319187999996\n",
      "Epoch: 73 Loss: 0.1249564815627371\n",
      "Epoch: 74 Loss: 0.12491157998578696\n",
      "Epoch: 75 Loss: 0.12485291488325032\n",
      "Epoch: 76 Loss: 0.12477348451731635\n",
      "Epoch: 77 Loss: 0.12473652243662516\n",
      "Epoch: 78 Loss: 0.12466312464111196\n",
      "Epoch: 79 Loss: 0.12461384633863709\n",
      "Epoch: 80 Loss: 0.124551809892002\n",
      "Epoch: 81 Loss: 0.12449328698401751\n",
      "Epoch: 82 Loss: 0.12443680445866807\n",
      "Epoch: 83 Loss: 0.12438827594821034\n",
      "Epoch: 84 Loss: 0.1243404454797093\n",
      "Epoch: 85 Loss: 0.12429856520110563\n",
      "Epoch: 86 Loss: 0.1242277859041571\n",
      "Epoch: 87 Loss: 0.12418888994212238\n",
      "Epoch: 88 Loss: 0.12412925309918846\n",
      "Epoch: 89 Loss: 0.12408628999526555\n",
      "Epoch: 90 Loss: 0.12404020144995903\n",
      "Epoch: 91 Loss: 0.12399606416621008\n",
      "Epoch: 92 Loss: 0.12393397954514898\n",
      "Epoch: 93 Loss: 0.12389642811158695\n",
      "Epoch: 94 Loss: 0.12386900447731812\n",
      "Epoch: 95 Loss: 0.12380590498544057\n",
      "Epoch: 96 Loss: 0.12376444034078404\n",
      "Epoch: 97 Loss: 0.12372604888412378\n",
      "Epoch: 98 Loss: 0.12366758317018367\n",
      "Epoch: 99 Loss: 0.1236159127884095\n",
      "Epoch: 100 Loss: 0.12358118991253934\n",
      "Epoch: 101 Loss: 0.12353399250313433\n",
      "Epoch: 102 Loss: 0.12349228695547704\n",
      "Epoch: 103 Loss: 0.12344692482231673\n",
      "Epoch: 104 Loss: 0.1234134847200619\n",
      "Epoch: 105 Loss: 0.12337608334771621\n",
      "Epoch: 106 Loss: 0.12331415203332342\n",
      "Epoch: 107 Loss: 0.12327466546211387\n",
      "Epoch: 108 Loss: 0.12324609871604081\n",
      "Epoch: 109 Loss: 0.12319865056527153\n",
      "Epoch: 110 Loss: 0.12316919753297431\n",
      "Epoch: 111 Loss: 0.12312057258910562\n",
      "Epoch: 112 Loss: 0.12307250419058814\n",
      "Epoch: 113 Loss: 0.12305240517365054\n",
      "Epoch: 114 Loss: 0.12300834844636822\n",
      "Epoch: 115 Loss: 0.12299130023615658\n",
      "Epoch: 116 Loss: 0.12293166130475151\n",
      "Epoch: 117 Loss: 0.12289702761304247\n",
      "Epoch: 118 Loss: 0.12287251639475674\n",
      "Epoch: 119 Loss: 0.1228352589484078\n",
      "Epoch: 120 Loss: 0.12277930635248212\n",
      "Epoch: 121 Loss: 0.12275920436390239\n",
      "Epoch: 122 Loss: 0.1227196941148524\n",
      "Epoch: 123 Loss: 0.12268330697443351\n",
      "Epoch: 124 Loss: 0.12265620184812304\n",
      "Epoch: 125 Loss: 0.12262933085741513\n",
      "Epoch: 126 Loss: 0.12258335005091324\n",
      "Epoch: 127 Loss: 0.12255564973448187\n",
      "Epoch: 128 Loss: 0.1225151404580046\n",
      "Epoch: 129 Loss: 0.12250069860477014\n",
      "Epoch: 130 Loss: 0.12245359875622837\n",
      "Epoch: 131 Loss: 0.12241944332844157\n",
      "Epoch: 132 Loss: 0.1224168238849645\n",
      "Epoch: 133 Loss: 0.12237351348278351\n",
      "Epoch: 134 Loss: 0.12234767201306089\n",
      "Epoch: 135 Loss: 0.12230693785548533\n",
      "Epoch: 136 Loss: 0.12228639534847562\n",
      "Epoch: 137 Loss: 0.12223935367781458\n",
      "Epoch: 138 Loss: 0.12221868876572493\n",
      "Epoch: 139 Loss: 0.12218255681473808\n",
      "Epoch: 140 Loss: 0.12215392817824494\n",
      "Epoch: 141 Loss: 0.12214132911760385\n",
      "Epoch: 142 Loss: 0.12211973860476\n",
      "Epoch: 143 Loss: 0.1220773966706802\n",
      "Epoch: 144 Loss: 0.12204055483009209\n",
      "Epoch: 145 Loss: 0.12200808158931893\n",
      "Epoch: 146 Loss: 0.1219959259006359\n",
      "Epoch: 147 Loss: 0.12194850982991108\n",
      "Epoch: 148 Loss: 0.12193881787280525\n",
      "Epoch: 149 Loss: 0.1219045766083169\n",
      "Epoch: 150 Loss: 0.12188784023530418\n",
      "Epoch: 151 Loss: 0.12184353298946364\n",
      "Epoch: 152 Loss: 0.12183178104681704\n",
      "Epoch: 153 Loss: 0.12181574956637592\n",
      "Epoch: 154 Loss: 0.1217608582842481\n",
      "Epoch: 155 Loss: 0.12174979193729014\n",
      "Epoch: 156 Loss: 0.12172335283329112\n",
      "Epoch: 157 Loss: 0.12170438008616796\n",
      "Epoch: 158 Loss: 0.1216563155491746\n",
      "Epoch: 159 Loss: 0.12164335209326665\n",
      "Epoch: 160 Loss: 0.12160388445520247\n",
      "Epoch: 161 Loss: 0.12160966410317564\n",
      "Epoch: 162 Loss: 0.12157947216011816\n",
      "Epoch: 163 Loss: 0.12155145434238228\n",
      "Epoch: 164 Loss: 0.12153139365345396\n",
      "Epoch: 165 Loss: 0.12149565981526865\n",
      "Epoch: 166 Loss: 0.12148673693900966\n",
      "Epoch: 167 Loss: 0.12145613987683992\n",
      "Epoch: 168 Loss: 0.12143458031658824\n",
      "Epoch: 169 Loss: 0.12141598942145165\n",
      "Epoch: 170 Loss: 0.12139006917905301\n",
      "Epoch: 171 Loss: 0.12137399213074822\n",
      "Epoch: 172 Loss: 0.12135472267710438\n",
      "Epoch: 173 Loss: 0.1213247511241269\n",
      "Epoch: 174 Loss: 0.12130073028450677\n",
      "Epoch: 175 Loss: 0.12129530291034604\n",
      "Epoch: 176 Loss: 0.1212669713065294\n",
      "Epoch: 177 Loss: 0.1212385103919875\n",
      "Epoch: 178 Loss: 0.12121590746053595\n",
      "Epoch: 179 Loss: 0.12120733301938279\n",
      "Epoch: 180 Loss: 0.12116806531216599\n",
      "Epoch: 181 Loss: 0.12114917429996555\n",
      "Epoch: 182 Loss: 0.12115028298018193\n",
      "Epoch: 183 Loss: 0.12112304199452745\n",
      "Epoch: 184 Loss: 0.12108883486499959\n",
      "Epoch: 185 Loss: 0.12109308705804928\n",
      "Epoch: 186 Loss: 0.1210736382066119\n",
      "Epoch: 187 Loss: 0.12103888246819171\n",
      "Epoch: 188 Loss: 0.12102321112151619\n",
      "Epoch: 189 Loss: 0.12100082579909563\n",
      "Epoch: 190 Loss: 0.1209772897693321\n",
      "Epoch: 191 Loss: 0.12095811981066909\n",
      "Epoch: 192 Loss: 0.12093995894999832\n",
      "Epoch: 193 Loss: 0.12091219657939783\n",
      "Epoch: 194 Loss: 0.12090872014198104\n",
      "Epoch: 195 Loss: 0.12088939146085208\n",
      "Epoch: 196 Loss: 0.12086642701584806\n",
      "Epoch: 197 Loss: 0.12083806329708961\n",
      "Epoch: 198 Loss: 0.12083248097474372\n",
      "Epoch: 199 Loss: 0.12083153427452509\n",
      "Epoch: 200 Loss: 0.12079554446037558\n",
      "Epoch: 201 Loss: 0.1207813120260365\n",
      "Epoch: 202 Loss: 0.12075210753254495\n",
      "Epoch: 203 Loss: 0.12073737001031438\n",
      "Epoch: 204 Loss: 0.12071228157940307\n",
      "Epoch: 205 Loss: 0.1207150386780946\n",
      "Epoch: 206 Loss: 0.1207004577294596\n",
      "Epoch: 207 Loss: 0.1206877322787316\n",
      "Epoch: 208 Loss: 0.12066236882596548\n",
      "Epoch: 209 Loss: 0.12064408178996061\n",
      "Epoch: 210 Loss: 0.12062406456834496\n",
      "Epoch: 211 Loss: 0.12062227897382259\n",
      "Epoch: 212 Loss: 0.12060831758304236\n",
      "Epoch: 213 Loss: 0.12056954934793085\n",
      "Epoch: 214 Loss: 0.1205735861788009\n",
      "Epoch: 215 Loss: 0.12055881212577371\n",
      "Epoch: 216 Loss: 0.12052094901205075\n",
      "Epoch: 217 Loss: 0.12051563091888833\n",
      "Epoch: 218 Loss: 0.12049755601437537\n",
      "Epoch: 219 Loss: 0.12049113167038782\n",
      "Epoch: 220 Loss: 0.12046010272822022\n",
      "Epoch: 221 Loss: 0.12045773685279139\n",
      "Epoch: 222 Loss: 0.12044594947995635\n",
      "Epoch: 223 Loss: 0.12042654958276615\n",
      "Epoch: 224 Loss: 0.12040301075853156\n",
      "Epoch: 225 Loss: 0.12039849563823737\n",
      "Epoch: 226 Loss: 0.12034839484723188\n",
      "Epoch: 227 Loss: 0.12037190698171775\n",
      "Epoch: 228 Loss: 0.12033830234927544\n",
      "Epoch: 229 Loss: 0.12030919316174253\n",
      "Epoch: 230 Loss: 0.12031388217947372\n",
      "Epoch: 231 Loss: 0.12030642594282444\n",
      "Epoch: 232 Loss: 0.12029553275240625\n",
      "Epoch: 233 Loss: 0.12028076326287646\n",
      "Epoch: 234 Loss: 0.12026498759733495\n",
      "Epoch: 235 Loss: 0.12025254364325923\n",
      "Epoch: 236 Loss: 0.12022585703797908\n",
      "Epoch: 237 Loss: 0.1202196231971902\n",
      "Epoch: 238 Loss: 0.12020071917902218\n",
      "Epoch: 239 Loss: 0.12020450376257384\n",
      "Epoch: 240 Loss: 0.12017612951153185\n",
      "Epoch: 241 Loss: 0.12017844174573324\n",
      "Epoch: 242 Loss: 0.12014210337433723\n",
      "Epoch: 243 Loss: 0.12012477094413912\n",
      "Epoch: 244 Loss: 0.12014092271031829\n",
      "Epoch: 245 Loss: 0.12012407798242879\n",
      "Epoch: 246 Loss: 0.12007383241983732\n",
      "Epoch: 247 Loss: 0.12010518583874942\n",
      "Epoch: 248 Loss: 0.12007088761107597\n",
      "Epoch: 249 Loss: 0.12006982360884126\n",
      "Epoch: 250 Loss: 0.12005957569024345\n",
      "Epoch: 251 Loss: 0.12003668895623122\n",
      "Epoch: 252 Loss: 0.12003178067050352\n",
      "Epoch: 253 Loss: 0.12003097850430057\n",
      "Epoch: 254 Loss: 0.12001411588213247\n",
      "Epoch: 255 Loss: 0.1199966806927107\n",
      "Epoch: 256 Loss: 0.11998142174258358\n",
      "Epoch: 257 Loss: 0.11996233846352694\n",
      "Epoch: 258 Loss: 0.11996528720494928\n",
      "Epoch: 259 Loss: 0.11993699921743008\n",
      "Epoch: 260 Loss: 0.11994037779601409\n",
      "Epoch: 261 Loss: 0.11993076220635272\n",
      "Epoch: 262 Loss: 0.11991698582646783\n",
      "Epoch: 263 Loss: 0.11989984898776798\n",
      "Epoch: 264 Loss: 0.11988698181897063\n",
      "Epoch: 265 Loss: 0.11987573830362183\n",
      "Epoch: 266 Loss: 0.11986659993767373\n",
      "Epoch: 267 Loss: 0.11985831751069213\n",
      "Epoch: 268 Loss: 0.11982860848498203\n",
      "Epoch: 269 Loss: 0.11982915115595895\n",
      "Epoch: 270 Loss: 0.11980108868037445\n",
      "Epoch: 271 Loss: 0.11980163041986103\n",
      "Epoch: 272 Loss: 0.11978794128196645\n",
      "Epoch: 273 Loss: 0.11978840127042382\n",
      "Epoch: 274 Loss: 0.11978952632956925\n",
      "Epoch: 275 Loss: 0.11976674956913318\n",
      "Epoch: 276 Loss: 0.11975658499610421\n",
      "Epoch: 277 Loss: 0.11971657100002565\n",
      "Epoch: 278 Loss: 0.11972680420134438\n",
      "Epoch: 279 Loss: 0.11972062643520082\n",
      "Epoch: 280 Loss: 0.11969671917566199\n",
      "Epoch: 281 Loss: 0.11969748229980254\n",
      "Epoch: 282 Loss: 0.11969430710824411\n",
      "Epoch: 283 Loss: 0.11968343626151448\n",
      "Epoch: 284 Loss: 0.11965410687583693\n",
      "Epoch: 285 Loss: 0.1196632699517812\n",
      "Epoch: 286 Loss: 0.11967558883113101\n",
      "Epoch: 287 Loss: 0.11966490977074816\n",
      "Epoch: 288 Loss: 0.11962027383498126\n",
      "Epoch: 289 Loss: 0.11963124344921654\n",
      "Epoch: 290 Loss: 0.11959496232018431\n",
      "Epoch: 291 Loss: 0.1195953628570083\n",
      "Epoch: 292 Loss: 0.11958963228676517\n",
      "Epoch: 293 Loss: 0.1195836989187245\n",
      "Epoch: 294 Loss: 0.11959069724733433\n",
      "Epoch: 295 Loss: 0.11956562075936414\n",
      "Epoch: 296 Loss: 0.11957720767118075\n",
      "Epoch: 297 Loss: 0.11954125117593103\n",
      "Epoch: 298 Loss: 0.11953692950900235\n",
      "Epoch: 299 Loss: 0.11952008858223774\n",
      "Epoch: 300 Loss: 0.11951810497085247\n",
      "Epoch: 301 Loss: 0.11950991178977341\n",
      "Epoch: 302 Loss: 0.11951374240022289\n",
      "Epoch: 303 Loss: 0.11948920126284292\n",
      "Epoch: 304 Loss: 0.11948466358276513\n",
      "Epoch: 305 Loss: 0.11947403622412334\n",
      "Epoch: 306 Loss: 0.11946251977404868\n",
      "Epoch: 307 Loss: 0.11944297945133267\n",
      "Epoch: 308 Loss: 0.11946573521833853\n",
      "Epoch: 309 Loss: 0.11944512382936229\n",
      "Epoch: 310 Loss: 0.11942107687311503\n",
      "Epoch: 311 Loss: 0.11940393915527082\n",
      "Epoch: 312 Loss: 0.11942224239571807\n",
      "Epoch: 313 Loss: 0.11939115963380974\n",
      "Epoch: 314 Loss: 0.11940035169601569\n",
      "Epoch: 315 Loss: 0.11940960674924993\n",
      "Epoch: 316 Loss: 0.11938498496144898\n",
      "Epoch: 317 Loss: 0.1193668465853983\n",
      "Epoch: 318 Loss: 0.11936382083227304\n",
      "Epoch: 319 Loss: 0.11934830470490812\n",
      "Epoch: 320 Loss: 0.11932308191523812\n",
      "Epoch: 321 Loss: 0.11931997130790545\n",
      "Epoch: 322 Loss: 0.11932911192607373\n",
      "Epoch: 323 Loss: 0.11931610835970811\n",
      "Epoch: 324 Loss: 0.11931512357441934\n",
      "Epoch: 325 Loss: 0.11929414229116189\n",
      "Epoch: 326 Loss: 0.11927470314252529\n",
      "Epoch: 327 Loss: 0.11929267430532045\n",
      "Epoch: 328 Loss: 0.11926561002768046\n",
      "Epoch: 329 Loss: 0.11926198039499838\n",
      "Epoch: 330 Loss: 0.11925898564278467\n",
      "Epoch: 331 Loss: 0.11924774299181597\n",
      "Epoch: 332 Loss: 0.11923428302336504\n",
      "Epoch: 333 Loss: 0.11926352229875996\n",
      "Epoch: 334 Loss: 0.11923024804071142\n",
      "Epoch: 335 Loss: 0.11923614189020687\n",
      "Epoch: 336 Loss: 0.11920710117124132\n",
      "Epoch: 337 Loss: 0.1192028300007535\n",
      "Epoch: 338 Loss: 0.11920045076233055\n",
      "Epoch: 339 Loss: 0.11917770396125486\n",
      "Epoch: 340 Loss: 0.1191838189390431\n",
      "Epoch: 341 Loss: 0.11918174977013495\n",
      "Epoch: 342 Loss: 0.11916645033910153\n",
      "Epoch: 343 Loss: 0.11915709427822133\n",
      "Epoch: 344 Loss: 0.11913053491130174\n",
      "Epoch: 345 Loss: 0.11915267017151382\n",
      "Epoch: 346 Loss: 0.11914993754122155\n",
      "Epoch: 347 Loss: 0.11911134161303397\n",
      "Epoch: 348 Loss: 0.11911788857953867\n",
      "Epoch: 349 Loss: 0.11913218321971605\n",
      "Epoch: 350 Loss: 0.11911420781555272\n",
      "Epoch: 351 Loss: 0.11909601779816022\n",
      "Epoch: 352 Loss: 0.11908999541507628\n",
      "Epoch: 353 Loss: 0.11907931442728689\n",
      "Epoch: 354 Loss: 0.11906367172863737\n",
      "Epoch: 355 Loss: 0.11906977441853778\n",
      "Epoch: 356 Loss: 0.11906368060061318\n",
      "Epoch: 357 Loss: 0.11905287677823821\n",
      "Epoch: 358 Loss: 0.11904013081593162\n",
      "Epoch: 359 Loss: 0.11903579100104758\n",
      "Epoch: 360 Loss: 0.11903203648154831\n",
      "Epoch: 361 Loss: 0.11900124161726848\n",
      "Epoch: 362 Loss: 0.11902241069112805\n",
      "Epoch: 363 Loss: 0.1190030771847717\n",
      "Epoch: 364 Loss: 0.11901369633717143\n",
      "Epoch: 365 Loss: 0.11900354682234167\n",
      "Epoch: 366 Loss: 0.1189919888914179\n",
      "Epoch: 367 Loss: 0.11896753576511178\n",
      "Epoch: 368 Loss: 0.11897183635405131\n",
      "Epoch: 369 Loss: 0.11897363002327965\n",
      "Epoch: 370 Loss: 0.11895008463431986\n",
      "Epoch: 371 Loss: 0.11894957677477482\n",
      "Epoch: 372 Loss: 0.11894717640870113\n",
      "Epoch: 373 Loss: 0.11893498134777324\n",
      "Epoch: 374 Loss: 0.11894562300224172\n",
      "Epoch: 375 Loss: 0.11893734574543423\n",
      "Epoch: 376 Loss: 0.1188980336863916\n",
      "Epoch: 377 Loss: 0.11891792388708393\n",
      "Epoch: 378 Loss: 0.11892006160240735\n",
      "Epoch: 379 Loss: 0.11890313632342005\n",
      "Epoch: 380 Loss: 0.1188917621631941\n",
      "Epoch: 381 Loss: 0.11889794853958348\n",
      "Epoch: 382 Loss: 0.118885130192155\n",
      "Epoch: 383 Loss: 0.11886141254056404\n",
      "Epoch: 384 Loss: 0.11888045912910165\n",
      "Epoch: 385 Loss: 0.11885356401225679\n",
      "Epoch: 386 Loss: 0.1188649034791649\n",
      "Epoch: 387 Loss: 0.11884296114483174\n",
      "Epoch: 388 Loss: 0.11885846016176584\n",
      "Epoch: 389 Loss: 0.11883431178984223\n",
      "Epoch: 390 Loss: 0.11882442300775162\n",
      "Epoch: 391 Loss: 0.11881684802621577\n",
      "Epoch: 392 Loss: 0.11883307211711759\n",
      "Epoch: 393 Loss: 0.11879104823196199\n",
      "Epoch: 394 Loss: 0.11878171023716262\n",
      "Epoch: 395 Loss: 0.11880839827054901\n",
      "Epoch: 396 Loss: 0.11879095621709028\n",
      "Epoch: 397 Loss: 0.11879577940969806\n",
      "Epoch: 398 Loss: 0.11878105683694808\n",
      "Epoch: 399 Loss: 0.11877748145888119\n",
      "Epoch: 400 Loss: 0.11875992317717046\n",
      "Epoch: 401 Loss: 0.11874777606786253\n",
      "Epoch: 402 Loss: 0.11874944633475254\n",
      "Epoch: 403 Loss: 0.11875410818356777\n",
      "Epoch: 404 Loss: 0.118725268274594\n",
      "Epoch: 405 Loss: 0.11872571465309151\n",
      "Epoch: 406 Loss: 0.11872353863928731\n",
      "Epoch: 407 Loss: 0.11872446919143619\n",
      "Epoch: 408 Loss: 0.1187290318056584\n",
      "Epoch: 409 Loss: 0.11871776002883869\n",
      "Epoch: 410 Loss: 0.1187204403319223\n",
      "Epoch: 411 Loss: 0.11869762907413035\n",
      "Epoch: 412 Loss: 0.11868737438410773\n",
      "Epoch: 413 Loss: 0.11868476709161357\n",
      "Epoch: 414 Loss: 0.11868549955442045\n",
      "Epoch: 415 Loss: 0.11866864784974876\n",
      "Epoch: 416 Loss: 0.11867207241764886\n",
      "Epoch: 417 Loss: 0.11866474334017003\n",
      "Epoch: 418 Loss: 0.11864827726445011\n",
      "Epoch: 419 Loss: 0.11865761688331766\n",
      "Epoch: 420 Loss: 0.11865507071216007\n",
      "Epoch: 421 Loss: 0.11864277004787588\n",
      "Epoch: 422 Loss: 0.11864038332397499\n",
      "Epoch: 423 Loss: 0.1186412798834371\n",
      "Epoch: 424 Loss: 0.11862458813337566\n",
      "Epoch: 425 Loss: 0.11861486211915721\n",
      "Epoch: 426 Loss: 0.11861965812808478\n",
      "Epoch: 427 Loss: 0.11859894067134928\n",
      "Epoch: 428 Loss: 0.1186079680355921\n",
      "Epoch: 429 Loss: 0.11859648068855097\n",
      "Epoch: 430 Loss: 0.1185904385133104\n",
      "Epoch: 431 Loss: 0.11860487860020293\n",
      "Epoch: 432 Loss: 0.1185739589054785\n",
      "Epoch: 433 Loss: 0.1185843970091724\n",
      "Epoch: 434 Loss: 0.1185666104975616\n",
      "Epoch: 435 Loss: 0.118569565442656\n",
      "Epoch: 436 Loss: 0.1185643137913312\n",
      "Epoch: 437 Loss: 0.11855925940120784\n",
      "Epoch: 438 Loss: 0.11855736499814337\n",
      "Epoch: 439 Loss: 0.11854975768154408\n",
      "Epoch: 440 Loss: 0.11853262607207536\n",
      "Epoch: 441 Loss: 0.11851989505790715\n",
      "Epoch: 442 Loss: 0.11853438381181496\n",
      "Epoch: 443 Loss: 0.11852713759536894\n",
      "Epoch: 444 Loss: 0.1185191316170062\n",
      "Epoch: 445 Loss: 0.11849339789682585\n",
      "Epoch: 446 Loss: 0.11851469095017196\n",
      "Epoch: 447 Loss: 0.11848705147187187\n",
      "Epoch: 448 Loss: 0.11849455633392808\n",
      "Epoch: 449 Loss: 0.1184996066236148\n",
      "Epoch: 450 Loss: 0.11849277394056491\n",
      "Epoch: 451 Loss: 0.11847465968528238\n",
      "Epoch: 452 Loss: 0.11848716130048986\n",
      "Epoch: 453 Loss: 0.11848177299792005\n",
      "Epoch: 454 Loss: 0.1184585044011836\n",
      "Epoch: 455 Loss: 0.11847034341670268\n",
      "Epoch: 456 Loss: 0.11846344652524447\n",
      "Epoch: 457 Loss: 0.11843944174645077\n",
      "Epoch: 458 Loss: 0.11845368700287531\n",
      "Epoch: 459 Loss: 0.118439949659684\n",
      "Epoch: 460 Loss: 0.11843094205404268\n",
      "Epoch: 461 Loss: 0.11843352568480287\n",
      "Epoch: 462 Loss: 0.11841422453766963\n",
      "Epoch: 463 Loss: 0.11841938249368192\n",
      "Epoch: 464 Loss: 0.11839643647849635\n",
      "Epoch: 465 Loss: 0.11841371202738386\n",
      "Epoch: 466 Loss: 0.11840926550316652\n",
      "Epoch: 467 Loss: 0.11839894926767051\n",
      "Epoch: 468 Loss: 0.1183953223261097\n",
      "Epoch: 469 Loss: 0.11838531114430798\n",
      "Epoch: 470 Loss: 0.11837961835579665\n",
      "Epoch: 471 Loss: 0.11836153427865394\n",
      "Epoch: 472 Loss: 0.1183806160960492\n",
      "Epoch: 473 Loss: 0.11834598619338521\n",
      "Epoch: 474 Loss: 0.11835547814179576\n",
      "Epoch: 475 Loss: 0.11834186379157022\n",
      "Epoch: 476 Loss: 0.11836563319590603\n",
      "Epoch: 477 Loss: 0.11834013565416444\n",
      "Epoch: 478 Loss: 0.11832511408911849\n",
      "Epoch: 479 Loss: 0.11833418758360895\n",
      "Epoch: 480 Loss: 0.11833028935420795\n",
      "Epoch: 481 Loss: 0.11831906619339971\n",
      "Epoch: 482 Loss: 0.11831637355276667\n",
      "Epoch: 483 Loss: 0.11830548170186919\n",
      "Epoch: 484 Loss: 0.11831878916897488\n",
      "Epoch: 485 Loss: 0.11831898258475866\n",
      "Epoch: 486 Loss: 0.11829020335016713\n",
      "Epoch: 487 Loss: 0.11829379859018532\n",
      "Epoch: 488 Loss: 0.11827644776909271\n",
      "Epoch: 489 Loss: 0.11828684112903685\n",
      "Epoch: 490 Loss: 0.11828986691840167\n",
      "Epoch: 491 Loss: 0.1182818853628032\n",
      "Epoch: 492 Loss: 0.11828221945242073\n",
      "Epoch: 493 Loss: 0.11827120673058376\n",
      "Epoch: 494 Loss: 0.11828012045300387\n",
      "Epoch: 495 Loss: 0.11826498762586486\n",
      "Epoch: 496 Loss: 0.11824551863866634\n",
      "Epoch: 497 Loss: 0.1182544103140252\n",
      "Epoch: 498 Loss: 0.11825186179669306\n",
      "Epoch: 499 Loss: 0.11824022409986149\n"
     ]
    }
   ],
   "source": [
    "# The fundamental training loop\n",
    "ls_a= []\n",
    "N_EPOCHS= 500\n",
    "# threshold= 0.5040 -- not using threshold values for now, instead just doing the training for the same number of EPOCHS for comparison\n",
    "for epoch in range(N_EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    for inputs, labels in trainLoader: \n",
    "        inputs= inputs.to(device) \n",
    "        labels= labels.to(device)\n",
    "        optimizer1.zero_grad() \n",
    "        outputs= model1(inputs) \n",
    "        loss= criterion1(outputs, labels, tau, h) \n",
    "        loss.backward() \n",
    "        optimizer1.step() \n",
    "        epoch_loss+= loss.item() \n",
    "    ls_a.append((epoch_loss/len(trainLoader)))\n",
    "    print(\"Epoch: {} Loss: {}\".format(epoch,\n",
    "           epoch_loss/len(trainLoader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the medium architecture \n",
    "size1= 1000\n",
    "size2= 1000\n",
    "tau= 0.5\n",
    "h= 0.4\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion2= TiltedLC()\n",
    "model2= Network(size1, size2, 0.1).to(device)\n",
    "optimizer2= optim.SGD(model2.parameters(), lr= 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.22437675972121016\n",
      "Epoch: 1 Loss: 0.18908993501724197\n",
      "Epoch: 2 Loss: 0.17391823054331743\n",
      "Epoch: 3 Loss: 0.1645287473692247\n",
      "Epoch: 4 Loss: 0.1580595013389736\n",
      "Epoch: 5 Loss: 0.15333088454234237\n",
      "Epoch: 6 Loss: 0.14964595208740775\n",
      "Epoch: 7 Loss: 0.14657005375756849\n",
      "Epoch: 8 Loss: 0.14395680926690507\n",
      "Epoch: 9 Loss: 0.1417040350850134\n",
      "Epoch: 10 Loss: 0.13972982083343424\n",
      "Epoch: 11 Loss: 0.13798714435045795\n",
      "Epoch: 12 Loss: 0.13643127892161677\n",
      "Epoch: 13 Loss: 0.13504712467090738\n",
      "Epoch: 14 Loss: 0.13380356526116038\n",
      "Epoch: 15 Loss: 0.13268313399871562\n",
      "Epoch: 16 Loss: 0.13165820268650696\n",
      "Epoch: 17 Loss: 0.13072460318305346\n",
      "Epoch: 18 Loss: 0.12984753040107944\n",
      "Epoch: 19 Loss: 0.12906806141411886\n",
      "Epoch: 20 Loss: 0.12830473296579167\n",
      "Epoch: 21 Loss: 0.12760660851179983\n",
      "Epoch: 22 Loss: 0.12694354597334817\n",
      "Epoch: 23 Loss: 0.12632951623731808\n",
      "Epoch: 24 Loss: 0.1257495993664084\n",
      "Epoch: 25 Loss: 0.12520848965455855\n",
      "Epoch: 26 Loss: 0.12469043632607872\n",
      "Epoch: 27 Loss: 0.1241981894102661\n",
      "Epoch: 28 Loss: 0.12373876608297472\n",
      "Epoch: 29 Loss: 0.12330116193598616\n",
      "Epoch: 30 Loss: 0.12285896964200745\n",
      "Epoch: 31 Loss: 0.12246928873942191\n",
      "Epoch: 32 Loss: 0.12208013219074008\n",
      "Epoch: 33 Loss: 0.12170944132422182\n",
      "Epoch: 34 Loss: 0.1213482718232236\n",
      "Epoch: 35 Loss: 0.1210080294730083\n",
      "Epoch: 36 Loss: 0.12068369802221095\n",
      "Epoch: 37 Loss: 0.1203735912706246\n",
      "Epoch: 38 Loss: 0.12006918939813324\n",
      "Epoch: 39 Loss: 0.11977331538019771\n",
      "Epoch: 40 Loss: 0.11950484929273116\n",
      "Epoch: 41 Loss: 0.11923183204273644\n",
      "Epoch: 42 Loss: 0.11896784890424966\n",
      "Epoch: 43 Loss: 0.11871211197870828\n",
      "Epoch: 44 Loss: 0.11847036317127757\n",
      "Epoch: 45 Loss: 0.11823206691940805\n",
      "Epoch: 46 Loss: 0.1180042439571395\n",
      "Epoch: 47 Loss: 0.11778709633063462\n",
      "Epoch: 48 Loss: 0.11756972634906147\n",
      "Epoch: 49 Loss: 0.11736209591516093\n",
      "Epoch: 50 Loss: 0.11716249368653987\n",
      "Epoch: 51 Loss: 0.11696422154829107\n",
      "Epoch: 52 Loss: 0.11677896140545602\n",
      "Epoch: 53 Loss: 0.11659447396205279\n",
      "Epoch: 54 Loss: 0.11641268289195505\n",
      "Epoch: 55 Loss: 0.11623271543568178\n",
      "Epoch: 56 Loss: 0.11605668007264458\n",
      "Epoch: 57 Loss: 0.11589698441671564\n",
      "Epoch: 58 Loss: 0.11574004064098475\n",
      "Epoch: 59 Loss: 0.11558328371497022\n",
      "Epoch: 60 Loss: 0.11542911201425553\n",
      "Epoch: 61 Loss: 0.11527776128911603\n",
      "Epoch: 62 Loss: 0.11514015581275595\n",
      "Epoch: 63 Loss: 0.11499795902851272\n",
      "Epoch: 64 Loss: 0.11484592189565046\n",
      "Epoch: 65 Loss: 0.11472566644713977\n",
      "Epoch: 66 Loss: 0.11458222148596027\n",
      "Epoch: 67 Loss: 0.11444303970377932\n",
      "Epoch: 68 Loss: 0.11433071839612918\n",
      "Epoch: 69 Loss: 0.1141979187187408\n",
      "Epoch: 70 Loss: 0.11407349393821326\n",
      "Epoch: 71 Loss: 0.11396365435037328\n",
      "Epoch: 72 Loss: 0.11384301153341299\n",
      "Epoch: 73 Loss: 0.11372311540931823\n",
      "Epoch: 74 Loss: 0.11360921018017539\n",
      "Epoch: 75 Loss: 0.11350718766069695\n",
      "Epoch: 76 Loss: 0.1133900940206175\n",
      "Epoch: 77 Loss: 0.1132889800558883\n",
      "Epoch: 78 Loss: 0.11319891560686492\n",
      "Epoch: 79 Loss: 0.11308913731619037\n",
      "Epoch: 80 Loss: 0.1129887402916659\n",
      "Epoch: 81 Loss: 0.11289370845425474\n",
      "Epoch: 82 Loss: 0.11279948243116006\n",
      "Epoch: 83 Loss: 0.11270114523782276\n",
      "Epoch: 84 Loss: 0.11262995507443967\n",
      "Epoch: 85 Loss: 0.11252258365684921\n",
      "Epoch: 86 Loss: 0.1124307819555051\n",
      "Epoch: 87 Loss: 0.11233986630952587\n",
      "Epoch: 88 Loss: 0.11226586040785196\n",
      "Epoch: 89 Loss: 0.11217360408418653\n",
      "Epoch: 90 Loss: 0.11209549854852298\n",
      "Epoch: 91 Loss: 0.11201442313975975\n",
      "Epoch: 92 Loss: 0.11193278728519726\n",
      "Epoch: 93 Loss: 0.11185304728596517\n",
      "Epoch: 94 Loss: 0.11178001024163192\n",
      "Epoch: 95 Loss: 0.11169829540635161\n",
      "Epoch: 96 Loss: 0.11162968615442112\n",
      "Epoch: 97 Loss: 0.11155942542513005\n",
      "Epoch: 98 Loss: 0.11148463840034498\n",
      "Epoch: 99 Loss: 0.1114166050054129\n",
      "Epoch: 100 Loss: 0.11134049209269978\n",
      "Epoch: 101 Loss: 0.1112743230166467\n",
      "Epoch: 102 Loss: 0.11120715364075592\n",
      "Epoch: 103 Loss: 0.11114426892719877\n",
      "Epoch: 104 Loss: 0.11107171205943006\n",
      "Epoch: 105 Loss: 0.11101853908842436\n",
      "Epoch: 106 Loss: 0.1109458285354855\n",
      "Epoch: 107 Loss: 0.11087576584721702\n",
      "Epoch: 108 Loss: 0.11082568462019331\n",
      "Epoch: 109 Loss: 0.11076226874909473\n",
      "Epoch: 110 Loss: 0.110709014224364\n",
      "Epoch: 111 Loss: 0.11064725572156425\n",
      "Epoch: 112 Loss: 0.11058607771845042\n",
      "Epoch: 113 Loss: 0.11052760232721143\n",
      "Epoch: 114 Loss: 0.11046320964083803\n",
      "Epoch: 115 Loss: 0.11042137573676332\n",
      "Epoch: 116 Loss: 0.11034838149803562\n",
      "Epoch: 117 Loss: 0.11030100227755225\n",
      "Epoch: 118 Loss: 0.11025153029085258\n",
      "Epoch: 119 Loss: 0.11018453726192312\n",
      "Epoch: 120 Loss: 0.11014518082823717\n",
      "Epoch: 121 Loss: 0.11008951268852606\n",
      "Epoch: 122 Loss: 0.11003688409175556\n",
      "Epoch: 123 Loss: 0.1099880975034404\n",
      "Epoch: 124 Loss: 0.10992945457950108\n",
      "Epoch: 125 Loss: 0.10988773495882315\n",
      "Epoch: 126 Loss: 0.10982901715997145\n",
      "Epoch: 127 Loss: 0.10977584882896885\n",
      "Epoch: 128 Loss: 0.10974331806933767\n",
      "Epoch: 129 Loss: 0.10968949247256976\n",
      "Epoch: 130 Loss: 0.10963051506708901\n",
      "Epoch: 131 Loss: 0.10959498360390664\n",
      "Epoch: 132 Loss: 0.10953836728486686\n",
      "Epoch: 133 Loss: 0.1094984610195397\n",
      "Epoch: 134 Loss: 0.10943938726789133\n",
      "Epoch: 135 Loss: 0.10939850493355541\n",
      "Epoch: 136 Loss: 0.10934587636362902\n",
      "Epoch: 137 Loss: 0.10930654026837719\n",
      "Epoch: 138 Loss: 0.10925342586851833\n",
      "Epoch: 139 Loss: 0.10921663112859215\n",
      "Epoch: 140 Loss: 0.10915999677300002\n",
      "Epoch: 141 Loss: 0.10911543884485224\n",
      "Epoch: 142 Loss: 0.10908566336388245\n",
      "Epoch: 143 Loss: 0.10903646918545541\n",
      "Epoch: 144 Loss: 0.10899448471549524\n",
      "Epoch: 145 Loss: 0.1089488835020487\n",
      "Epoch: 146 Loss: 0.1089079679293453\n",
      "Epoch: 147 Loss: 0.10887087463870862\n",
      "Epoch: 148 Loss: 0.10881716318440596\n",
      "Epoch: 149 Loss: 0.10878747515749489\n",
      "Epoch: 150 Loss: 0.108746420188737\n",
      "Epoch: 151 Loss: 0.10870090012769616\n",
      "Epoch: 152 Loss: 0.1086638025889803\n",
      "Epoch: 153 Loss: 0.10863011811009786\n",
      "Epoch: 154 Loss: 0.10858617209728892\n",
      "Epoch: 155 Loss: 0.10854658175885967\n",
      "Epoch: 156 Loss: 0.10851242100654518\n",
      "Epoch: 157 Loss: 0.1084746895456954\n",
      "Epoch: 158 Loss: 0.10844024898388047\n",
      "Epoch: 159 Loss: 0.10839593050244091\n",
      "Epoch: 160 Loss: 0.10837539329445699\n",
      "Epoch: 161 Loss: 0.1083246696800117\n",
      "Epoch: 162 Loss: 0.10829938207123005\n",
      "Epoch: 163 Loss: 0.10827034201531384\n",
      "Epoch: 164 Loss: 0.10822933698463946\n",
      "Epoch: 165 Loss: 0.10818561837627445\n",
      "Epoch: 166 Loss: 0.1081552472676249\n",
      "Epoch: 167 Loss: 0.1081233897258668\n",
      "Epoch: 168 Loss: 0.10809062871094803\n",
      "Epoch: 169 Loss: 0.10806069916362183\n",
      "Epoch: 170 Loss: 0.10802475052966747\n",
      "Epoch: 171 Loss: 0.1079973466177606\n",
      "Epoch: 172 Loss: 0.1079551073757942\n",
      "Epoch: 173 Loss: 0.1079268540480377\n",
      "Epoch: 174 Loss: 0.10789610658910033\n",
      "Epoch: 175 Loss: 0.1078639848033945\n",
      "Epoch: 176 Loss: 0.10784425972882873\n",
      "Epoch: 177 Loss: 0.10779987666987445\n",
      "Epoch: 178 Loss: 0.10776501350806782\n",
      "Epoch: 179 Loss: 0.10773837149621043\n",
      "Epoch: 180 Loss: 0.10770459242273807\n",
      "Epoch: 181 Loss: 0.10767997168319932\n",
      "Epoch: 182 Loss: 0.1076612065926562\n",
      "Epoch: 183 Loss: 0.10762505582395362\n",
      "Epoch: 184 Loss: 0.10759126845565363\n",
      "Epoch: 185 Loss: 0.10756805132845676\n",
      "Epoch: 186 Loss: 0.10753962745072283\n",
      "Epoch: 187 Loss: 0.10750483534391282\n",
      "Epoch: 188 Loss: 0.10747796438541783\n",
      "Epoch: 189 Loss: 0.1074527211995989\n",
      "Epoch: 190 Loss: 0.10742651164622978\n",
      "Epoch: 191 Loss: 0.10739189745541887\n",
      "Epoch: 192 Loss: 0.10736377799651618\n",
      "Epoch: 193 Loss: 0.10733626995230984\n",
      "Epoch: 194 Loss: 0.1073093586551823\n",
      "Epoch: 195 Loss: 0.10728222243669032\n",
      "Epoch: 196 Loss: 0.10726169138137466\n",
      "Epoch: 197 Loss: 0.10723505827907484\n",
      "Epoch: 198 Loss: 0.10721156846009124\n",
      "Epoch: 199 Loss: 0.1071757096465032\n",
      "Epoch: 200 Loss: 0.10715806678171051\n",
      "Epoch: 201 Loss: 0.10712363006799763\n",
      "Epoch: 202 Loss: 0.10710888917012121\n",
      "Epoch: 203 Loss: 0.10707987646968746\n",
      "Epoch: 204 Loss: 0.1070583802776237\n",
      "Epoch: 205 Loss: 0.10702953973781351\n",
      "Epoch: 206 Loss: 0.10700079857352789\n",
      "Epoch: 207 Loss: 0.106974950578004\n",
      "Epoch: 208 Loss: 0.1069610524279022\n",
      "Epoch: 209 Loss: 0.10692656611031486\n",
      "Epoch: 210 Loss: 0.1069107714364645\n",
      "Epoch: 211 Loss: 0.10688916559563824\n",
      "Epoch: 212 Loss: 0.10686136614259482\n",
      "Epoch: 213 Loss: 0.10683445041753262\n",
      "Epoch: 214 Loss: 0.10680001393602122\n",
      "Epoch: 215 Loss: 0.10678123350407363\n",
      "Epoch: 216 Loss: 0.10676204547261874\n",
      "Epoch: 217 Loss: 0.10673315114125413\n",
      "Epoch: 218 Loss: 0.1067095223637749\n",
      "Epoch: 219 Loss: 0.10669131989206114\n",
      "Epoch: 220 Loss: 0.106673595784935\n",
      "Epoch: 221 Loss: 0.106648922406796\n",
      "Epoch: 222 Loss: 0.10662538166589253\n",
      "Epoch: 223 Loss: 0.10660382359548853\n",
      "Epoch: 224 Loss: 0.10658711524906082\n",
      "Epoch: 225 Loss: 0.10654986578346351\n",
      "Epoch: 226 Loss: 0.10652429266080966\n",
      "Epoch: 227 Loss: 0.10650854867130888\n",
      "Epoch: 228 Loss: 0.10648552326603293\n",
      "Epoch: 229 Loss: 0.10647350061135213\n",
      "Epoch: 230 Loss: 0.10643862943255435\n",
      "Epoch: 231 Loss: 0.10643680478120533\n",
      "Epoch: 232 Loss: 0.10640246076286215\n",
      "Epoch: 233 Loss: 0.10637019622156242\n",
      "Epoch: 234 Loss: 0.10635806026732121\n",
      "Epoch: 235 Loss: 0.10633718663025624\n",
      "Epoch: 236 Loss: 0.10631349887494203\n",
      "Epoch: 237 Loss: 0.1062986566472904\n",
      "Epoch: 238 Loss: 0.1062760770030976\n",
      "Epoch: 239 Loss: 0.10625600765023527\n",
      "Epoch: 240 Loss: 0.10624288372431032\n",
      "Epoch: 241 Loss: 0.10621048867638058\n",
      "Epoch: 242 Loss: 0.10620842765599967\n",
      "Epoch: 243 Loss: 0.10617292191483613\n",
      "Epoch: 244 Loss: 0.10615019051422193\n",
      "Epoch: 245 Loss: 0.10614316964998823\n",
      "Epoch: 246 Loss: 0.1061150403379578\n",
      "Epoch: 247 Loss: 0.10608765261304462\n",
      "Epoch: 248 Loss: 0.10607416915331976\n",
      "Epoch: 249 Loss: 0.10605157684944831\n",
      "Epoch: 250 Loss: 0.10602398370541102\n",
      "Epoch: 251 Loss: 0.10600600496586798\n",
      "Epoch: 252 Loss: 0.10598364997459862\n",
      "Epoch: 253 Loss: 0.10597007444228725\n",
      "Epoch: 254 Loss: 0.10594691121939001\n",
      "Epoch: 255 Loss: 0.10593291010336497\n",
      "Epoch: 256 Loss: 0.10591104184667434\n",
      "Epoch: 257 Loss: 0.10588901968334971\n",
      "Epoch: 258 Loss: 0.1058773661015206\n",
      "Epoch: 259 Loss: 0.10585220716725897\n",
      "Epoch: 260 Loss: 0.10584528310822702\n",
      "Epoch: 261 Loss: 0.1058166357466303\n",
      "Epoch: 262 Loss: 0.10579308462242805\n",
      "Epoch: 263 Loss: 0.10576974196683993\n",
      "Epoch: 264 Loss: 0.10575798227161658\n",
      "Epoch: 265 Loss: 0.10573814695369142\n",
      "Epoch: 266 Loss: 0.10572525002015344\n",
      "Epoch: 267 Loss: 0.1056959773117713\n",
      "Epoch: 268 Loss: 0.10567749237030653\n",
      "Epoch: 269 Loss: 0.10565353206816375\n",
      "Epoch: 270 Loss: 0.10564268850712191\n",
      "Epoch: 271 Loss: 0.10562832725598906\n",
      "Epoch: 272 Loss: 0.10560528749314499\n",
      "Epoch: 273 Loss: 0.10558517276569651\n",
      "Epoch: 274 Loss: 0.1055710397364105\n",
      "Epoch: 275 Loss: 0.10555433196578878\n",
      "Epoch: 276 Loss: 0.1055274654674414\n",
      "Epoch: 277 Loss: 0.10550893819148509\n",
      "Epoch: 278 Loss: 0.10550162118982569\n",
      "Epoch: 279 Loss: 0.10547811132635089\n",
      "Epoch: 280 Loss: 0.10547272042258755\n",
      "Epoch: 281 Loss: 0.10542449770537911\n",
      "Epoch: 282 Loss: 0.10543587191124001\n",
      "Epoch: 283 Loss: 0.10541756708967516\n",
      "Epoch: 284 Loss: 0.10539525791591964\n",
      "Epoch: 285 Loss: 0.10537992452172798\n",
      "Epoch: 286 Loss: 0.10536640521304669\n",
      "Epoch: 287 Loss: 0.10535273263796324\n",
      "Epoch: 288 Loss: 0.10532216415134041\n",
      "Epoch: 289 Loss: 0.1053115073527716\n",
      "Epoch: 290 Loss: 0.10529286281909198\n",
      "Epoch: 291 Loss: 0.10527707197857943\n",
      "Epoch: 292 Loss: 0.10525902624152583\n",
      "Epoch: 293 Loss: 0.10524171381152615\n",
      "Epoch: 294 Loss: 0.10522263674151036\n",
      "Epoch: 295 Loss: 0.10520226586226879\n",
      "Epoch: 296 Loss: 0.10519144562472525\n",
      "Epoch: 297 Loss: 0.10518188232469078\n",
      "Epoch: 298 Loss: 0.10515219023492482\n",
      "Epoch: 299 Loss: 0.10514817102452352\n",
      "Epoch: 300 Loss: 0.10512874528585824\n",
      "Epoch: 301 Loss: 0.10510874345128203\n",
      "Epoch: 302 Loss: 0.10509479102911637\n",
      "Epoch: 303 Loss: 0.10507774511004886\n",
      "Epoch: 304 Loss: 0.10506441808701464\n",
      "Epoch: 305 Loss: 0.10505625594442888\n",
      "Epoch: 306 Loss: 0.10504150340694969\n",
      "Epoch: 307 Loss: 0.10501506518209502\n",
      "Epoch: 308 Loss: 0.10499982961854984\n",
      "Epoch: 309 Loss: 0.10498510515329065\n",
      "Epoch: 310 Loss: 0.10496433040184537\n",
      "Epoch: 311 Loss: 0.10495415837823582\n",
      "Epoch: 312 Loss: 0.10494111719254523\n",
      "Epoch: 313 Loss: 0.10492384178861587\n",
      "Epoch: 314 Loss: 0.1049036729904343\n",
      "Epoch: 315 Loss: 0.10489900903704144\n",
      "Epoch: 316 Loss: 0.10487566283668963\n",
      "Epoch: 317 Loss: 0.10486174755199731\n",
      "Epoch: 318 Loss: 0.10485323681592555\n",
      "Epoch: 319 Loss: 0.10484267620648635\n",
      "Epoch: 320 Loss: 0.10483485060170027\n",
      "Epoch: 321 Loss: 0.10480975620200281\n",
      "Epoch: 322 Loss: 0.10478753032012794\n",
      "Epoch: 323 Loss: 0.10477110703793373\n",
      "Epoch: 324 Loss: 0.10475765485427246\n",
      "Epoch: 325 Loss: 0.10474328924499961\n",
      "Epoch: 326 Loss: 0.10473480895550868\n",
      "Epoch: 327 Loss: 0.10472305356876374\n",
      "Epoch: 328 Loss: 0.10470465527066582\n",
      "Epoch: 329 Loss: 0.1046985515298188\n",
      "Epoch: 330 Loss: 0.10468342855446533\n",
      "Epoch: 331 Loss: 0.10466841440510265\n",
      "Epoch: 332 Loss: 0.10465447642237963\n",
      "Epoch: 333 Loss: 0.10462998242995435\n",
      "Epoch: 334 Loss: 0.10463068552128668\n",
      "Epoch: 335 Loss: 0.10461206910277762\n",
      "Epoch: 336 Loss: 0.10460303099149842\n",
      "Epoch: 337 Loss: 0.1045833742767814\n",
      "Epoch: 338 Loss: 0.1045793435220739\n",
      "Epoch: 339 Loss: 0.10456783198849658\n",
      "Epoch: 340 Loss: 0.10455367927308813\n",
      "Epoch: 341 Loss: 0.10453853264271172\n",
      "Epoch: 342 Loss: 0.10452156744922868\n",
      "Epoch: 343 Loss: 0.10451488220069971\n",
      "Epoch: 344 Loss: 0.1044995461113602\n",
      "Epoch: 345 Loss: 0.10448127935875903\n",
      "Epoch: 346 Loss: 0.10446753305634858\n",
      "Epoch: 347 Loss: 0.10446039179094717\n",
      "Epoch: 348 Loss: 0.10443396998126106\n",
      "Epoch: 349 Loss: 0.10443606916187562\n",
      "Epoch: 350 Loss: 0.10440961225755235\n",
      "Epoch: 351 Loss: 0.10440282733016047\n",
      "Epoch: 352 Loss: 0.104403758102431\n",
      "Epoch: 353 Loss: 0.10438521198057266\n",
      "Epoch: 354 Loss: 0.10438371549300558\n",
      "Epoch: 355 Loss: 0.10435725158774044\n",
      "Epoch: 356 Loss: 0.10435172913311151\n",
      "Epoch: 357 Loss: 0.10435394840484438\n",
      "Epoch: 358 Loss: 0.10431336654459887\n",
      "Epoch: 359 Loss: 0.10430960362557999\n",
      "Epoch: 360 Loss: 0.104299101949683\n",
      "Epoch: 361 Loss: 0.10428103685266284\n",
      "Epoch: 362 Loss: 0.10427738786269858\n",
      "Epoch: 363 Loss: 0.10427134509151893\n",
      "Epoch: 364 Loss: 0.10426088568031884\n",
      "Epoch: 365 Loss: 0.10423765175945156\n",
      "Epoch: 366 Loss: 0.10423408907461845\n",
      "Epoch: 367 Loss: 0.104232853178859\n",
      "Epoch: 368 Loss: 0.10420503264379982\n",
      "Epoch: 369 Loss: 0.10419467000021974\n",
      "Epoch: 370 Loss: 0.104193701750898\n",
      "Epoch: 371 Loss: 0.1041729863463941\n",
      "Epoch: 372 Loss: 0.1041615425592778\n",
      "Epoch: 373 Loss: 0.10414783136358434\n",
      "Epoch: 374 Loss: 0.10413320037150271\n",
      "Epoch: 375 Loss: 0.10412535069493899\n",
      "Epoch: 376 Loss: 0.10411631351246574\n",
      "Epoch: 377 Loss: 0.10409893608886774\n",
      "Epoch: 378 Loss: 0.10409995265757366\n",
      "Epoch: 379 Loss: 0.1040852569584545\n",
      "Epoch: 380 Loss: 0.10406209147259944\n",
      "Epoch: 381 Loss: 0.10406899646292332\n",
      "Epoch: 382 Loss: 0.10405690858915376\n",
      "Epoch: 383 Loss: 0.1040485061381641\n",
      "Epoch: 384 Loss: 0.10402999753764977\n",
      "Epoch: 385 Loss: 0.10401669231202662\n",
      "Epoch: 386 Loss: 0.10401157019289092\n",
      "Epoch: 387 Loss: 0.10399537600363468\n",
      "Epoch: 388 Loss: 0.10398226116980717\n",
      "Epoch: 389 Loss: 0.10397271331855358\n",
      "Epoch: 390 Loss: 0.10396347595692497\n",
      "Epoch: 391 Loss: 0.10395403796516009\n",
      "Epoch: 392 Loss: 0.10394375530895167\n",
      "Epoch: 393 Loss: 0.10393696218739756\n",
      "Epoch: 394 Loss: 0.10392642906766135\n",
      "Epoch: 395 Loss: 0.10391103566213677\n",
      "Epoch: 396 Loss: 0.10389618678788541\n",
      "Epoch: 397 Loss: 0.10389402443236047\n",
      "Epoch: 398 Loss: 0.10388463214804859\n",
      "Epoch: 399 Loss: 0.10386501948718788\n",
      "Epoch: 400 Loss: 0.10387351593414398\n",
      "Epoch: 401 Loss: 0.10385273038331634\n",
      "Epoch: 402 Loss: 0.10385656069311187\n",
      "Epoch: 403 Loss: 0.10383146293790163\n",
      "Epoch: 404 Loss: 0.10381976812487356\n",
      "Epoch: 405 Loss: 0.1038146670300388\n",
      "Epoch: 406 Loss: 0.10380220881637593\n",
      "Epoch: 407 Loss: 0.10379210337553556\n",
      "Epoch: 408 Loss: 0.10379399948588493\n",
      "Epoch: 409 Loss: 0.10377758855063557\n",
      "Epoch: 410 Loss: 0.10376139487007506\n",
      "Epoch: 411 Loss: 0.10375027183239964\n",
      "Epoch: 412 Loss: 0.10375207416287759\n",
      "Epoch: 413 Loss: 0.10374293216760685\n",
      "Epoch: 414 Loss: 0.10373286439575305\n",
      "Epoch: 415 Loss: 0.10371785136042061\n",
      "Epoch: 416 Loss: 0.10371983635964598\n",
      "Epoch: 417 Loss: 0.10370499559634012\n",
      "Epoch: 418 Loss: 0.10369022808522066\n",
      "Epoch: 419 Loss: 0.10368483315349979\n",
      "Epoch: 420 Loss: 0.10367688991785737\n",
      "Epoch: 421 Loss: 0.10366823436724905\n",
      "Epoch: 422 Loss: 0.1036544964093278\n",
      "Epoch: 423 Loss: 0.10365294892870398\n",
      "Epoch: 424 Loss: 0.1036419560902936\n",
      "Epoch: 425 Loss: 0.10362868588414413\n",
      "Epoch: 426 Loss: 0.10362509033541877\n",
      "Epoch: 427 Loss: 0.10361159689639886\n",
      "Epoch: 428 Loss: 0.10360263308556826\n",
      "Epoch: 429 Loss: 0.10360126089424813\n",
      "Epoch: 430 Loss: 0.10358504736247395\n",
      "Epoch: 431 Loss: 0.10358518587133086\n",
      "Epoch: 432 Loss: 0.10357052905052508\n",
      "Epoch: 433 Loss: 0.10356416936995338\n",
      "Epoch: 434 Loss: 0.10354308824005738\n",
      "Epoch: 435 Loss: 0.10355072990100118\n",
      "Epoch: 436 Loss: 0.10353373186778558\n",
      "Epoch: 437 Loss: 0.10352207076692341\n",
      "Epoch: 438 Loss: 0.10352307570810641\n",
      "Epoch: 439 Loss: 0.10350782964449061\n",
      "Epoch: 440 Loss: 0.10349307298509951\n",
      "Epoch: 441 Loss: 0.10348636697936844\n",
      "Epoch: 442 Loss: 0.10348383011506111\n",
      "Epoch: 443 Loss: 0.10347291810290059\n",
      "Epoch: 444 Loss: 0.10345944900745488\n",
      "Epoch: 445 Loss: 0.10345763568431703\n",
      "Epoch: 446 Loss: 0.10345417933836862\n",
      "Epoch: 447 Loss: 0.10343541322774961\n",
      "Epoch: 448 Loss: 0.10343315325901543\n",
      "Epoch: 449 Loss: 0.1034249129866699\n",
      "Epoch: 450 Loss: 0.10342587973809075\n",
      "Epoch: 451 Loss: 0.10341541652845018\n",
      "Epoch: 452 Loss: 0.1033933641252443\n",
      "Epoch: 453 Loss: 0.10339549928500918\n",
      "Epoch: 454 Loss: 0.10338105304544841\n",
      "Epoch: 455 Loss: 0.10337371743143066\n",
      "Epoch: 456 Loss: 0.10336548910471066\n",
      "Epoch: 457 Loss: 0.103357460793397\n",
      "Epoch: 458 Loss: 0.10334328100436917\n",
      "Epoch: 459 Loss: 0.10334906246297704\n",
      "Epoch: 460 Loss: 0.10334614187162258\n",
      "Epoch: 461 Loss: 0.10333365458713911\n",
      "Epoch: 462 Loss: 0.10332164422839939\n",
      "Epoch: 463 Loss: 0.10331137001331894\n",
      "Epoch: 464 Loss: 0.10331402301192391\n",
      "Epoch: 465 Loss: 0.10329656467483271\n",
      "Epoch: 466 Loss: 0.10328089032027554\n",
      "Epoch: 467 Loss: 0.10327118982674517\n",
      "Epoch: 468 Loss: 0.10327870119849619\n",
      "Epoch: 469 Loss: 0.1032566529400197\n",
      "Epoch: 470 Loss: 0.1032649866580276\n",
      "Epoch: 471 Loss: 0.10325614679312839\n",
      "Epoch: 472 Loss: 0.1032511410109633\n",
      "Epoch: 473 Loss: 0.10324398431422968\n",
      "Epoch: 474 Loss: 0.10323796826501168\n",
      "Epoch: 475 Loss: 0.10321770024834356\n",
      "Epoch: 476 Loss: 0.10321054643600873\n",
      "Epoch: 477 Loss: 0.1031965388143155\n",
      "Epoch: 478 Loss: 0.10319795284216908\n",
      "Epoch: 479 Loss: 0.10318651434334092\n",
      "Epoch: 480 Loss: 0.10318289706565693\n",
      "Epoch: 481 Loss: 0.10317094712225387\n",
      "Epoch: 482 Loss: 0.1031670493465182\n",
      "Epoch: 483 Loss: 0.10315950221171187\n",
      "Epoch: 484 Loss: 0.10315316906116649\n",
      "Epoch: 485 Loss: 0.1031505773704646\n",
      "Epoch: 486 Loss: 0.10313939196320186\n",
      "Epoch: 487 Loss: 0.10314111693703018\n",
      "Epoch: 488 Loss: 0.10312126013979657\n",
      "Epoch: 489 Loss: 0.10312177732900937\n",
      "Epoch: 490 Loss: 0.10311698762441923\n",
      "Epoch: 491 Loss: 0.10311244048447979\n",
      "Epoch: 492 Loss: 0.10310488798756402\n",
      "Epoch: 493 Loss: 0.10308231818576134\n",
      "Epoch: 494 Loss: 0.10308262460850484\n",
      "Epoch: 495 Loss: 0.10306773597276914\n",
      "Epoch: 496 Loss: 0.10306390788298087\n",
      "Epoch: 497 Loss: 0.10307109261837935\n",
      "Epoch: 498 Loss: 0.10306727243954804\n",
      "Epoch: 499 Loss: 0.10304578343365046\n"
     ]
    }
   ],
   "source": [
    "# training loop for medium architecture\n",
    "ls_b= []\n",
    "N_EPOCHS= 500\n",
    "# threshold= 0.5040 -- not using threshold values for now, instead just doing the training for the same number of EPOCHS for comparison\n",
    "for epoch in range(N_EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    for inputs, labels in trainLoader: \n",
    "        inputs= inputs.to(device) \n",
    "        labels= labels.to(device)\n",
    "        optimizer2.zero_grad() \n",
    "        outputs= model2(inputs) \n",
    "        loss= criterion2(outputs, labels, tau, h) \n",
    "        loss.backward() \n",
    "        optimizer2.step() \n",
    "        epoch_loss+= loss.item() \n",
    "    ls_b.append((epoch_loss/len(trainLoader)))\n",
    "    print(\"Epoch: {} Loss: {}\".format(epoch,\n",
    "           epoch_loss/len(trainLoader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The large architecture\n",
    "size1= 3000\n",
    "size2= 3000\n",
    "tau= 0.5\n",
    "h= 0.4\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion3= TiltedLC()\n",
    "model3= Network(size1, size2, 0.1).to(device)\n",
    "optimizer3= optim.SGD(model3.parameters(), lr= 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop for the large architecture\n",
    "ls_c= []\n",
    "N_EPOCHS= 500\n",
    "# threshold= 0.5040 -- not using threshold values for now, instead just doing the training for the same number of EPOCHS for comparison\n",
    "for epoch in range(N_EPOCHS):\n",
    "    epoch_loss= 0.0\n",
    "    for inputs, labels in trainLoader: \n",
    "        inputs= inputs.to(device) \n",
    "        labels= labels.to(device)\n",
    "        optimizer3.zero_grad() \n",
    "        outputs= model3(inputs) \n",
    "        loss= criterion3(outputs, labels, tau, h) \n",
    "        loss.backward() \n",
    "        optimizer3.step() \n",
    "        epoch_loss+= loss.item() \n",
    "    ls_c.append((epoch_loss/len(trainLoader)))\n",
    "    print(\"Epoch: {} Loss: {}\".format(epoch,\n",
    "           epoch_loss/len(trainLoader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the wide-medium architecture\n",
    "size1= 2000\n",
    "tau= 0.5\n",
    "h= 0.4\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion4= TiltedLC()\n",
    "model4= wideM(size1, 0.2).to(device)\n",
    "optimizer4= optim.SGD(model4.parameters(), lr= 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.209590157707024\n",
      "Epoch: 1 Loss: 0.1704578223560892\n",
      "Epoch: 2 Loss: 0.15676667146388742\n",
      "Epoch: 3 Loss: 0.14901596169373169\n",
      "Epoch: 4 Loss: 0.14367494402630138\n",
      "Epoch: 5 Loss: 0.13961779341904285\n",
      "Epoch: 6 Loss: 0.13637957213252885\n",
      "Epoch: 7 Loss: 0.1337346183063962\n",
      "Epoch: 8 Loss: 0.13150652691687018\n",
      "Epoch: 9 Loss: 0.12961816856574848\n",
      "Epoch: 10 Loss: 0.12796683837928938\n",
      "Epoch: 11 Loss: 0.1265219940304477\n",
      "Epoch: 12 Loss: 0.12522861315197525\n",
      "Epoch: 13 Loss: 0.12408294251423187\n",
      "Epoch: 14 Loss: 0.12303413431658614\n",
      "Epoch: 15 Loss: 0.12208520993925906\n",
      "Epoch: 16 Loss: 0.12122030708810656\n",
      "Epoch: 17 Loss: 0.12041974248751458\n",
      "Epoch: 18 Loss: 0.11967730591681987\n",
      "Epoch: 19 Loss: 0.11899999151695227\n",
      "Epoch: 20 Loss: 0.11835443277473687\n",
      "Epoch: 21 Loss: 0.11776173981170013\n",
      "Epoch: 22 Loss: 0.11720020041457599\n",
      "Epoch: 23 Loss: 0.11667675201344374\n",
      "Epoch: 24 Loss: 0.11619489071170955\n",
      "Epoch: 25 Loss: 0.11572806608288208\n",
      "Epoch: 26 Loss: 0.115306452062995\n",
      "Epoch: 27 Loss: 0.11489788307824278\n",
      "Epoch: 28 Loss: 0.11449551314834946\n",
      "Epoch: 29 Loss: 0.11412699899895087\n",
      "Epoch: 30 Loss: 0.11377498746277985\n",
      "Epoch: 31 Loss: 0.11345574598664787\n",
      "Epoch: 32 Loss: 0.11312768731411889\n",
      "Epoch: 33 Loss: 0.112835369244489\n",
      "Epoch: 34 Loss: 0.11254163256605945\n",
      "Epoch: 35 Loss: 0.1122614949273217\n",
      "Epoch: 36 Loss: 0.11201235260314457\n",
      "Epoch: 37 Loss: 0.11175530031390543\n",
      "Epoch: 38 Loss: 0.11150974638181599\n",
      "Epoch: 39 Loss: 0.11127882899249744\n",
      "Epoch: 40 Loss: 0.1110588432356938\n",
      "Epoch: 41 Loss: 0.1108505754798538\n",
      "Epoch: 42 Loss: 0.11063538510248352\n",
      "Epoch: 43 Loss: 0.110447556730692\n",
      "Epoch: 44 Loss: 0.11026529848999729\n",
      "Epoch: 45 Loss: 0.11008472502274548\n",
      "Epoch: 46 Loss: 0.1098959662625541\n",
      "Epoch: 47 Loss: 0.10973061172892609\n",
      "Epoch: 48 Loss: 0.10956842547718458\n",
      "Epoch: 49 Loss: 0.10941005891302731\n",
      "Epoch: 50 Loss: 0.10925695401841382\n",
      "Epoch: 51 Loss: 0.10911326576645064\n",
      "Epoch: 52 Loss: 0.10897210860328618\n",
      "Epoch: 53 Loss: 0.10883036182686308\n",
      "Epoch: 54 Loss: 0.10869832214359292\n",
      "Epoch: 55 Loss: 0.10856280305663266\n",
      "Epoch: 56 Loss: 0.10844892906060286\n",
      "Epoch: 57 Loss: 0.10831826703948086\n",
      "Epoch: 58 Loss: 0.10820790359435307\n",
      "Epoch: 59 Loss: 0.10809585882793395\n",
      "Epoch: 60 Loss: 0.10798718515998973\n",
      "Epoch: 61 Loss: 0.10787191130845016\n",
      "Epoch: 62 Loss: 0.10776604325051695\n",
      "Epoch: 63 Loss: 0.10767131978875773\n",
      "Epoch: 64 Loss: 0.10757453716154508\n",
      "Epoch: 65 Loss: 0.10748254373425344\n",
      "Epoch: 66 Loss: 0.10737911459992026\n",
      "Epoch: 67 Loss: 0.10728901813125163\n",
      "Epoch: 68 Loss: 0.10719985644938902\n",
      "Epoch: 69 Loss: 0.10710860898211891\n",
      "Epoch: 70 Loss: 0.1070315623120711\n",
      "Epoch: 71 Loss: 0.10694947012253354\n",
      "Epoch: 72 Loss: 0.10686606109421481\n",
      "Epoch: 73 Loss: 0.10679595461113563\n",
      "Epoch: 74 Loss: 0.10671480238931584\n",
      "Epoch: 75 Loss: 0.10663914816312114\n",
      "Epoch: 76 Loss: 0.10656152149134553\n",
      "Epoch: 77 Loss: 0.10649263399089914\n",
      "Epoch: 78 Loss: 0.10642265144167151\n",
      "Epoch: 79 Loss: 0.10635857896248814\n",
      "Epoch: 80 Loss: 0.10629347398262413\n",
      "Epoch: 81 Loss: 0.10622724124041362\n",
      "Epoch: 82 Loss: 0.10616347088623296\n",
      "Epoch: 83 Loss: 0.10610586971076753\n",
      "Epoch: 84 Loss: 0.10604059733913473\n",
      "Epoch: 85 Loss: 0.10597764833395255\n",
      "Epoch: 86 Loss: 0.10591502857571185\n",
      "Epoch: 87 Loss: 0.10586799799837608\n",
      "Epoch: 88 Loss: 0.10581840709767198\n",
      "Epoch: 89 Loss: 0.10575377340494803\n",
      "Epoch: 90 Loss: 0.10570100123518072\n",
      "Epoch: 91 Loss: 0.10565029115687402\n",
      "Epoch: 92 Loss: 0.10560414254117755\n",
      "Epoch: 93 Loss: 0.10554936322580048\n",
      "Epoch: 94 Loss: 0.10550452336075714\n",
      "Epoch: 95 Loss: 0.10545023775674098\n",
      "Epoch: 96 Loss: 0.10541035587793857\n",
      "Epoch: 97 Loss: 0.10535486455227615\n",
      "Epoch: 98 Loss: 0.1053185725003787\n",
      "Epoch: 99 Loss: 0.10526984411276175\n",
      "Epoch: 100 Loss: 0.10523686372381226\n",
      "Epoch: 101 Loss: 0.10518098848587842\n",
      "Epoch: 102 Loss: 0.10513129439912944\n",
      "Epoch: 103 Loss: 0.10509832050645272\n",
      "Epoch: 104 Loss: 0.10504590702355392\n",
      "Epoch: 105 Loss: 0.10501569790698197\n",
      "Epoch: 106 Loss: 0.10497218367618992\n",
      "Epoch: 107 Loss: 0.10493754842349633\n",
      "Epoch: 108 Loss: 0.10489926329900973\n",
      "Epoch: 109 Loss: 0.10485949360171087\n",
      "Epoch: 110 Loss: 0.10481814844552256\n",
      "Epoch: 111 Loss: 0.10478405396190126\n",
      "Epoch: 112 Loss: 0.10474214529968601\n",
      "Epoch: 113 Loss: 0.10471564487436187\n",
      "Epoch: 114 Loss: 0.10468117781607574\n",
      "Epoch: 115 Loss: 0.10464268947099699\n",
      "Epoch: 116 Loss: 0.10461196599726505\n",
      "Epoch: 117 Loss: 0.10456817454474399\n",
      "Epoch: 118 Loss: 0.10453716238550916\n",
      "Epoch: 119 Loss: 0.1045095759672617\n",
      "Epoch: 120 Loss: 0.10447310123587918\n",
      "Epoch: 121 Loss: 0.10444632107353365\n",
      "Epoch: 122 Loss: 0.10441316542615119\n",
      "Epoch: 123 Loss: 0.10438340527450603\n",
      "Epoch: 124 Loss: 0.10434994138300774\n",
      "Epoch: 125 Loss: 0.1043184320803935\n",
      "Epoch: 126 Loss: 0.10429552619579868\n",
      "Epoch: 127 Loss: 0.1042689005011289\n",
      "Epoch: 128 Loss: 0.10423467702220053\n",
      "Epoch: 129 Loss: 0.10420848227872266\n",
      "Epoch: 130 Loss: 0.10417950503934806\n",
      "Epoch: 131 Loss: 0.1041501391577712\n",
      "Epoch: 132 Loss: 0.10412344745613823\n",
      "Epoch: 133 Loss: 0.1040949845363419\n",
      "Epoch: 134 Loss: 0.10407420293965054\n",
      "Epoch: 135 Loss: 0.10404508290038927\n",
      "Epoch: 136 Loss: 0.10403084256180427\n",
      "Epoch: 137 Loss: 0.10399666876862487\n",
      "Epoch: 138 Loss: 0.10396395304641773\n",
      "Epoch: 139 Loss: 0.10394459667291282\n",
      "Epoch: 140 Loss: 0.10391513611755593\n",
      "Epoch: 141 Loss: 0.10389566488082162\n",
      "Epoch: 142 Loss: 0.1038731609732369\n",
      "Epoch: 143 Loss: 0.10385035013018631\n",
      "Epoch: 144 Loss: 0.10381796858618225\n",
      "Epoch: 145 Loss: 0.10379738585731185\n",
      "Epoch: 146 Loss: 0.10377387053508884\n",
      "Epoch: 147 Loss: 0.10375136653220754\n",
      "Epoch: 148 Loss: 0.10373325826450203\n",
      "Epoch: 149 Loss: 0.1037145816668844\n",
      "Epoch: 150 Loss: 0.10368730967897614\n",
      "Epoch: 151 Loss: 0.10366399719146131\n",
      "Epoch: 152 Loss: 0.10363986937339575\n",
      "Epoch: 153 Loss: 0.10362007547189386\n",
      "Epoch: 154 Loss: 0.10359200548264512\n",
      "Epoch: 155 Loss: 0.10358420220678335\n",
      "Epoch: 156 Loss: 0.1035488396654836\n",
      "Epoch: 157 Loss: 0.10352378808347154\n",
      "Epoch: 158 Loss: 0.10351444775860409\n",
      "Epoch: 159 Loss: 0.10348971286317804\n",
      "Epoch: 160 Loss: 0.10347152608318429\n",
      "Epoch: 161 Loss: 0.10345525129316863\n",
      "Epoch: 162 Loss: 0.10342910025725814\n",
      "Epoch: 163 Loss: 0.10341329418003312\n",
      "Epoch: 164 Loss: 0.10338326136881415\n",
      "Epoch: 165 Loss: 0.10336822038206017\n",
      "Epoch: 166 Loss: 0.10335664635407046\n",
      "Epoch: 167 Loss: 0.1033358869816114\n",
      "Epoch: 168 Loss: 0.10331632428970493\n",
      "Epoch: 169 Loss: 0.10329918581619926\n",
      "Epoch: 170 Loss: 0.10328141087976539\n",
      "Epoch: 171 Loss: 0.10325873956698982\n",
      "Epoch: 172 Loss: 0.10324278163133581\n",
      "Epoch: 173 Loss: 0.10322739176789053\n",
      "Epoch: 174 Loss: 0.10320386402087477\n",
      "Epoch: 175 Loss: 0.10318051306509023\n",
      "Epoch: 176 Loss: 0.10316413003025861\n",
      "Epoch: 177 Loss: 0.10314996181103887\n",
      "Epoch: 178 Loss: 0.1031392691823045\n",
      "Epoch: 179 Loss: 0.10311441126572637\n",
      "Epoch: 180 Loss: 0.10309815209897825\n",
      "Epoch: 181 Loss: 0.10307044893841424\n",
      "Epoch: 182 Loss: 0.10305909569913944\n",
      "Epoch: 183 Loss: 0.1030487001084504\n",
      "Epoch: 184 Loss: 0.10302453374963605\n",
      "Epoch: 185 Loss: 0.10300505526230971\n",
      "Epoch: 186 Loss: 0.1029831528505255\n",
      "Epoch: 187 Loss: 0.1029731171412675\n",
      "Epoch: 188 Loss: 0.10295636445643601\n",
      "Epoch: 189 Loss: 0.10293996658621124\n",
      "Epoch: 190 Loss: 0.1029260839305253\n",
      "Epoch: 191 Loss: 0.10291057999306726\n",
      "Epoch: 192 Loss: 0.10288252994361771\n",
      "Epoch: 193 Loss: 0.10287546272498813\n",
      "Epoch: 194 Loss: 0.10285688839266266\n",
      "Epoch: 195 Loss: 0.102833136028963\n",
      "Epoch: 196 Loss: 0.10281716164726971\n",
      "Epoch: 197 Loss: 0.10282112858280837\n",
      "Epoch: 198 Loss: 0.10279419139185976\n",
      "Epoch: 199 Loss: 0.10277791175111481\n",
      "Epoch: 200 Loss: 0.10275924472207745\n",
      "Epoch: 201 Loss: 0.10274024372824762\n",
      "Epoch: 202 Loss: 0.10273265154098438\n",
      "Epoch: 203 Loss: 0.1027155512934804\n",
      "Epoch: 204 Loss: 0.10269064534475\n",
      "Epoch: 205 Loss: 0.10267916758105339\n",
      "Epoch: 206 Loss: 0.10266222062679567\n",
      "Epoch: 207 Loss: 0.10265173411781649\n",
      "Epoch: 208 Loss: 0.10264002243327013\n",
      "Epoch: 209 Loss: 0.10262282822469702\n",
      "Epoch: 210 Loss: 0.10260539851147213\n",
      "Epoch: 211 Loss: 0.1025852051401993\n",
      "Epoch: 212 Loss: 0.10257766809340663\n",
      "Epoch: 213 Loss: 0.10256198487761127\n",
      "Epoch: 214 Loss: 0.10254658893531904\n",
      "Epoch: 215 Loss: 0.10253290815936232\n",
      "Epoch: 216 Loss: 0.10251800601433213\n",
      "Epoch: 217 Loss: 0.10250299266626589\n",
      "Epoch: 218 Loss: 0.10249230726799048\n",
      "Epoch: 219 Loss: 0.10246714103481744\n",
      "Epoch: 220 Loss: 0.10245412728581273\n",
      "Epoch: 221 Loss: 0.10245098414946796\n",
      "Epoch: 222 Loss: 0.10242707734225226\n",
      "Epoch: 223 Loss: 0.10240657253644204\n",
      "Epoch: 224 Loss: 0.10238494188898084\n",
      "Epoch: 225 Loss: 0.1023723871841922\n",
      "Epoch: 226 Loss: 0.10236521613391707\n",
      "Epoch: 227 Loss: 0.1023540572687605\n",
      "Epoch: 228 Loss: 0.10234594205235001\n",
      "Epoch: 229 Loss: 0.1023208359110422\n",
      "Epoch: 230 Loss: 0.10231043313955578\n",
      "Epoch: 231 Loss: 0.1022959002888411\n",
      "Epoch: 232 Loss: 0.10228191398673392\n",
      "Epoch: 233 Loss: 0.1022703003918796\n",
      "Epoch: 234 Loss: 0.10224838021064186\n",
      "Epoch: 235 Loss: 0.10223575504008467\n",
      "Epoch: 236 Loss: 0.10221171145411445\n",
      "Epoch: 237 Loss: 0.10221015288150755\n",
      "Epoch: 238 Loss: 0.10220308044438448\n",
      "Epoch: 239 Loss: 0.1021778339970071\n",
      "Epoch: 240 Loss: 0.10215782613861434\n",
      "Epoch: 241 Loss: 0.10215903261256656\n",
      "Epoch: 242 Loss: 0.10213067081651694\n",
      "Epoch: 243 Loss: 0.10211731828084789\n",
      "Epoch: 244 Loss: 0.10211798032889\n",
      "Epoch: 245 Loss: 0.10209132527616771\n",
      "Epoch: 246 Loss: 0.10208267997061105\n",
      "Epoch: 247 Loss: 0.10206404683359506\n",
      "Epoch: 248 Loss: 0.10206375983927062\n",
      "Epoch: 249 Loss: 0.10203568472011393\n",
      "Epoch: 250 Loss: 0.10202201620922757\n",
      "Epoch: 251 Loss: 0.10201059984604618\n",
      "Epoch: 252 Loss: 0.10199394737427524\n",
      "Epoch: 253 Loss: 0.10197768800753856\n",
      "Epoch: 254 Loss: 0.1019656256625103\n",
      "Epoch: 255 Loss: 0.10195481297218613\n",
      "Epoch: 256 Loss: 0.10194666932250246\n",
      "Epoch: 257 Loss: 0.10192486167039942\n",
      "Epoch: 258 Loss: 0.1019077928439172\n",
      "Epoch: 259 Loss: 0.10189925225854586\n",
      "Epoch: 260 Loss: 0.10188659047665971\n",
      "Epoch: 261 Loss: 0.10186917971805547\n",
      "Epoch: 262 Loss: 0.10184682227591957\n",
      "Epoch: 263 Loss: 0.10184424522464887\n",
      "Epoch: 264 Loss: 0.10182165523876566\n",
      "Epoch: 265 Loss: 0.10181969180609209\n",
      "Epoch: 266 Loss: 0.10179735307161109\n",
      "Epoch: 267 Loss: 0.10178933587241358\n",
      "Epoch: 268 Loss: 0.10177070968130905\n",
      "Epoch: 269 Loss: 0.10176596616199694\n",
      "Epoch: 270 Loss: 0.10174658717367804\n",
      "Epoch: 271 Loss: 0.10173153135287313\n",
      "Epoch: 272 Loss: 0.10171509931010612\n",
      "Epoch: 273 Loss: 0.10170970497969313\n",
      "Epoch: 274 Loss: 0.10168164400200955\n",
      "Epoch: 275 Loss: 0.10167694781824525\n",
      "Epoch: 276 Loss: 0.10165590950804795\n",
      "Epoch: 277 Loss: 0.10164917343222457\n",
      "Epoch: 278 Loss: 0.10163926665921659\n",
      "Epoch: 279 Loss: 0.10162903845895413\n",
      "Epoch: 280 Loss: 0.10162131713813671\n",
      "Epoch: 281 Loss: 0.10159298133858688\n",
      "Epoch: 282 Loss: 0.10158430494353313\n",
      "Epoch: 283 Loss: 0.10156630660041327\n",
      "Epoch: 284 Loss: 0.10155095232218112\n",
      "Epoch: 285 Loss: 0.10153888795981857\n",
      "Epoch: 286 Loss: 0.10152238860546502\n",
      "Epoch: 287 Loss: 0.10151758321318148\n",
      "Epoch: 288 Loss: 0.10149709075297392\n",
      "Epoch: 289 Loss: 0.10148998759546016\n",
      "Epoch: 290 Loss: 0.10147266490027575\n",
      "Epoch: 291 Loss: 0.10146249093181098\n",
      "Epoch: 292 Loss: 0.10144410966361156\n",
      "Epoch: 293 Loss: 0.10143146055079949\n",
      "Epoch: 294 Loss: 0.10141817810105495\n",
      "Epoch: 295 Loss: 0.10140417578778295\n",
      "Epoch: 296 Loss: 0.10139222275328151\n",
      "Epoch: 297 Loss: 0.10137011352379544\n",
      "Epoch: 298 Loss: 0.10136742985311918\n",
      "Epoch: 299 Loss: 0.10135486745883744\n",
      "Epoch: 300 Loss: 0.10133924110019299\n",
      "Epoch: 301 Loss: 0.10132830021720043\n",
      "Epoch: 302 Loss: 0.10130959121606689\n",
      "Epoch: 303 Loss: 0.10129793363312432\n",
      "Epoch: 304 Loss: 0.10128110386679526\n",
      "Epoch: 305 Loss: 0.10126851267281835\n",
      "Epoch: 306 Loss: 0.10126941630838583\n",
      "Epoch: 307 Loss: 0.10125293323900694\n",
      "Epoch: 308 Loss: 0.1012332234132335\n",
      "Epoch: 309 Loss: 0.1012274583214748\n",
      "Epoch: 310 Loss: 0.1012106326435025\n",
      "Epoch: 311 Loss: 0.10119605155930479\n",
      "Epoch: 312 Loss: 0.10117789839081154\n",
      "Epoch: 313 Loss: 0.10116290845482913\n",
      "Epoch: 314 Loss: 0.10116460936426193\n",
      "Epoch: 315 Loss: 0.10114079913809973\n",
      "Epoch: 316 Loss: 0.10113220435314363\n",
      "Epoch: 317 Loss: 0.10111221901963711\n",
      "Epoch: 318 Loss: 0.10110154074847086\n",
      "Epoch: 319 Loss: 0.10108751177632085\n",
      "Epoch: 320 Loss: 0.10107537472684025\n",
      "Epoch: 321 Loss: 0.10107044946624447\n",
      "Epoch: 322 Loss: 0.10104946827157256\n",
      "Epoch: 323 Loss: 0.1010361361253414\n",
      "Epoch: 324 Loss: 0.10102984219268729\n",
      "Epoch: 325 Loss: 0.10101458044674692\n",
      "Epoch: 326 Loss: 0.10101292762718121\n",
      "Epoch: 327 Loss: 0.10099568068873795\n",
      "Epoch: 328 Loss: 0.10097282512743953\n",
      "Epoch: 329 Loss: 0.10096583670027684\n",
      "Epoch: 330 Loss: 0.10095026729944868\n",
      "Epoch: 331 Loss: 0.1009421270817835\n",
      "Epoch: 332 Loss: 0.1009302841510521\n",
      "Epoch: 333 Loss: 0.1009276456241027\n",
      "Epoch: 334 Loss: 0.1008942016455595\n",
      "Epoch: 335 Loss: 0.10089040655791061\n",
      "Epoch: 336 Loss: 0.10087916106683588\n",
      "Epoch: 337 Loss: 0.10086833786704566\n",
      "Epoch: 338 Loss: 0.10085208969966847\n",
      "Epoch: 339 Loss: 0.10084320102413803\n",
      "Epoch: 340 Loss: 0.10082892060999268\n",
      "Epoch: 341 Loss: 0.10082293686094079\n",
      "Epoch: 342 Loss: 0.10079690658054015\n",
      "Epoch: 343 Loss: 0.10078448922628541\n",
      "Epoch: 344 Loss: 0.10077973584042349\n",
      "Epoch: 345 Loss: 0.10077117324691488\n",
      "Epoch: 346 Loss: 0.10075274642830044\n",
      "Epoch: 347 Loss: 0.10074700147901391\n",
      "Epoch: 348 Loss: 0.10074045138181577\n",
      "Epoch: 349 Loss: 0.10071771417146673\n",
      "Epoch: 350 Loss: 0.10070499216349485\n",
      "Epoch: 351 Loss: 0.10069634587812845\n",
      "Epoch: 352 Loss: 0.10068049695100512\n",
      "Epoch: 353 Loss: 0.10067729377849663\n",
      "Epoch: 354 Loss: 0.10066156098164432\n",
      "Epoch: 355 Loss: 0.10064096304961656\n",
      "Epoch: 356 Loss: 0.10064328728313168\n",
      "Epoch: 357 Loss: 0.10062222812781783\n",
      "Epoch: 358 Loss: 0.10061385060023496\n",
      "Epoch: 359 Loss: 0.10060060402092215\n",
      "Epoch: 360 Loss: 0.10059657179918445\n",
      "Epoch: 361 Loss: 0.1005796473490088\n",
      "Epoch: 362 Loss: 0.10057144859844963\n",
      "Epoch: 363 Loss: 0.10055846779185616\n",
      "Epoch: 364 Loss: 0.10054260414879594\n",
      "Epoch: 365 Loss: 0.10053034038038818\n",
      "Epoch: 366 Loss: 0.1005230542158913\n",
      "Epoch: 367 Loss: 0.10051246441483304\n",
      "Epoch: 368 Loss: 0.10050090199087766\n",
      "Epoch: 369 Loss: 0.10049149321824705\n",
      "Epoch: 370 Loss: 0.10047496833380998\n",
      "Epoch: 371 Loss: 0.1004514853949698\n",
      "Epoch: 372 Loss: 0.10045734804356513\n",
      "Epoch: 373 Loss: 0.10043900751531758\n",
      "Epoch: 374 Loss: 0.10042682784142913\n",
      "Epoch: 375 Loss: 0.10041260701179547\n",
      "Epoch: 376 Loss: 0.10041596695654587\n",
      "Epoch: 377 Loss: 0.10039512528946455\n",
      "Epoch: 378 Loss: 0.10038127890485855\n",
      "Epoch: 379 Loss: 0.10037437858138186\n",
      "Epoch: 380 Loss: 0.1003629698613875\n",
      "Epoch: 381 Loss: 0.10036020237402837\n",
      "Epoch: 382 Loss: 0.10033547900411766\n",
      "Epoch: 383 Loss: 0.10032100810556449\n",
      "Epoch: 384 Loss: 0.10031539843981598\n",
      "Epoch: 385 Loss: 0.10031560355962837\n",
      "Epoch: 386 Loss: 0.10029445598252591\n",
      "Epoch: 387 Loss: 0.10028021933903158\n",
      "Epoch: 388 Loss: 0.10028016331404786\n",
      "Epoch: 389 Loss: 0.10026729796796506\n",
      "Epoch: 390 Loss: 0.10025232975825686\n",
      "Epoch: 391 Loss: 0.1002471756127649\n",
      "Epoch: 392 Loss: 0.10023498020017926\n",
      "Epoch: 393 Loss: 0.10022239207448326\n",
      "Epoch: 394 Loss: 0.10020509182365278\n",
      "Epoch: 395 Loss: 0.10019993778285283\n",
      "Epoch: 396 Loss: 0.10019684149416433\n",
      "Epoch: 397 Loss: 0.1001838542194844\n",
      "Epoch: 398 Loss: 0.10016310254725609\n",
      "Epoch: 399 Loss: 0.1001561396998385\n",
      "Epoch: 400 Loss: 0.10014926116571966\n",
      "Epoch: 401 Loss: 0.1001308442566056\n",
      "Epoch: 402 Loss: 0.1001177263786569\n",
      "Epoch: 403 Loss: 0.10012117304596423\n",
      "Epoch: 404 Loss: 0.1001057620494994\n",
      "Epoch: 405 Loss: 0.10008794959440594\n",
      "Epoch: 406 Loss: 0.10008635652175875\n",
      "Epoch: 407 Loss: 0.10008114585064055\n",
      "Epoch: 408 Loss: 0.10006268613323653\n",
      "Epoch: 409 Loss: 0.1000471170538665\n",
      "Epoch: 410 Loss: 0.1000473975337984\n",
      "Epoch: 411 Loss: 0.10002945172257346\n",
      "Epoch: 412 Loss: 0.10001343273537758\n",
      "Epoch: 413 Loss: 0.10000640898137625\n",
      "Epoch: 414 Loss: 0.09999975947442731\n",
      "Epoch: 415 Loss: 0.09999362939548931\n",
      "Epoch: 416 Loss: 0.0999824431842457\n",
      "Epoch: 417 Loss: 0.09997439850876727\n",
      "Epoch: 418 Loss: 0.09996375624493765\n",
      "Epoch: 419 Loss: 0.09994998629555747\n",
      "Epoch: 420 Loss: 0.09994424100132424\n",
      "Epoch: 421 Loss: 0.09993344703341922\n",
      "Epoch: 422 Loss: 0.09991590102214767\n",
      "Epoch: 423 Loss: 0.09991928456554841\n",
      "Epoch: 424 Loss: 0.09990173981192305\n",
      "Epoch: 425 Loss: 0.09989007775272642\n",
      "Epoch: 426 Loss: 0.09987948223655878\n",
      "Epoch: 427 Loss: 0.09987127656760333\n",
      "Epoch: 428 Loss: 0.09986203887981888\n",
      "Epoch: 429 Loss: 0.09985154057688336\n",
      "Epoch: 430 Loss: 0.09983898084016775\n",
      "Epoch: 431 Loss: 0.09983042552141086\n",
      "Epoch: 432 Loss: 0.09981419943062342\n",
      "Epoch: 433 Loss: 0.09981564275881467\n",
      "Epoch: 434 Loss: 0.09980120597374674\n",
      "Epoch: 435 Loss: 0.0997953321472672\n",
      "Epoch: 436 Loss: 0.09978376221971412\n",
      "Epoch: 437 Loss: 0.09977513252937177\n",
      "Epoch: 438 Loss: 0.09976264168938695\n",
      "Epoch: 439 Loss: 0.09975614525556263\n",
      "Epoch: 440 Loss: 0.09974780724249838\n",
      "Epoch: 441 Loss: 0.09973015195559734\n",
      "Epoch: 442 Loss: 0.09972523336643208\n",
      "Epoch: 443 Loss: 0.09972132752337255\n",
      "Epoch: 444 Loss: 0.09972009184505033\n",
      "Epoch: 445 Loss: 0.09969933887759408\n",
      "Epoch: 446 Loss: 0.09969233950340647\n",
      "Epoch: 447 Loss: 0.09967595734973252\n",
      "Epoch: 448 Loss: 0.09968166850229424\n",
      "Epoch: 449 Loss: 0.09966912896979223\n",
      "Epoch: 450 Loss: 0.09965184733601781\n",
      "Epoch: 451 Loss: 0.09963965538363696\n",
      "Epoch: 452 Loss: 0.0996380294403865\n",
      "Epoch: 453 Loss: 0.0996237309091565\n",
      "Epoch: 454 Loss: 0.09962129969652483\n",
      "Epoch: 455 Loss: 0.09960306555950799\n",
      "Epoch: 456 Loss: 0.09960055721034834\n",
      "Epoch: 457 Loss: 0.09959131075125345\n",
      "Epoch: 458 Loss: 0.09958383512779348\n",
      "Epoch: 459 Loss: 0.09957353154929173\n",
      "Epoch: 460 Loss: 0.09957006603071356\n",
      "Epoch: 461 Loss: 0.09955679245160262\n",
      "Epoch: 462 Loss: 0.09954941880217631\n",
      "Epoch: 463 Loss: 0.09953357365604479\n",
      "Epoch: 464 Loss: 0.09953330706055369\n",
      "Epoch: 465 Loss: 0.09952412567290501\n",
      "Epoch: 466 Loss: 0.09952039312127689\n",
      "Epoch: 467 Loss: 0.09949855849017991\n",
      "Epoch: 468 Loss: 0.09950240410312707\n",
      "Epoch: 469 Loss: 0.09949588545446589\n",
      "Epoch: 470 Loss: 0.09948331562839442\n",
      "Epoch: 471 Loss: 0.09946949868841314\n",
      "Epoch: 472 Loss: 0.09946075165444634\n",
      "Epoch: 473 Loss: 0.09946074759427588\n",
      "Epoch: 474 Loss: 0.09944440065737974\n",
      "Epoch: 475 Loss: 0.09943453001720458\n",
      "Epoch: 476 Loss: 0.0994294846480418\n",
      "Epoch: 477 Loss: 0.09941362946758484\n",
      "Epoch: 478 Loss: 0.09941679434161255\n",
      "Epoch: 479 Loss: 0.09940350536472453\n",
      "Epoch: 480 Loss: 0.09939964773702828\n",
      "Epoch: 481 Loss: 0.09939119869541464\n",
      "Epoch: 482 Loss: 0.09938573027796369\n",
      "Epoch: 483 Loss: 0.09937823165045023\n",
      "Epoch: 484 Loss: 0.09936660736761659\n",
      "Epoch: 485 Loss: 0.09935769491492252\n",
      "Epoch: 486 Loss: 0.09936178599235454\n",
      "Epoch: 487 Loss: 0.09934401299674926\n",
      "Epoch: 488 Loss: 0.09933317511457727\n",
      "Epoch: 489 Loss: 0.09932894682888511\n",
      "Epoch: 490 Loss: 0.09931960128418198\n",
      "Epoch: 491 Loss: 0.09931299018577464\n",
      "Epoch: 492 Loss: 0.09930329764749397\n",
      "Epoch: 493 Loss: 0.09929395011498507\n",
      "Epoch: 494 Loss: 0.09928985414789981\n",
      "Epoch: 495 Loss: 0.09928293357054793\n",
      "Epoch: 496 Loss: 0.09926897084494471\n",
      "Epoch: 497 Loss: 0.09926328945242592\n",
      "Epoch: 498 Loss: 0.0992587442063379\n",
      "Epoch: 499 Loss: 0.09924991644947105\n"
     ]
    }
   ],
   "source": [
    "# training loop for wide-medium architecture\n",
    "ls_d= []\n",
    "N_EPOCHS= 500\n",
    "# threshold= 0.5040 -- not using threshold values for now, instead just doing the training for the same number of EPOCHS for comparison\n",
    "for epoch in range(N_EPOCHS):\n",
    "    epoch_loss= 0.0\n",
    "    for inputs, labels in trainLoader: \n",
    "        inputs= inputs.to(device) \n",
    "        labels= labels.to(device)\n",
    "        optimizer4.zero_grad() \n",
    "        outputs= model4(inputs) \n",
    "        loss= criterion4(outputs, labels, tau, h) \n",
    "        loss.backward() \n",
    "        optimizer4.step() \n",
    "        epoch_loss+= loss.item() \n",
    "    ls_d.append((epoch_loss/len(trainLoader)))\n",
    "    print(\"Epoch: {} Loss: {}\".format(epoch,\n",
    "           epoch_loss/len(trainLoader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the EPOCH-losses\n",
    "A= np.asarray(ls_a)\n",
    "B= np.asarray(ls_b)\n",
    "C= np.asarray(ls_c)\n",
    "D= np.asarray(ls_d)\n",
    "\n",
    "np.save(\"/home/aryamanj/Downloads/ls_a.npy\", A)\n",
    "np.save(\"/home/aryamanj/Downloads/ls_b.npy\", B)\n",
    "np.save(\"/home/aryamanj/Downloads/ls_c.npy\", C)\n",
    "np.save(\"/home/aryamanj/Downloads/ls_d.npy\", D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiations and training loops using adaH as the optimizer algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the small architecture\n",
    "size1= 300\n",
    "size2= 300\n",
    "tau= 0.5\n",
    "h= 0.4\n",
    "device= ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion5= TiltedLC()\n",
    "model5= Network(size1, size2, 0.1).to(device)\n",
    "optimizer5= Adahessian(model5.parameters(),lr=.15)\n",
    "scheduler5= lr_scheduler.MultiStepLR(\n",
    "    optimizer5,\n",
    "    [30,45], # \n",
    "    gamma=.1,\n",
    "    last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aryamanj/Documents/Code/tmpBQR/adahessian.py:131: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1616554794034/work/torch/csrc/utils/python_arg_parser.cpp:1005.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.16603829289149474\n",
      "Epoch: 1 Loss: 0.16330337790439117\n",
      "Epoch: 2 Loss: 0.16223583395680616\n",
      "Epoch: 3 Loss: 0.16146252985082002\n",
      "Epoch: 4 Loss: 0.16090061644421377\n",
      "Epoch: 5 Loss: 0.16058397461134902\n",
      "Epoch: 6 Loss: 0.16024065135277363\n",
      "Epoch: 7 Loss: 0.15990553915747607\n",
      "Epoch: 8 Loss: 0.15965878882938153\n",
      "Epoch: 9 Loss: 0.15938243182677017\n",
      "Epoch: 10 Loss: 0.15918316465281213\n",
      "Epoch: 11 Loss: 0.15904777714758817\n",
      "Epoch: 12 Loss: 0.1588188072938658\n",
      "Epoch: 13 Loss: 0.15868130317815166\n",
      "Epoch: 14 Loss: 0.15861690969316622\n",
      "Epoch: 15 Loss: 0.15847898997706006\n",
      "Epoch: 16 Loss: 0.1583482665661605\n",
      "Epoch: 17 Loss: 0.1583395162182893\n",
      "Epoch: 18 Loss: 0.15824741795940928\n",
      "Epoch: 19 Loss: 0.15810321528451804\n",
      "Epoch: 20 Loss: 0.1580750745006605\n",
      "Epoch: 21 Loss: 0.15792126501547155\n",
      "Epoch: 22 Loss: 0.15787840801823183\n",
      "Epoch: 23 Loss: 0.1578085291758247\n",
      "Epoch: 24 Loss: 0.15776998399775471\n",
      "Epoch: 25 Loss: 0.1576643417349615\n",
      "Epoch: 26 Loss: 0.15766901335502997\n",
      "Epoch: 27 Loss: 0.15766645248001535\n",
      "Epoch: 28 Loss: 0.1575484261899204\n",
      "Epoch: 29 Loss: 0.15752525437422601\n",
      "Epoch: 30 Loss: 0.15745147961101066\n",
      "Epoch: 31 Loss: 0.15737512585435381\n",
      "Epoch: 32 Loss: 0.1572593455136606\n",
      "Epoch: 33 Loss: 0.15713651534663173\n",
      "Epoch: 34 Loss: 0.15712145681457249\n",
      "Epoch: 35 Loss: 0.15723184449321365\n",
      "Epoch: 36 Loss: 0.15722235934844084\n",
      "Epoch: 37 Loss: 0.15707911044853137\n",
      "Epoch: 38 Loss: 0.15705692537237645\n",
      "Epoch: 39 Loss: 0.1570314907371042\n",
      "Epoch: 40 Loss: 0.1570276267903063\n",
      "Epoch: 41 Loss: 0.15694594900525596\n",
      "Epoch: 42 Loss: 0.15694015278776063\n",
      "Epoch: 43 Loss: 0.15688299841224515\n",
      "Epoch: 44 Loss: 0.15683284422697796\n",
      "Epoch: 45 Loss: 0.15684057325071052\n",
      "Epoch: 46 Loss: 0.15677739028575463\n",
      "Epoch: 47 Loss: 0.1567656194730463\n",
      "Epoch: 48 Loss: 0.15675930427583654\n",
      "Epoch: 49 Loss: 0.15667489261895423\n",
      "Epoch: 50 Loss: 0.15663850073518248\n",
      "Epoch: 51 Loss: 0.156624761774634\n",
      "Epoch: 52 Loss: 0.15665044842904977\n",
      "Epoch: 53 Loss: 0.15665554548921595\n",
      "Epoch: 54 Loss: 0.156639395981968\n",
      "Epoch: 55 Loss: 0.15652444436166377\n",
      "Epoch: 56 Loss: 0.15660409730412986\n",
      "Epoch: 57 Loss: 0.15646601340144073\n",
      "Epoch: 58 Loss: 0.156479403878564\n",
      "Epoch: 59 Loss: 0.15645535493719537\n",
      "Epoch: 60 Loss: 0.15633846330301243\n",
      "Epoch: 61 Loss: 0.1563521484961662\n",
      "Epoch: 62 Loss: 0.15638302344982527\n",
      "Epoch: 63 Loss: 0.15628433458935223\n",
      "Epoch: 64 Loss: 0.156270930161349\n",
      "Epoch: 65 Loss: 0.15640400797182366\n",
      "Epoch: 66 Loss: 0.1563564763895844\n",
      "Epoch: 67 Loss: 0.156360039260984\n",
      "Epoch: 68 Loss: 0.15639724307550093\n",
      "Epoch: 69 Loss: 0.1563251310574581\n",
      "Epoch: 70 Loss: 0.15622837453345054\n",
      "Epoch: 71 Loss: 0.15622173997463998\n",
      "Epoch: 72 Loss: 0.15620611149799585\n",
      "Epoch: 73 Loss: 0.15622360685713074\n",
      "Epoch: 74 Loss: 0.15622060366647347\n",
      "Epoch: 75 Loss: 0.15617412885600995\n",
      "Epoch: 76 Loss: 0.1561663104709452\n",
      "Epoch: 77 Loss: 0.15611615428445016\n",
      "Epoch: 78 Loss: 0.15606712923594754\n",
      "Epoch: 79 Loss: 0.15615436599165916\n",
      "Epoch: 80 Loss: 0.15606235321379636\n",
      "Epoch: 81 Loss: 0.1560226616811546\n",
      "Epoch: 82 Loss: 0.15602186404892054\n",
      "Epoch: 83 Loss: 0.15606955346374377\n",
      "Epoch: 84 Loss: 0.1560858917655633\n",
      "Epoch: 85 Loss: 0.1560525786573877\n",
      "Epoch: 86 Loss: 0.1560218659843803\n",
      "Epoch: 87 Loss: 0.15600622401320596\n",
      "Epoch: 88 Loss: 0.15594598794011377\n",
      "Epoch: 89 Loss: 0.15594606935022287\n",
      "Epoch: 90 Loss: 0.15591940284446432\n",
      "Epoch: 91 Loss: 0.15589748309273221\n",
      "Epoch: 92 Loss: 0.15593684476236033\n",
      "Epoch: 93 Loss: 0.1558784814720926\n",
      "Epoch: 94 Loss: 0.1558644410146934\n",
      "Epoch: 95 Loss: 0.15594932688203852\n",
      "Epoch: 96 Loss: 0.15588770855108783\n",
      "Epoch: 97 Loss: 0.15588746021898173\n",
      "Epoch: 98 Loss: 0.1559115803509097\n",
      "Epoch: 99 Loss: 0.15580172398282136\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1424/1880343114.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcriterion5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradsH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_params_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradsH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop for the small architecture\n",
    "ls_e= []\n",
    "N_EPOCHS= 100\n",
    "# threshold= 0.5040 -- not using threshold values for now, instead just doing the training for the same number of EPOCHS for comparison\n",
    "for epoch in range(N_EPOCHS):\n",
    "    epoch_loss= 0.0\n",
    "    for inputs, labels in trainLoader: \n",
    "        inputs= inputs.to(device) \n",
    "        labels= labels.to(device)\n",
    "        optimizer5.zero_grad() \n",
    "        outputs= model5(inputs) \n",
    "        loss= criterion5(outputs, labels, tau, h) \n",
    "        loss.backward(create_graph=True)\n",
    "        _, gradsH = get_params_grad(model5)\n",
    "        optimizer5.step(gradsH)\n",
    "        epoch_loss+= loss.item() \n",
    "    ls_e.append((epoch_loss/len(trainLoader)))\n",
    "    print(\"Epoch: {} Loss: {}\".format(epoch,\n",
    "           epoch_loss/len(trainLoader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the medium architecture\n",
    "size1= 1000\n",
    "size2= 1000\n",
    "tau= 0.5\n",
    "h= 0.4\n",
    "device= ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion6= TiltedLC()\n",
    "model6= Network(size1, size2, 0.1).to(device)\n",
    "optimizer6= Adahessian(model6.parameters(),lr=.15)\n",
    "scheduler6= lr_scheduler.MultiStepLR(\n",
    "    optimizer6,\n",
    "    [30,45], # \n",
    "    gamma=.1,\n",
    "    last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop for the medium architecture\n",
    "ls_f= []\n",
    "N_EPOCHS= 100\n",
    "for epoch in range(N_EPOCHS):\n",
    "    epoch_loss= 0.0\n",
    "    for inputs, labels in trainLoader: \n",
    "        inputs= inputs.to(device) \n",
    "        labels= labels.to(device)\n",
    "        optimizer6.zero_grad() \n",
    "        outputs= model6(inputs) \n",
    "        loss= criterion6(outputs, labels, tau, h) \n",
    "        loss.backward(create_graph=True)\n",
    "        _, gradsH = get_params_grad(model6)\n",
    "        optimizer6.step(gradsH)\n",
    "        epoch_loss+= loss.item() \n",
    "    ls_f.append((epoch_loss/len(trainLoader)))\n",
    "    print(\"Epoch: {} Loss: {}\".format(epoch,\n",
    "           epoch_loss/len(trainLoader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the wide-medium architecture\n",
    "size1= 2000\n",
    "tau= 0.5\n",
    "h= 0.4\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion7= TiltedLC()\n",
    "model7= wideM(size1, 0.2).to(device)\n",
    "optimizer7= Adahessian(model7.parameters(),lr=.15)\n",
    "scheduler7= lr_scheduler.MultiStepLR(\n",
    "    optimizer7,\n",
    "    [30,45], # \n",
    "    gamma=.1,\n",
    "    last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop for the wide-M architecture:\n",
    "ls_g= []\n",
    "N_EPOCHS= 100\n",
    "for epoch in range(N_EPOCHS):\n",
    "    epoch_loss= 0.0\n",
    "    for inputs, labels in trainLoader: \n",
    "        inputs= inputs.to(device) \n",
    "        labels= labels.to(device)\n",
    "        optimizer7.zero_grad() \n",
    "        outputs= model7(inputs) \n",
    "        loss= criterion7(outputs, labels, tau, h) \n",
    "        loss.backward(create_graph=True)\n",
    "        _, gradsH = get_params_grad(model7)\n",
    "        optimizer7.step(gradsH)\n",
    "        epoch_loss+= loss.item() \n",
    "    ls_g.append((epoch_loss/len(trainLoader)))\n",
    "    print(\"Epoch: {} Loss: {}\".format(epoch,\n",
    "           epoch_loss/len(trainLoader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E= np.asarray(ls_e)\n",
    "F= np.asarray(ls_f)\n",
    "G= np.asarray(ls_g)\n",
    "\n",
    "np.save(\"/home/aryamanj/Downloads/ls_e.npy\", E)\n",
    "np.save(\"/home/aryamanj/Downloads/ls_fnpy\", F)\n",
    "np.save(\"/home/aryamanj/Downloads/ls_g.npy\", G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I see the following steps to incorporating LALR into the code:\n",
    "1. Be able to compute the $K_z$: \n",
    "    * This is defined to be the largest activation in the penultimate layer\n",
    "    * Hence, our network needs a method that allows us to obtain it's penultimate output and then simply take a supremum over it \n",
    "2. For now, a possible LC of the log-cosh seems to be just the same as for the check, namely, $\\frac{K_z}{m}\\times \\max(\\tau, 1-\\tau)$ -- these kinds of LC's are all expressable in the form: $C.\\frac{K_z}{m}$ (where C is some constant which we know)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LALR code:\n",
    "\n",
    "# A new network class for LALR training, that supports returning penultimate activations\n",
    "class LALRnetwork(nn.Module):\n",
    "    def __init__(self, size1, size2, drop):\n",
    "        super(LALRnetwork, self).__init__()\n",
    "        self.l1= nn.Linear(943, size1)\n",
    "        self.l2= nn.Dropout(p= drop)\n",
    "        self.l3= nn.Linear(size1, size2)\n",
    "        self.l4= nn.Dropout(p= drop)\n",
    "        self.l5= nn.Linear(size2, 4760)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x= F.tanh(self.l2(self.l1(x)))\n",
    "        x= F.tanh(self.l4(self.l3(x)))\n",
    "        return self.l5(x)\n",
    "    \n",
    "    def penU(self, x):\n",
    "        x= F.tanh(self.l2(self.l1(x)))\n",
    "        x= F.tanh(self.l4(self.l3(x)))\n",
    "        return x\n",
    "\n",
    "def computeLR(model, input, factor, bSize= 16, flag= 0):\n",
    "    \"\"\"\n",
    "    Takes in a network of the LALRnetwork class(during some arbitrary EPOCH of training) and the current input, and returns Kz for the EPOCH\n",
    "    flag= 1 => LR*0.01 modification\n",
    "    \"\"\"\n",
    "    Kz = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i,j in enumerate(trainLoader):\n",
    "            inputs,labels= j[0],j[1]\n",
    "            inputs= inputs.to(device)\n",
    "            labels= labels.to(device)\n",
    "            op= model.penU(inputs)\n",
    "            activ= np.linalg.norm(op.detach().cpu().numpy())\n",
    "            if activ > Kz:\n",
    "                Kz= activ\n",
    "    LR= Kz/bSize*(max(tau,1-tau))\n",
    "    if LR==0:\n",
    "        return 0.1\n",
    "    if flag:\n",
    "        return 1/(LR*0.01)\n",
    "    return 1/LR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing training loops for networks with LALR\n",
    "\n",
    "ls_e= []\n",
    "def train_adaptive_lr(model,optimizer,loader, epochs, verbose=False):\n",
    "    train_preds_Q = []\n",
    "    train_labels = []\n",
    "    epoch_loss= 0.0\n",
    "    lr_val= computeLR(model, loader,bSize=16,flag=0)\n",
    "    optimizer.param_groups[0]['lr'] = lr_val\n",
    "    model.train()\n",
    "    for inputs, labels in trainLoader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs= model(inputs)\n",
    "        loss= criterion1(outputs, labels, tau, h) \n",
    "        optimizer.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss+= loss.item()\n",
    "    ls_e.append((epoch_loss/len(trainLoader)))\n",
    "    print(\"Epoch: {} Loss: {}\".format(epoch,\n",
    "           epoch_loss/len(trainLoader)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f631849613479c9658f644ddf8baf1a688d5be660ada6c6034305de03a8a6229"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
