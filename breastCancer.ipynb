{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#My imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from adahessian import Adahessian, get_params_grad\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import math\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from codecarbon import OfflineEmissionsTracker\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Other imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import auc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tracker= OfflineEmissionsTracker(country_iso_code= \"IND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1904 rows and 693 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aryamanj/miniconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (678,688,690,692) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# Reading the data:\n",
    "nRowsRead = None\n",
    "# METABRIC_RNA_Mutation.csv may have more rows in reality, but we are only loading/previewing the first 1000 rows\n",
    "df1 = pd.read_csv('/home/aryamanj/Downloads/METABRIC_RNA_Mutation.csv', delimiter=',', nrows = nRowsRead)\n",
    "df1.dataframeName = 'METABRIC_RNA_Mutation.csv'\n",
    "nRow, nCol = df1.shape\n",
    "print(f'There are {nRow} rows and {nCol} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parseY(dataset, target_columns, delim):\n",
    "    with open(dataset, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    X = []\n",
    "    Y = []\n",
    "    for line in lines:\n",
    "        while len(line) > 0 and line[-1] == \"\\n\":\n",
    "            line = line[:len(line)-1]\n",
    "        split_array = line.split(delim)\n",
    "        all_columns = []\n",
    "        for value in split_array:\n",
    "            if value !=\"\" and value !=\" \":\n",
    "                all_columns.append(value)\n",
    "        if len(all_columns)==0:\n",
    "            break\n",
    "        point = []\n",
    "        for i in target_columns:\n",
    "            point.append(float(all_columns[i]))\n",
    "        Y.append(point)\n",
    "    Y_arr = np.asarray(Y)\n",
    "    return Y_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clinicalData= \"./METABRIC_clinical_1980.txt\"\n",
    "geneData= \"./METABRIC_gene_exp_1980.txt\"\n",
    "cnvData= \"METABRIC_cnv_1980.txt\"\n",
    "Clinical= parseY(clinicalData, range(25), \"\\t\")\n",
    "GeneExp= parseY(geneData, range(400), \"\\t\")\n",
    "CNV= parseY(cnvData, range(200), \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65438/3544158016.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df3['er_status_measured_by_ihc'] = df3['er_status_measured_by_ihc'].apply(lambda x: 1 if \"Positive\" in str(x) else 0)\n",
      "/tmp/ipykernel_65438/3544158016.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df3['her2_status'] = df3['her2_status'].apply(lambda x: 1 if \"Positive\" in str(x) else 0)\n",
      "/tmp/ipykernel_65438/3544158016.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df3['inferred_menopausal_state'] = df3['inferred_menopausal_state'].apply(lambda x: 1 if \"Post\" in str(x) else 0)\n",
      "/tmp/ipykernel_65438/3544158016.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df3['primary_tumor_laterality'] = df3['primary_tumor_laterality'].apply(lambda x: 1 if \"Left\" in str(x) else 0)\n",
      "/tmp/ipykernel_65438/3544158016.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df3['pr_status'] = df3['pr_status'].apply(lambda x: 1 if \"Positive\" in str(x) else 0)\n",
      "/tmp/ipykernel_65438/3544158016.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df3['er_status'] = df3['er_status'].apply(lambda x: 1 if \"Positive\" in str(x) else 0)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Processing code for KaggleClinical+Categorical GeneExp+CNV\n",
    "\"\"\"\n",
    "\n",
    "df2 = df1.drop(columns=['patient_id', 'cancer_type', 'cancer_type_detailed', 'cohort'])\n",
    "df3 = df2[df2['her2_status_measured_by_snp6'] != 'UNDEF']\n",
    "df3['er_status_measured_by_ihc'] = df3['er_status_measured_by_ihc'].apply(lambda x: 1 if \"Positive\" in str(x) else 0)\n",
    "df3['her2_status'] = df3['her2_status'].apply(lambda x: 1 if \"Positive\" in str(x) else 0)\n",
    "df3['inferred_menopausal_state'] = df3['inferred_menopausal_state'].apply(lambda x: 1 if \"Post\" in str(x) else 0)\n",
    "df3['primary_tumor_laterality'] = df3['primary_tumor_laterality'].apply(lambda x: 1 if \"Left\" in str(x) else 0)\n",
    "df3['pr_status'] = df3['pr_status'].apply(lambda x: 1 if \"Positive\" in str(x) else 0)\n",
    "df3['er_status'] = df3['er_status'].apply(lambda x: 1 if \"Positive\" in str(x) else 0)\n",
    "dummyList = ['cellularity',\n",
    "             'pam50_+_claudin-low_subtype',\n",
    "             'neoplasm_histologic_grade',\n",
    "             #'cancer_type_detailed',\n",
    "             'tumor_other_histologic_subtype',\n",
    "             'integrative_cluster',\n",
    "             #'gene_classifier_subtype',\n",
    "             'oncotree_code',\n",
    "             'her2_status_measured_by_snp6',\n",
    "             '3-gene_classifier_subtype',\n",
    "             'death_from_cancer'\n",
    "            ]\n",
    "df4 = pd.get_dummies(df3, columns=dummyList)\n",
    "df4['type_of_breast_surgery'] = df4['type_of_breast_surgery'].apply(lambda x: 1 if \"MASTECTOMY\" in str(x) else 0)\n",
    "# df5 = df4.applymap(lambda x: 0 if \"0\" in str(x) else 1)\n",
    "y = df4[\"death_from_cancer_Died of Disease\"]\n",
    "X = df4.drop(columns=[\"overall_survival\", \"death_from_cancer_Died of Other Causes\", \"death_from_cancer_Living\", \"death_from_cancer_Died of Disease\"])\n",
    "clinicalDF= X.iloc[:,:17] # collect relevant clinical modality\n",
    "clinicalDF[\"y\"]= df4[\"death_from_cancer_Died of Disease\"] # adding labels to existing dataFrame\n",
    "clinicalDF[\"dummyTuple\"]= list(zip(clinicalDF.age_at_diagnosis, clinicalDF.nottingham_prognostic_index)) # constructing column to use for ordering\n",
    "# Loading data from .txt files\n",
    "textClinical= pd.DataFrame(data= Clinical)\n",
    "textGeneExp= pd.DataFrame(data= GeneExp)\n",
    "textCNV= pd.DataFrame(data= CNV)\n",
    "textClinical[\"dummyTuple\"]= list(zip(textClinical[4], textClinical[6])) # constructing equivalent index column for the .txt data\n",
    "textCombined= pd.concat([textGeneExp, textCNV], axis= 1) # combining GeneExp and CNV .txt data\n",
    "textCombined[\"dummyTuple\"]= textClinical[\"dummyTuple\"] # appending constructed index to the combined dataFrame\n",
    "tmp= textCombined.dummyTuple.isin(clinicalDF.dummyTuple).astype(int) # for checking which terms don't match up\n",
    "tmp_= clinicalDF.dummyTuple.isin(textCombined.dummyTuple).astype(int) # \"\" \"\" \"\" \n",
    "textCombined['tmp']= tmp # appending this mask\n",
    "clinicalDF['tmp']= tmp_\n",
    "textCombined= textCombined[textCombined['tmp'] == 1] # selectively removing terms on the basis of this mask\n",
    "clinicalDF= clinicalDF[clinicalDF['tmp'] == 1]\n",
    "clinicalDF= clinicalDF.drop_duplicates(subset= [\"dummyTuple\"]) # duplicates exist in the formed index column -- dropping them\n",
    "textCombined= textCombined.drop_duplicates(subset= [\"dummyTuple\"]) # \"\" \"\" \"\" \"\" \n",
    "clinicalDF= clinicalDF.set_index(\"dummyTuple\") # reindexing kaggleClinical data on the basis of the index of the .txt data\n",
    "clinicalDF= clinicalDF.reindex(index= textCombined['dummyTuple'])\n",
    "clinicalDF= clinicalDF.reset_index()\n",
    "textCombined= textCombined.reset_index() # resetting indices to ensure concatentation works as expected\n",
    "finalDF= pd.concat([clinicalDF.drop(columns= [\"dummyTuple\", \"tmp\"]),textCombined.drop(columns= [\"dummyTuple\", \"tmp\"])], axis= 1)\n",
    "# cleaning up resultant DF's\n",
    "finalDF= finalDF.drop(columns= \"index\")\n",
    "Y= finalDF[\"y\"]\n",
    "fullX= finalDF.drop(columns= \"y\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Processing code for combined .txt files:\n",
    "\"\"\"\n",
    "textClinical= pd.DataFrame(data= Clinical)\n",
    "scaler= MinMaxScaler()\n",
    "toTransform= [4, 6, 9, 12]\n",
    "for col in toTransform:\n",
    "    textClinical[col]= scaler.fit_transform(textClinical[col].to_numpy().reshape(-1,1))\n",
    "textCombined= pd.concat([pd.DataFrame(data= Clinical), pd.DataFrame(data= GeneExp), pd.DataFrame(data= CNV)], axis= 1, ignore_index= True)\n",
    "fullX= textCombined.drop(columns= [0,1])\n",
    "Y= textCombined[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clinicalDF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_79263/1551867006.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# outputs 1720 if above code worked correctly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mclinicalDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dummyTuple\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtextCombined\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dummyTuple\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'clinicalDF' is not defined"
     ]
    }
   ],
   "source": [
    "# outputs 1720 if above code worked correctly\n",
    "(clinicalDF[\"dummyTuple\"] == textCombined[\"dummyTuple\"]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(fullX, Y, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.285906</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.563433</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.054945</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.362292</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.565299</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.082418</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.346288</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.569030</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.137363</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.764389</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571269</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.170330</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.464228</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.563433</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.054945</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.393491</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.755970</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.375740</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.758582</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.181319</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.236686</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.752239</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.538731</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.957090</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.543437</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.779851</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.494505</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1980 rows Ã— 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       2    3         4    5         6    7    8         9    10   11  \\\n",
       "0     0.0  1.0  0.285906  1.0  0.563433  1.0  1.0  0.000000  0.0  3.0   \n",
       "1     3.0  1.0  0.362292  1.0  0.565299  1.0  1.0  0.022222  1.0  2.0   \n",
       "2     9.0  1.0  0.346288  1.0  0.569030  1.0  1.0  0.066667  1.0  2.0   \n",
       "3     7.0  1.0  0.764389  0.0  0.571269  1.0  0.0  0.000000  1.0  3.0   \n",
       "4     3.0  1.0  0.464228  1.0  0.563433  1.0  0.0  0.022222  0.0  2.0   \n",
       "...   ...  ...       ...  ...       ...  ...  ...       ...  ...  ...   \n",
       "1975  5.0  1.0  0.393491  1.0  0.755970  1.0  0.0  0.022222  0.0  3.0   \n",
       "1976  8.0  1.0  0.375740  0.0  0.758582  1.0  1.0  0.400000  0.0  2.0   \n",
       "1977  5.0  1.0  0.236686  1.0  0.752239  0.0  1.0  0.022222  0.0  3.0   \n",
       "1978  0.0  1.0  0.538731  1.0  0.957090  1.0  0.0  0.511111  1.0  3.0   \n",
       "1979  5.0  1.0  0.543437  0.0  0.779851  0.0  0.0  0.288889  1.0  2.0   \n",
       "\n",
       "            12   13   14   15   16   17   18   19   20  \n",
       "0     0.054945  1.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0  \n",
       "1     0.082418  2.0  0.0  1.0  1.0  4.0  0.0  1.0  0.0  \n",
       "2     0.137363  2.0  1.0  1.0  1.0  4.0  0.0  1.0  0.0  \n",
       "3     0.170330  4.0  1.0  1.0  1.0  0.0  0.0  1.0  0.0  \n",
       "4     0.054945  2.0  1.0  2.0  1.0  4.0  0.0  1.0  0.0  \n",
       "...        ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "1975  0.142857  2.0  1.0  0.0  1.0  2.0  0.0  0.0  0.0  \n",
       "1976  0.181319  4.0  1.0  1.0  1.0  1.0  0.0  1.0  0.0  \n",
       "1977  0.087912  2.0  1.0  0.0  2.0  2.0  1.0  0.0  1.0  \n",
       "1978  0.357143  3.0  1.0  2.0  2.0  1.0  0.0  1.0  1.0  \n",
       "1979  0.494505  3.0  0.0  0.0  3.0  2.0  1.0  0.0  1.0  \n",
       "\n",
       "[1980 rows x 19 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullX.iloc[:,:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_is= 64\n",
    "# creating torch dataLoaders:\n",
    "train_dataset = data_utils.TensorDataset(torch.Tensor(X_train.to_numpy()), torch.Tensor(y_train.to_numpy()))\n",
    "test_dataset = data_utils.TensorDataset(torch.Tensor(X_test.to_numpy()), torch.Tensor(y_test.to_numpy()))\n",
    "train_loader= data_utils.DataLoader(train_dataset, batch_size =batch_is, pin_memory=True,shuffle=True,num_workers = 1)\n",
    "test_loader= data_utils.DataLoader(test_dataset,batch_size =batch_is,pin_memory=True,shuffle = False,num_workers = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loss function code:\n",
    "def bareCDF(yhat, tau):\n",
    "    # yhat is a torch tensor, tau is a float\n",
    "    ind= (torch.sign(yhat)+1)/2 # mask about the origin\n",
    "    quantFactor= (1-tau)*ind + tau*(1-ind)\n",
    "    val= tau+4*quantFactor/math.pi*torch.atan(torch.tanh(yhat/2))\n",
    "    # print(\"bareCDF val: {}\".format(val))\n",
    "    return val\n",
    "\n",
    "def baresBQR(y, yhat, tau):\n",
    "    # y and yhat are torch tensors, tau is a float\n",
    "    val= torch.matmul(y,torch.log(1-bareCDF(yhat, tau)))+ torch.matmul((1-y),torch.log(bareCDF(yhat, tau)))\n",
    "    # print(\"bareBQR val: {}\".format(val))\n",
    "    return val\n",
    "\n",
    "class sBQRL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(sBQRL, self).__init__()\n",
    "    \n",
    "    def forward(self, y, yhat, tau):\n",
    "        return baresBQR(y, yhat, tau)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Network definition:\n",
    "class LALRnetwork(nn.Module):\n",
    "    def __init__(self, indim):\n",
    "        super(LALRnetwork,self).__init__()\n",
    "        self.l1 = nn.Linear(indim,1500)\n",
    "        self.l2 = nn.Linear(1500,1500)\n",
    "        self.l3 = nn.Linear(1500,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.tanh(self.l1(x))\n",
    "        x = F.tanh(self.l2(x))\n",
    "        # x = F.softmax(self.l3(x))\n",
    "        x = F.sigmoid(self.l3(x))\n",
    "        # x= torch.sign(x-torch.ones_like(x)*0.5)\n",
    "        # x= (x+torch.ones_like(x))/2\n",
    "        return x\n",
    "    \n",
    "    # Used in LALR\n",
    "    def penU(self, x):\n",
    "        op = F.tanh(self.l1(x))\n",
    "        op = F.tanh(self.l2(op))\n",
    "        return op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Global initialisations:\n",
    "device= ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "indim = X_train.shape[1]\n",
    "modelLALR_sBQC = LALRnetwork(indim).to(device)\n",
    "modelCLR_sBQC= LALRnetwork(indim).to(device)\n",
    "modelLALR_BCE= LALRnetwork(indim).to(device)\n",
    "modelCLR_BCE= LALRnetwork(indim).to(device)\n",
    "modelLBFGS_sBQC = LALRnetwork(indim).to(device)\n",
    "\n",
    "criterion= sBQRL()\n",
    "criterion_= nn.BCELoss()\n",
    "h= 0.4\n",
    "lr_is = 1e-2\n",
    "optimizerLALR_sBQC= torch.optim.Adam(modelLALR_sBQC.parameters(), lr = lr_is)\n",
    "optimizerLALR_BCE= torch.optim.Adam(modelLALR_BCE.parameters(), lr = lr_is)\n",
    "optimizerCLR_sBQC= torch.optim.Adam(modelCLR_sBQC.parameters(), lr = lr_is)\n",
    "optimizerCLR_BCE= torch.optim.Adam(modelCLR_BCE.parameters(), lr = lr_is)\n",
    "optimizerLBFGS_sBQC= torch.optim.LBFGS(modelLBFGS_sBQC.parameters(), lr = lr_is)\n",
    "\n",
    "all_qs = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "all_qs = torch.Tensor(all_qs).to(device)\n",
    "mean_is = 0\n",
    "std_is = 1\n",
    "penalty = 1\n",
    "alpha = 0.0\n",
    "tau= 0.2\n",
    "\n",
    "ls_list_LALR_sBQC= []\n",
    "val_list_LALR_sBQC= []\n",
    "acc_list_LALR_sBQC= []\n",
    "ls_list_CLR_sBQC= []\n",
    "val_list_CLR_sBQC= []\n",
    "acc_list_CLR_sBQC= []\n",
    "ls_list_LALR_BCE= []\n",
    "val_list_LALR_BCE= []\n",
    "acc_list_LALR_BCE= []\n",
    "ls_list_CLR_BCE= []\n",
    "val_list_CLR_BCE= []\n",
    "acc_list_CLR_BCE= []\n",
    "\n",
    "ls_list_LBFGS_sBQC= []\n",
    "val_list_LBFGS_sBQC= []\n",
    "acc_list_LBFGS_sBQC= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training loops:\n",
    "def trainConstantLR(model, trainLoader, valLoader, optimizer, criterion, tau, epochs, ls_list, valList, acc_list, loss_name= \"sBQC\"):\n",
    "    \"\"\"\n",
    "    Training loop used for CLR training\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss= 0.0\n",
    "        # training loop\n",
    "        model.train()\n",
    "        for inputs, labels in trainLoader: \n",
    "            inputs= inputs.to(device) \n",
    "            labels= labels.to(device)\n",
    "            optimizer.zero_grad() \n",
    "            outputs= model(inputs) \n",
    "            if loss_name== \"BCE\":\n",
    "                loss= criterion(outputs.view(outputs.shape[0],), labels) # For BCE\n",
    "            elif loss_name== \"sBQC\":\n",
    "                loss= criterion(labels, outputs.view(outputs.shape[0],), tau) # For sBQC\n",
    "            loss.backward() \n",
    "            optimizer.step() \n",
    "            epoch_loss+= loss.item()\n",
    "        ls_list.append(epoch_loss/len(trainLoader))\n",
    "\n",
    "        # validation loop\n",
    "        val_loss= 0.0\n",
    "        num_correct= 0\n",
    "        total= 0 \n",
    "        model.eval()\n",
    "        for inputs, labels in valLoader:\n",
    "            inputs= inputs.to(device)\n",
    "            labels= labels.to(device)\n",
    "            outputs= model(inputs)\n",
    "            if loss_name== \"BCE\":\n",
    "                loss= criterion(outputs.view(outputs.shape[0],), labels) # For BCE\n",
    "            elif loss_name== \"sBQC\":\n",
    "                loss= criterion(labels, outputs.view(outputs.shape[0],), tau) # For sBQC\n",
    "            val_loss+= loss.item()\n",
    "            x= torch.where(outputs.view(outputs.shape[0]) > 0.5, 1, 0)\n",
    "            num_correct += (x==labels).sum()\n",
    "            total += labels.size(0)\n",
    "        valList.append(val_loss/len(valLoader))\n",
    "        acc_list.append(float(num_correct)/float(total)*100)\n",
    "        print(\"Epoch: {} Training Loss: {} Validation loss: {} Accuracy: {}\".format(epoch, epoch_loss/len(trainLoader), val_loss/len(valLoader),\n",
    "         float(num_correct)/float(total)*100))\n",
    "\n",
    "\n",
    "def trainLALR(model, trainLoader, valLoader, optimizer, criterion, tau, epochs, ls_list, valList, acc_list, mask, loss_name= \"sBQC\"):\n",
    "    \"\"\"\n",
    "    Training loop used for LALR training\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss= 0.0\n",
    "        lr_val= computeLR(model,train_loader, mask, tau, bSize= batch_is)\n",
    "        optimizer.param_groups[0]['lr']= lr_val\n",
    "        # training loop\n",
    "        model.train()\n",
    "        for inputs, labels in trainLoader: \n",
    "            inputs= inputs.to(device) \n",
    "            labels= labels.to(device)\n",
    "            optimizer.zero_grad() \n",
    "            outputs= model(inputs) \n",
    "            if loss_name== \"BCE\":\n",
    "                loss= criterion(outputs.view(outputs.shape[0],), labels) # For BCE\n",
    "            elif loss_name== \"sBQC\":\n",
    "                loss= criterion(labels, outputs.view(outputs.shape[0],), tau) # For sBQC\n",
    "            loss.backward() \n",
    "            optimizer.step() \n",
    "            epoch_loss+= loss.item()\n",
    "        ls_list.append(epoch_loss/len(trainLoader))\n",
    "\n",
    "        # validation loop\n",
    "        val_loss= 0.0\n",
    "        num_correct= 0\n",
    "        total= 0 \n",
    "        model.eval()\n",
    "        for inputs, labels in valLoader:\n",
    "            inputs= inputs.to(device)\n",
    "            labels= labels.to(device)\n",
    "            outputs= model(inputs)\n",
    "            if loss_name== \"BCE\":\n",
    "                loss= criterion_(outputs.view(outputs.shape[0],), labels) # For BCE\n",
    "            elif loss_name== \"sBQC\":\n",
    "                loss= criterion(labels, outputs.view(outputs.shape[0],), tau) # For sBQC\n",
    "            val_loss+= loss.item()\n",
    "            x= torch.where(outputs.view(outputs.shape[0]) > 0.5, 1, 0)\n",
    "            num_correct += (x==labels).sum()\n",
    "            total += labels.size(0)\n",
    "        valList.append(val_loss/len(valLoader))\n",
    "        acc_list.append(float(num_correct)/float(total)*100)\n",
    "        print(\"Epoch: {} Training Loss: {} Validation loss: {} LR: {} Accuracy: {}\".format(epoch, epoch_loss/len(trainLoader), val_loss/len(valLoader), optimizer.param_groups[0]['lr'], float(num_correct)/float(total)*100))\n",
    "\n",
    "def trainLBFGS(model, trainLoader, valLoader, optimizer, criterion, tau, epochs, ls_list, valList, acc_list, loss_name= \"sBQC\"):\n",
    "    \"\"\"\n",
    "    Training loop used for LBFGS and conjugate gradient training\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss= 0.0\n",
    "        # training loop\n",
    "        model.train()\n",
    "        for inputs, labels in trainLoader: \n",
    "            inputs= inputs.to(device) \n",
    "            labels= labels.to(device)\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                outputs= model(inputs)\n",
    "                if loss_name== \"BCE\":\n",
    "                    loss= criterion(outputs.view(outputs.shape[0],), labels) # For BCE\n",
    "                elif loss_name== \"sBQC\":\n",
    "                    loss= criterion(labels, outputs.view(outputs.shape[0],), tau) # For sBQC\n",
    "                loss.backward()\n",
    "                return loss\n",
    "            optimizer.step(closure) \n",
    "\n",
    "        # validation loop\n",
    "        val_loss= 0.0\n",
    "        num_correct= 0\n",
    "        total= 0 \n",
    "        model.eval()\n",
    "        for inputs, labels in valLoader:\n",
    "            inputs= inputs.to(device)\n",
    "            labels= labels.to(device)\n",
    "            outputs= model(inputs)\n",
    "            if loss_name== \"BCE\":\n",
    "                loss= criterion(outputs.view(outputs.shape[0],), labels) # For BCE\n",
    "            elif loss_name== \"sBQC\":\n",
    "                loss= criterion(labels, outputs.view(outputs.shape[0],), tau) # For sBQC\n",
    "            val_loss+= loss.item()\n",
    "            x= torch.where(outputs.view(outputs.shape[0]) > 0.5, 1, 0)\n",
    "            num_correct += (x==labels).sum()\n",
    "            total += labels.size(0)\n",
    "        valList.append(val_loss/len(valLoader))\n",
    "        acc_list.append(float(num_correct)/float(total)*100)\n",
    "        print(\"Epoch: {} Training Loss: {} Validation loss: {} Accuracy: {}\".format(epoch, epoch_loss/len(trainLoader), val_loss/len(valLoader),\n",
    "         float(num_correct)/float(total)*100))\n",
    "\n",
    "\n",
    "def computeLR(model, trainLoader, mask, tau, bSize= 16):\n",
    "    \"\"\"\n",
    "    Takes in a network of the LALRnetwork class(during some arbitrary EPOCH of training) and the current input, and returns Kz for the EPOCH\n",
    "    \"\"\"\n",
    "    Kz = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i,j in enumerate(trainLoader):\n",
    "            inputs,labels= j[0],j[1]\n",
    "            inputs= inputs.to(device)\n",
    "            labels= labels.to(device)\n",
    "            op1= model.penU(inputs)\n",
    "            op2= model(inputs)\n",
    "            val1= torch.linalg.norm(op1)\n",
    "            if val1 > Kz:\n",
    "                Kz= val1 \n",
    "    LR= 1\n",
    "    factor= 1\n",
    "    if mask== 1: \n",
    "        LR=  max(2/math.pi, 2-2*tau/(math.pi*tau), 2*tau/(math.pi*(1-tau)))*Kz*(1/bSize)\n",
    "        # factor= 0.15\n",
    "    else:\n",
    "        LR= 0.5*Kz*(1/bSize)\n",
    "        # factor= 0.05\n",
    "\n",
    "    return (1/LR)*factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aryamanj/miniconda3/lib/python3.9/site-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/home/aryamanj/miniconda3/lib/python3.9/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_65438/2784884612.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainLBFGS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelLBFGS_sBQC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizerLBFGS_sBQC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mls_list_LBFGS_sBQC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_list_LBFGS_sBQC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_list_LBFGS_sBQC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sBQC\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_65438/3547269055.py\u001b[0m in \u001b[0;36mtrainLBFGS\u001b[0;34m(model, trainLoader, valLoader, optimizer, criterion, tau, epochs, ls_list, valList, acc_list, loss_name)\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# validation loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    435\u001b[0m                     \u001b[0;31m# no use to re-evaluate that function here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m                         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m                     \u001b[0mflat_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_flat_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m                     \u001b[0mopt_cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mtolerance_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainLBFGS(modelLBFGS_sBQC, train_loader, test_loader, optimizerLBFGS_sBQC, criterion, tau, 10, ls_list_LBFGS_sBQC, val_list_LBFGS_sBQC, acc_list_LBFGS_sBQC, \"sBQC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training Loss: -54.2373664855957 Validation loss: -48.90763977595738 LR: 0.15150514245033264 Accuracy: 67.92929292929293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Training Loss: -54.23736602783203 Validation loss: -48.90763977595738 LR: 0.15150514245033264 Accuracy: 67.92929292929293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Training Loss: -54.23736633300781 Validation loss: -48.90763977595738 LR: 0.15150514245033264 Accuracy: 67.92929292929293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Training Loss: -54.23736602783203 Validation loss: -48.90763977595738 LR: 0.15150514245033264 Accuracy: 67.92929292929293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Training Loss: -54.23736572265625 Validation loss: -48.90763977595738 LR: 0.15150514245033264 Accuracy: 67.92929292929293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Training Loss: -54.23736602783203 Validation loss: -48.90763977595738 LR: 0.15150514245033264 Accuracy: 67.92929292929293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Training Loss: -54.237366638183595 Validation loss: -48.90763977595738 LR: 0.15150514245033264 Accuracy: 67.92929292929293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Training Loss: -54.23736572265625 Validation loss: -48.90763977595738 LR: 0.15150514245033264 Accuracy: 67.92929292929293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Training Loss: -54.23736633300781 Validation loss: -48.90763977595738 LR: 0.15150514245033264 Accuracy: 67.92929292929293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Training Loss: -54.23736572265625 Validation loss: -48.90763977595738 LR: 0.15150514245033264 Accuracy: 67.92929292929293\n"
     ]
    }
   ],
   "source": [
    "# LALR, sBQC\n",
    "# tracker.start()\n",
    "trainLALR(modelLALR_sBQC, train_loader, test_loader, optimizerLALR_sBQC, criterion, tau, 10, ls_list_LALR_sBQC, val_list_LALR_sBQC, acc_list_LALR_sBQC, 1, \"sBQC\")\n",
    "# emissions= tracker.stop()\n",
    "# print(emissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tracker= OfflineEmissionsTracker(country_iso_code= \"IND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training Loss: -52.99820724487304 Validation loss: -46.98584883553641 Accuracy: 66.66666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Training Loss: -52.998207550048825 Validation loss: -46.98583085196359 Accuracy: 66.66666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Training Loss: -52.99820693969727 Validation loss: -46.98581177847726 Accuracy: 66.66666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Training Loss: -52.99820739746094 Validation loss: -46.985786710466655 Accuracy: 66.66666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Training Loss: -52.99820663452149 Validation loss: -46.98576818193708 Accuracy: 66.66666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Training Loss: -52.998206329345706 Validation loss: -46.98575292314802 Accuracy: 66.66666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Training Loss: -52.998206787109375 Validation loss: -46.98573330470494 Accuracy: 66.66666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Training Loss: -52.998207092285156 Validation loss: -46.98571695600237 Accuracy: 66.66666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Training Loss: -52.998207092285156 Validation loss: -46.985700607299805 Accuracy: 66.66666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Training Loss: -52.998206787109375 Validation loss: -46.98568207877023 Accuracy: 66.66666666666666\n"
     ]
    }
   ],
   "source": [
    "# CLR, sBQC\n",
    "# tracker.start()\n",
    "trainConstantLR(modelCLR_sBQC, train_loader, test_loader, optimizerCLR_sBQC, criterion, tau, 10, ls_list_CLR_sBQC, val_list_CLR_sBQC, acc_list_CLR_sBQC, \"sBQC\")\n",
    "# emissions= tracker.stop()\n",
    "# print(emissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training Loss: 32.45833343505859 Validation loss: 34.30059541974749 LR: 0.41311824321746826 Accuracy: 66.66666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Training Loss: 32.45833343505859 Validation loss: 34.30059541974749 LR: 0.41311824321746826 Accuracy: 66.66666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Training Loss: 32.375 Validation loss: 34.30059541974749 LR: 0.41311824321746826 Accuracy: 66.66666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Training Loss: 32.5625 Validation loss: 34.30059541974749 LR: 0.41311824321746826 Accuracy: 66.66666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Training Loss: 32.5 Validation loss: 34.30059541974749 LR: 0.41311824321746826 Accuracy: 66.66666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Training Loss: 32.479166717529296 Validation loss: 34.30059541974749 LR: 0.41311824321746826 Accuracy: 66.66666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Training Loss: 32.39583335876465 Validation loss: 34.30059541974749 LR: 0.41311824321746826 Accuracy: 66.66666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Training Loss: 32.416666717529296 Validation loss: 34.30059541974749 LR: 0.41311824321746826 Accuracy: 66.66666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Training Loss: 32.375 Validation loss: 34.30059541974749 LR: 0.41311824321746826 Accuracy: 66.66666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Training Loss: 32.45833343505859 Validation loss: 34.30059541974749 LR: 0.41311824321746826 Accuracy: 66.66666666666666\n"
     ]
    }
   ],
   "source": [
    "# LALR BCE\n",
    "trainLALR(modelLALR_BCE, train_loader, test_loader, optimizerLALR_BCE, criterion_, tau, 10, ls_list_LALR_BCE, val_list_LALR_BCE, acc_list_LALR_BCE, 2, \"BCE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training Loss: 0.1559533405303955 Validation loss: 1.6228360789162772 Accuracy: 64.39393939393939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Training Loss: 0.12344533950090408 Validation loss: 1.706842643874032 Accuracy: 65.65656565656566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Training Loss: 0.10109413743019104 Validation loss: 2.0157253742218018 Accuracy: 68.43434343434343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Training Loss: 0.09247495770454407 Validation loss: 2.4977846145629883 Accuracy: 61.61616161616161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Training Loss: 0.07711075592786074 Validation loss: 3.0457121644701277 Accuracy: 68.43434343434343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Training Loss: 0.05175116047263145 Validation loss: 4.591686402048383 Accuracy: 64.39393939393939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Training Loss: 0.025274073742330073 Validation loss: 4.294656566211155 Accuracy: 68.43434343434343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Training Loss: 0.015890953028574586 Validation loss: 3.132297158241272 Accuracy: 64.39393939393939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Training Loss: 0.010366284605115652 Validation loss: 4.545402032988412 Accuracy: 65.40404040404042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Training Loss: 0.006581745324656368 Validation loss: 4.626339537756784 Accuracy: 66.41414141414141\n"
     ]
    }
   ],
   "source": [
    "# CLR BCE\n",
    "trainConstantLR(modelCLR_BCE, train_loader, test_loader, optimizerCLR_BCE, criterion_, tau, 10, ls_list_CLR_BCE, val_list_CLR_BCE, acc_list_CLR_BCE, \"BCE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f3da6923820>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# error plots\n",
    "# plt.plot(range(10), ls_list_LALR_sBQC)\n",
    "# plt.plot(range(len(val_list_LALR_sBQC)), val_list_LALR_sBQC, label= \"LALR_sBQC\")\n",
    "# plt.plot(range(len(val_list_CLR_sBQC)), val_list_CLR_sBQC, label= \"CLR_sBQC\")\n",
    "# plt.plot(range(len(val_list_LALR_BCE)), val_list_LALR_BCE, label= \"LALR_BCE\")\n",
    "# plt.plot(range(len(val_list_CLR_BCE)), val_list_CLR_BCE, label= \"CLR_BCE\")\n",
    "# plt.plot(range(len(acc_list_CLR_BCE)), acc_list_CLR_BCE, label= \"CLR_BCE_acc\")\n",
    "plt.plot(range(len(acc_list_CLR_sBQC)), acc_list_CLR_sBQC, label= \"CLR_sBQC_acc\")\n",
    "plt.plot(range(len(acc_list_LALR_BCE)), acc_list_LALR_BCE, label= \"LALR_BCE_acc\")\n",
    "plt.plot(range(len(acc_list_LALR_sBQC)), acc_list_LALR_sBQC, label= \"LALR_sBQC_acc\")\n",
    "# plt.plot(range(len(val_list_LBFGS_sBQC)), val_list_LBFGS_sBQC, label= \"LBFGS_sBQC_val\")\n",
    "# plt.plot(range(len(acc_list_LBFGS_sBQC)), acc_list_LBFGS_sBQC, label= \"LBFGS_sBQC_acc\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation loss\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/home/aryamanj/miniconda3/bin/python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (ipykernel)",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": {
    "debugger": true
   },
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "name": "breastCancer.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
