{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from adahessian import Adahessian, get_params_grad\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from UCI_loader import UCIDatasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataClass= UCIDatasets(\"energy\", data_path=\"\", n_splits=10) # has a field named 'data' that contains the data\n",
    "\n",
    "trainData= dataClass.get_split(train=True)\n",
    "trainLoader= torch.utils.data.DataLoader(trainData, batch_size= dataClass.data.shape[0], shuffle= True) # creating a loader with full batch size to ensure LBFGS works\n",
    "testData= dataClass.get_split(train= False)\n",
    "testLoader= torch.utils.data.DataLoader(testData, batch_size= dataClass.data.shape[0], shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inDim, outDim= 8, 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining all the criterions to be used in the following experiments:\n",
    "def tiltedLC(x, y, tau, h):\n",
    "    e= y-x # errors\n",
    "    ind= (torch.sign(e)+1)/2 # the division in the log-cosh is only about the origin\n",
    "    quantFactor= (1-tau)*(1-ind) + tau*ind\n",
    "    loss= quantFactor*torch.log(torch.cosh(e))\n",
    "    loss= torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "class TiltedLC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TiltedLC, self).__init__()\n",
    "    def forward(self, x, y, tau, h):\n",
    "        return tiltedLC(x, y, tau, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global initialisations:\n",
    "h= 0.4 # smoothing parameter for the log-cosh \n",
    "tau= 0.5\n",
    "device= ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion1= TiltedLC()\n",
    "criterion2= nn.MSELoss()\n",
    "criterion3= nn.L1Loss()\n",
    "N_EPOCHS= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LALRnetwork(nn.Module):\n",
    "    def __init__(self, size1, size2, inDim, outDim, drop):\n",
    "        super(LALRnetwork, self).__init__()\n",
    "        self.l1= nn.Linear(inDim, size1)\n",
    "        self.l2= nn.Dropout(p= drop)\n",
    "        self.l3= nn.Linear(size1, size2)\n",
    "        self.l4= nn.Dropout(p= drop)\n",
    "        self.l5= nn.Linear(size2, outDim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x= F.tanh(self.l1(x))\n",
    "        x= F.tanh(self.l3(self.l2(x)))\n",
    "        x= self.l5(self.l4(x))\n",
    "        return x\n",
    "    \n",
    "    def penU(self, x):\n",
    "        x= F.tanh(self.l2(self.l1(x)))\n",
    "        x= F.tanh(self.l4(self.l3(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "size1, size2= 300, 300\n",
    "model_LBFGS_LC= LALRnetwork(size1, size2, inDim, outDim, 0.1).to(device)\n",
    "optimizer_LBFGS_LC= optim.LBFGS(model_LBFGS_LC.parameters())\n",
    "lossList_LBFGS_LC= []\n",
    "valList_LBFGS_LC= []\n",
    "\n",
    "model_LBFGS_MSE= LALRnetwork(size1, size2, inDim, outDim, 0.1).to(device)\n",
    "optimizer_LBFGS_MSE= optim.LBFGS(model_LBFGS_MSE.parameters())\n",
    "lossList_LBFGS_MSE= []\n",
    "valList_LBFGS_MSE= []\n",
    "\n",
    "model_CLR_L1= LALRnetwork(size1, size2, inDim, outDim, 0.1).to(device)\n",
    "optimizer_CLR_L1= optim.SGD(model_CLR_L1.parameters(), lr= 0.1)\n",
    "lossList_CLR_L1= []\n",
    "valList_CLR_L1= []\n",
    "\n",
    "model_CLR_LC= LALRnetwork(size1, size2, inDim, outDim, 0.1).to(device)\n",
    "optimizer_CLR_LC= optim.SGD(model_CLR_LC.parameters(), lr= 0.1)\n",
    "lossList_CLR_LC= []\n",
    "valList_CLR_LC= []\n",
    "\n",
    "model_CLR_MSE= LALRnetwork(size1, size2, inDim, outDim, 0.1).to(device)\n",
    "optimizer_CLR_MSE= optim.SGD(model_CLR_MSE.parameters(), lr= 0.1)\n",
    "lossList_CLR_MSE= []\n",
    "valList_CLR_MSE= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainLBFGS(model, optimizer, criterion, tau, epochs, ls_list, valList, loss_name):\n",
    "    \"\"\"\n",
    "    Training loop used for LBFGS and conjugate gradient training\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss= 0.0\n",
    "        # training loop\n",
    "        model.train()\n",
    "        for inputs, labels in trainLoader: \n",
    "            inputs= inputs.to(device) \n",
    "            labels= labels.to(device)\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                outputs= model(inputs)\n",
    "                if loss_name== \"MSE\":\n",
    "                    loss= criterion(outputs, labels)\n",
    "                else:\n",
    "                    loss= criterion(outputs, labels, tau, h)\n",
    "                loss.backward()\n",
    "                return loss\n",
    "            # optimizer.step(closure)\n",
    "            # optimizer.zero_grad() \n",
    "            # outputs= model(inputs) \n",
    "            # loss= criterion(outputs, labels, tau, h) \n",
    "            # loss.backward()\n",
    "            optimizer.step(closure) \n",
    "        # ls_list.append(epoch_loss/len(trainLoader))\n",
    "\n",
    "        # validation loop\n",
    "        val_loss= 0.0\n",
    "        model.eval()\n",
    "        for inputs, labels in testLoader:\n",
    "            inputs= inputs.to(device)\n",
    "            labels= labels.to(device)\n",
    "            outputs= model(inputs)\n",
    "            loss= torch.sqrt(criterion2(outputs, labels))\n",
    "            val_loss+= loss.item()\n",
    "        valList.append(val_loss/len(testLoader))\n",
    "        print(\"Epoch: {} Training loss: {} Validation loss: {}\".format(epoch, epoch_loss/len(trainLoader), val_loss/len(testLoader)))\n",
    "        \n",
    "def trainConstantLR(model, optimizer, criterion, tau, epochs, ls_list, valList, loss_name):\n",
    "    \"\"\"\n",
    "    Training loop used for constantLR\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss= 0.0\n",
    "        # training loop\n",
    "        model.train()\n",
    "        for inputs, labels in trainLoader: \n",
    "            inputs= inputs.to(device) \n",
    "            labels= labels.to(device)\n",
    "            optimizer.zero_grad() \n",
    "            outputs= model(inputs) \n",
    "            if loss_name== \"LC\":\n",
    "                loss= criterion(outputs, labels, tau, h)\n",
    "            else:\n",
    "                loss= criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "            epoch_loss+= loss.item()\n",
    "        ls_list.append(epoch_loss/len(trainLoader))\n",
    "\n",
    "        # validation loop\n",
    "        val_loss= 0.0\n",
    "        model.eval()\n",
    "        for inputs, labels in testLoader:\n",
    "            inputs= inputs.to(device)\n",
    "            labels= labels.to(device)\n",
    "            outputs= model(inputs)\n",
    "            loss= torch.sqrt(criterion2(outputs, labels))\n",
    "            val_loss+= loss.item()\n",
    "        valList.append(val_loss/len(testLoader))\n",
    "        print(\"Epoch: {} Training loss: {} Validation loss: {}\".format(epoch, epoch_loss/len(trainLoader), val_loss/len(testLoader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aryamanj/miniconda3/lib/python3.9/site-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training loss: 0.0 Validation loss: 0.3282132148742676\n",
      "Epoch: 1 Training loss: 0.0 Validation loss: 0.31446823477745056\n",
      "Epoch: 2 Training loss: 0.0 Validation loss: 0.29763635993003845\n",
      "Epoch: 3 Training loss: 0.0 Validation loss: 0.2670964300632477\n",
      "Epoch: 4 Training loss: 0.0 Validation loss: 0.22761330008506775\n",
      "Epoch: 5 Training loss: 0.0 Validation loss: 0.20399606227874756\n",
      "Epoch: 6 Training loss: 0.0 Validation loss: 0.18691617250442505\n",
      "Epoch: 7 Training loss: 0.0 Validation loss: 0.17712640762329102\n",
      "Epoch: 8 Training loss: 0.0 Validation loss: 0.19225065410137177\n",
      "Epoch: 9 Training loss: 0.0 Validation loss: 0.16895215213298798\n",
      "Epoch: 10 Training loss: 0.0 Validation loss: 0.1661866158246994\n",
      "Epoch: 11 Training loss: 0.0 Validation loss: 0.16492925584316254\n",
      "Epoch: 12 Training loss: 0.0 Validation loss: 0.15785811841487885\n",
      "Epoch: 13 Training loss: 0.0 Validation loss: 0.15650437772274017\n",
      "Epoch: 14 Training loss: 0.0 Validation loss: 0.15457773208618164\n",
      "Epoch: 15 Training loss: 0.0 Validation loss: 0.1495630443096161\n",
      "Epoch: 16 Training loss: 0.0 Validation loss: 0.15124838054180145\n",
      "Epoch: 17 Training loss: 0.0 Validation loss: 0.15246747434139252\n",
      "Epoch: 18 Training loss: 0.0 Validation loss: 0.149846151471138\n",
      "Epoch: 19 Training loss: 0.0 Validation loss: 0.14970020949840546\n",
      "Epoch: 20 Training loss: 0.0 Validation loss: 0.14701449871063232\n",
      "Epoch: 21 Training loss: 0.0 Validation loss: 0.146750345826149\n",
      "Epoch: 22 Training loss: 0.0 Validation loss: 0.14503975212574005\n",
      "Epoch: 23 Training loss: 0.0 Validation loss: 0.14578592777252197\n",
      "Epoch: 24 Training loss: 0.0 Validation loss: 0.14387919008731842\n",
      "Epoch: 25 Training loss: 0.0 Validation loss: 0.14693297445774078\n",
      "Epoch: 26 Training loss: 0.0 Validation loss: 0.14412282407283783\n",
      "Epoch: 27 Training loss: 0.0 Validation loss: 0.14301583170890808\n",
      "Epoch: 28 Training loss: 0.0 Validation loss: 0.14216728508472443\n",
      "Epoch: 29 Training loss: 0.0 Validation loss: 0.14185546338558197\n",
      "Epoch: 30 Training loss: 0.0 Validation loss: 0.1390218585729599\n",
      "Epoch: 31 Training loss: 0.0 Validation loss: 0.1389201581478119\n",
      "Epoch: 32 Training loss: 0.0 Validation loss: 0.138339564204216\n",
      "Epoch: 33 Training loss: 0.0 Validation loss: 0.13950377702713013\n",
      "Epoch: 34 Training loss: 0.0 Validation loss: 0.13688965141773224\n",
      "Epoch: 35 Training loss: 0.0 Validation loss: 0.13646875321865082\n",
      "Epoch: 36 Training loss: 0.0 Validation loss: 0.1354372352361679\n",
      "Epoch: 37 Training loss: 0.0 Validation loss: 0.13654229044914246\n",
      "Epoch: 38 Training loss: 0.0 Validation loss: 0.13474278151988983\n",
      "Epoch: 39 Training loss: 0.0 Validation loss: 0.1338864266872406\n",
      "Epoch: 40 Training loss: 0.0 Validation loss: 0.1333298534154892\n",
      "Epoch: 41 Training loss: 0.0 Validation loss: 0.13306081295013428\n",
      "Epoch: 42 Training loss: 0.0 Validation loss: 0.1335463672876358\n",
      "Epoch: 43 Training loss: 0.0 Validation loss: 0.1332533210515976\n",
      "Epoch: 44 Training loss: 0.0 Validation loss: 0.13399238884449005\n",
      "Epoch: 45 Training loss: 0.0 Validation loss: 0.13333939015865326\n",
      "Epoch: 46 Training loss: 0.0 Validation loss: 0.13145242631435394\n",
      "Epoch: 47 Training loss: 0.0 Validation loss: 0.13122084736824036\n",
      "Epoch: 48 Training loss: 0.0 Validation loss: 0.13107021152973175\n",
      "Epoch: 49 Training loss: 0.0 Validation loss: 0.12948903441429138\n",
      "Epoch: 50 Training loss: 0.0 Validation loss: 0.1300279200077057\n",
      "Epoch: 51 Training loss: 0.0 Validation loss: 0.13380898535251617\n",
      "Epoch: 52 Training loss: 0.0 Validation loss: 0.12805786728858948\n",
      "Epoch: 53 Training loss: 0.0 Validation loss: 0.12679636478424072\n",
      "Epoch: 54 Training loss: 0.0 Validation loss: 0.12569601833820343\n",
      "Epoch: 55 Training loss: 0.0 Validation loss: 0.12689003348350525\n",
      "Epoch: 56 Training loss: 0.0 Validation loss: 0.12654761970043182\n",
      "Epoch: 57 Training loss: 0.0 Validation loss: 0.12760233879089355\n",
      "Epoch: 58 Training loss: 0.0 Validation loss: 0.1262432336807251\n",
      "Epoch: 59 Training loss: 0.0 Validation loss: 0.12551486492156982\n",
      "Epoch: 60 Training loss: 0.0 Validation loss: 0.12609940767288208\n",
      "Epoch: 61 Training loss: 0.0 Validation loss: 0.1241464838385582\n",
      "Epoch: 62 Training loss: 0.0 Validation loss: 0.12532512843608856\n",
      "Epoch: 63 Training loss: 0.0 Validation loss: 0.12518210709095\n",
      "Epoch: 64 Training loss: 0.0 Validation loss: 0.12495731562376022\n",
      "Epoch: 65 Training loss: 0.0 Validation loss: 0.12520964443683624\n",
      "Epoch: 66 Training loss: 0.0 Validation loss: 0.12454431504011154\n",
      "Epoch: 67 Training loss: 0.0 Validation loss: 0.12517020106315613\n",
      "Epoch: 68 Training loss: 0.0 Validation loss: 0.12492822855710983\n",
      "Epoch: 69 Training loss: 0.0 Validation loss: 0.129716157913208\n",
      "Epoch: 70 Training loss: 0.0 Validation loss: 0.12484122812747955\n",
      "Epoch: 71 Training loss: 0.0 Validation loss: 0.12373628467321396\n",
      "Epoch: 72 Training loss: 0.0 Validation loss: 0.12321831285953522\n",
      "Epoch: 73 Training loss: 0.0 Validation loss: 0.12276550382375717\n",
      "Epoch: 74 Training loss: 0.0 Validation loss: 0.12203552573919296\n",
      "Epoch: 75 Training loss: 0.0 Validation loss: 0.1230153739452362\n",
      "Epoch: 76 Training loss: 0.0 Validation loss: 0.12474055588245392\n",
      "Epoch: 77 Training loss: 0.0 Validation loss: 0.12198682874441147\n",
      "Epoch: 78 Training loss: 0.0 Validation loss: 0.12197712063789368\n",
      "Epoch: 79 Training loss: 0.0 Validation loss: 0.12259773164987564\n",
      "Epoch: 80 Training loss: 0.0 Validation loss: 0.12354039400815964\n",
      "Epoch: 81 Training loss: 0.0 Validation loss: 0.12279929965734482\n",
      "Epoch: 82 Training loss: 0.0 Validation loss: 0.12291291356086731\n",
      "Epoch: 83 Training loss: 0.0 Validation loss: 0.12301008403301239\n",
      "Epoch: 84 Training loss: 0.0 Validation loss: 0.12193258106708527\n",
      "Epoch: 85 Training loss: 0.0 Validation loss: 0.12368807196617126\n",
      "Epoch: 86 Training loss: 0.0 Validation loss: 0.12023110687732697\n",
      "Epoch: 87 Training loss: 0.0 Validation loss: 0.12140139937400818\n",
      "Epoch: 88 Training loss: 0.0 Validation loss: 0.12101847678422928\n",
      "Epoch: 89 Training loss: 0.0 Validation loss: 0.12108003348112106\n",
      "Epoch: 90 Training loss: 0.0 Validation loss: 0.12156087905168533\n",
      "Epoch: 91 Training loss: 0.0 Validation loss: 0.12101440876722336\n",
      "Epoch: 92 Training loss: 0.0 Validation loss: 0.12061973661184311\n",
      "Epoch: 93 Training loss: 0.0 Validation loss: 0.12013471126556396\n",
      "Epoch: 94 Training loss: 0.0 Validation loss: 0.11998715996742249\n",
      "Epoch: 95 Training loss: 0.0 Validation loss: 0.1196947991847992\n",
      "Epoch: 96 Training loss: 0.0 Validation loss: 0.11977288126945496\n",
      "Epoch: 97 Training loss: 0.0 Validation loss: 0.11924012750387192\n",
      "Epoch: 98 Training loss: 0.0 Validation loss: 0.11866604536771774\n",
      "Epoch: 99 Training loss: 0.0 Validation loss: 0.119219571352005\n"
     ]
    }
   ],
   "source": [
    "# LBFGS, LC\n",
    "trainLBFGS(model_LBFGS_LC, optimizer_LBFGS_LC, criterion1, tau,  N_EPOCHS, lossList_LBFGS_LC, valList_LBFGS_LC, \"LC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training loss: 0.0 Validation loss: 0.3298576772212982\n",
      "Epoch: 1 Training loss: 0.0 Validation loss: 0.3177228569984436\n",
      "Epoch: 2 Training loss: 0.0 Validation loss: 0.30034708976745605\n",
      "Epoch: 3 Training loss: 0.0 Validation loss: 0.2673923969268799\n",
      "Epoch: 4 Training loss: 0.0 Validation loss: 0.2226599007844925\n",
      "Epoch: 5 Training loss: 0.0 Validation loss: 0.19695940613746643\n",
      "Epoch: 6 Training loss: 0.0 Validation loss: 0.17282728850841522\n",
      "Epoch: 7 Training loss: 0.0 Validation loss: 0.1654045432806015\n",
      "Epoch: 8 Training loss: 0.0 Validation loss: 0.16192102432250977\n",
      "Epoch: 9 Training loss: 0.0 Validation loss: 0.1597985476255417\n",
      "Epoch: 10 Training loss: 0.0 Validation loss: 0.15620875358581543\n",
      "Epoch: 11 Training loss: 0.0 Validation loss: 0.15327146649360657\n",
      "Epoch: 12 Training loss: 0.0 Validation loss: 0.1523575633764267\n",
      "Epoch: 13 Training loss: 0.0 Validation loss: 0.1536063253879547\n",
      "Epoch: 14 Training loss: 0.0 Validation loss: 0.1494077742099762\n",
      "Epoch: 15 Training loss: 0.0 Validation loss: 0.1499895602464676\n",
      "Epoch: 16 Training loss: 0.0 Validation loss: 0.14608702063560486\n",
      "Epoch: 17 Training loss: 0.0 Validation loss: 0.1450245976448059\n",
      "Epoch: 18 Training loss: 0.0 Validation loss: 0.1439696103334427\n",
      "Epoch: 19 Training loss: 0.0 Validation loss: 0.14392919838428497\n",
      "Epoch: 20 Training loss: 0.0 Validation loss: 0.14082825183868408\n",
      "Epoch: 21 Training loss: 0.0 Validation loss: 0.13919486105442047\n",
      "Epoch: 22 Training loss: 0.0 Validation loss: 0.13926012814044952\n",
      "Epoch: 23 Training loss: 0.0 Validation loss: 0.13878439366817474\n",
      "Epoch: 24 Training loss: 0.0 Validation loss: 0.13694334030151367\n",
      "Epoch: 25 Training loss: 0.0 Validation loss: 0.13758601248264313\n",
      "Epoch: 26 Training loss: 0.0 Validation loss: 0.13561953604221344\n",
      "Epoch: 27 Training loss: 0.0 Validation loss: 0.1343354731798172\n",
      "Epoch: 28 Training loss: 0.0 Validation loss: 0.1321115791797638\n",
      "Epoch: 29 Training loss: 0.0 Validation loss: 0.1423444151878357\n",
      "Epoch: 30 Training loss: 0.0 Validation loss: 0.13520435988903046\n",
      "Epoch: 31 Training loss: 0.0 Validation loss: 0.13243739306926727\n",
      "Epoch: 32 Training loss: 0.0 Validation loss: 0.13266126811504364\n",
      "Epoch: 33 Training loss: 0.0 Validation loss: 0.14366163313388824\n",
      "Epoch: 34 Training loss: 0.0 Validation loss: 0.13070768117904663\n",
      "Epoch: 35 Training loss: 0.0 Validation loss: 0.12922251224517822\n",
      "Epoch: 36 Training loss: 0.0 Validation loss: 0.12947480380535126\n",
      "Epoch: 37 Training loss: 0.0 Validation loss: 0.12798729538917542\n",
      "Epoch: 38 Training loss: 0.0 Validation loss: 0.12740632891654968\n",
      "Epoch: 39 Training loss: 0.0 Validation loss: 0.1291741132736206\n",
      "Epoch: 40 Training loss: 0.0 Validation loss: 0.12723422050476074\n",
      "Epoch: 41 Training loss: 0.0 Validation loss: 0.1245817318558693\n",
      "Epoch: 42 Training loss: 0.0 Validation loss: 0.1250857561826706\n",
      "Epoch: 43 Training loss: 0.0 Validation loss: 0.12378828227519989\n",
      "Epoch: 44 Training loss: 0.0 Validation loss: 0.1264229416847229\n",
      "Epoch: 45 Training loss: 0.0 Validation loss: 0.1265655905008316\n",
      "Epoch: 46 Training loss: 0.0 Validation loss: 0.12626448273658752\n",
      "Epoch: 47 Training loss: 0.0 Validation loss: 0.1266937255859375\n",
      "Epoch: 48 Training loss: 0.0 Validation loss: 0.12459956109523773\n",
      "Epoch: 49 Training loss: 0.0 Validation loss: 0.12467096745967865\n",
      "Epoch: 50 Training loss: 0.0 Validation loss: 0.12399916350841522\n",
      "Epoch: 51 Training loss: 0.0 Validation loss: 0.12177660316228867\n",
      "Epoch: 52 Training loss: 0.0 Validation loss: 0.12319105118513107\n",
      "Epoch: 53 Training loss: 0.0 Validation loss: 0.1202782690525055\n",
      "Epoch: 54 Training loss: 0.0 Validation loss: 0.12105561792850494\n",
      "Epoch: 55 Training loss: 0.0 Validation loss: 0.11940128356218338\n",
      "Epoch: 56 Training loss: 0.0 Validation loss: 0.11969199031591415\n",
      "Epoch: 57 Training loss: 0.0 Validation loss: 0.12131135165691376\n",
      "Epoch: 58 Training loss: 0.0 Validation loss: 0.11977081745862961\n",
      "Epoch: 59 Training loss: 0.0 Validation loss: 0.11941470950841904\n",
      "Epoch: 60 Training loss: 0.0 Validation loss: 0.11948595941066742\n",
      "Epoch: 61 Training loss: 0.0 Validation loss: 0.12147808074951172\n",
      "Epoch: 62 Training loss: 0.0 Validation loss: 0.1194683089852333\n",
      "Epoch: 63 Training loss: 0.0 Validation loss: 0.11970622837543488\n",
      "Epoch: 64 Training loss: 0.0 Validation loss: 0.11917232722043991\n",
      "Epoch: 65 Training loss: 0.0 Validation loss: 0.12093936651945114\n",
      "Epoch: 66 Training loss: 0.0 Validation loss: 0.12080823630094528\n",
      "Epoch: 67 Training loss: 0.0 Validation loss: 0.1198546439409256\n",
      "Epoch: 68 Training loss: 0.0 Validation loss: 0.11965937912464142\n",
      "Epoch: 69 Training loss: 0.0 Validation loss: 0.11970673501491547\n",
      "Epoch: 70 Training loss: 0.0 Validation loss: 0.11931583285331726\n",
      "Epoch: 71 Training loss: 0.0 Validation loss: 0.11960173398256302\n",
      "Epoch: 72 Training loss: 0.0 Validation loss: 0.11921503394842148\n",
      "Epoch: 73 Training loss: 0.0 Validation loss: 0.11877475678920746\n",
      "Epoch: 74 Training loss: 0.0 Validation loss: 0.11916469782590866\n",
      "Epoch: 75 Training loss: 0.0 Validation loss: 0.11903391778469086\n",
      "Epoch: 76 Training loss: 0.0 Validation loss: 0.11875279992818832\n",
      "Epoch: 77 Training loss: 0.0 Validation loss: 0.11778061836957932\n",
      "Epoch: 78 Training loss: 0.0 Validation loss: 0.11818422377109528\n",
      "Epoch: 79 Training loss: 0.0 Validation loss: 0.11681979894638062\n",
      "Epoch: 80 Training loss: 0.0 Validation loss: 0.12328136712312698\n",
      "Epoch: 81 Training loss: 0.0 Validation loss: 0.11646093428134918\n",
      "Epoch: 82 Training loss: 0.0 Validation loss: 0.11686933785676956\n",
      "Epoch: 83 Training loss: 0.0 Validation loss: 0.1166316568851471\n",
      "Epoch: 84 Training loss: 0.0 Validation loss: 0.11755268275737762\n",
      "Epoch: 85 Training loss: 0.0 Validation loss: 0.11734959483146667\n",
      "Epoch: 86 Training loss: 0.0 Validation loss: 0.11760292947292328\n",
      "Epoch: 87 Training loss: 0.0 Validation loss: 0.11726238578557968\n",
      "Epoch: 88 Training loss: 0.0 Validation loss: 0.11698191612958908\n",
      "Epoch: 89 Training loss: 0.0 Validation loss: 0.11699104309082031\n",
      "Epoch: 90 Training loss: 0.0 Validation loss: 0.11640875786542892\n",
      "Epoch: 91 Training loss: 0.0 Validation loss: 0.11702919751405716\n",
      "Epoch: 92 Training loss: 0.0 Validation loss: 0.1171942874789238\n",
      "Epoch: 93 Training loss: 0.0 Validation loss: 0.11718667298555374\n",
      "Epoch: 94 Training loss: 0.0 Validation loss: 0.11676277220249176\n",
      "Epoch: 95 Training loss: 0.0 Validation loss: 0.1166098490357399\n",
      "Epoch: 96 Training loss: 0.0 Validation loss: 0.11677955836057663\n",
      "Epoch: 97 Training loss: 0.0 Validation loss: 0.11616947501897812\n",
      "Epoch: 98 Training loss: 0.0 Validation loss: 0.11605720967054367\n",
      "Epoch: 99 Training loss: 0.0 Validation loss: 0.1155485138297081\n"
     ]
    }
   ],
   "source": [
    "# LBFGS, MSE\n",
    "trainLBFGS(model_LBFGS_MSE, optimizer_LBFGS_MSE, criterion2, tau,  N_EPOCHS, lossList_LBFGS_MSE, valList_LBFGS_MSE, \"MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training loss: 0.9093198180198669 Validation loss: 0.5454108119010925\n",
      "Epoch: 1 Training loss: 0.4440552890300751 Validation loss: 0.43683093786239624\n",
      "Epoch: 2 Training loss: 0.3357156813144684 Validation loss: 0.40775665640830994\n",
      "Epoch: 3 Training loss: 0.3305158317089081 Validation loss: 0.4206060767173767\n",
      "Epoch: 4 Training loss: 0.33660072088241577 Validation loss: 0.4167877435684204\n",
      "Epoch: 5 Training loss: 0.3506762385368347 Validation loss: 0.431005597114563\n",
      "Epoch: 6 Training loss: 0.34609922766685486 Validation loss: 0.4131128191947937\n",
      "Epoch: 7 Training loss: 0.3478061854839325 Validation loss: 0.40818554162979126\n",
      "Epoch: 8 Training loss: 0.3271120488643646 Validation loss: 0.3968852460384369\n",
      "Epoch: 9 Training loss: 0.3321595788002014 Validation loss: 0.4038558602333069\n",
      "Epoch: 10 Training loss: 0.3260256052017212 Validation loss: 0.38983747363090515\n",
      "Epoch: 11 Training loss: 0.3211667537689209 Validation loss: 0.3879890441894531\n",
      "Epoch: 12 Training loss: 0.3061442971229553 Validation loss: 0.37560179829597473\n",
      "Epoch: 13 Training loss: 0.31229832768440247 Validation loss: 0.3800894021987915\n",
      "Epoch: 14 Training loss: 0.30102309584617615 Validation loss: 0.3638160526752472\n",
      "Epoch: 15 Training loss: 0.29813581705093384 Validation loss: 0.3816251754760742\n",
      "Epoch: 16 Training loss: 0.3023682236671448 Validation loss: 0.37007254362106323\n",
      "Epoch: 17 Training loss: 0.3019162118434906 Validation loss: 0.3666541278362274\n",
      "Epoch: 18 Training loss: 0.2856408357620239 Validation loss: 0.3622105121612549\n",
      "Epoch: 19 Training loss: 0.2969232499599457 Validation loss: 0.3632006049156189\n",
      "Epoch: 20 Training loss: 0.28378939628601074 Validation loss: 0.3598417341709137\n",
      "Epoch: 21 Training loss: 0.2901631295681 Validation loss: 0.3749333918094635\n",
      "Epoch: 22 Training loss: 0.29450133442878723 Validation loss: 0.36029937863349915\n",
      "Epoch: 23 Training loss: 0.29269644618034363 Validation loss: 0.3595575988292694\n",
      "Epoch: 24 Training loss: 0.27633291482925415 Validation loss: 0.35114437341690063\n",
      "Epoch: 25 Training loss: 0.28715962171554565 Validation loss: 0.3577462136745453\n",
      "Epoch: 26 Training loss: 0.27676326036453247 Validation loss: 0.35101428627967834\n",
      "Epoch: 27 Training loss: 0.2839753329753876 Validation loss: 0.36511728167533875\n",
      "Epoch: 28 Training loss: 0.280474990606308 Validation loss: 0.36161768436431885\n",
      "Epoch: 29 Training loss: 0.29263851046562195 Validation loss: 0.3688431978225708\n",
      "Epoch: 30 Training loss: 0.2850119471549988 Validation loss: 0.36445578932762146\n",
      "Epoch: 31 Training loss: 0.29403337836265564 Validation loss: 0.35776078701019287\n",
      "Epoch: 32 Training loss: 0.27640312910079956 Validation loss: 0.3485162556171417\n",
      "Epoch: 33 Training loss: 0.2773616313934326 Validation loss: 0.3617282509803772\n",
      "Epoch: 34 Training loss: 0.27487918734550476 Validation loss: 0.3560751974582672\n",
      "Epoch: 35 Training loss: 0.28589412569999695 Validation loss: 0.35768958926200867\n",
      "Epoch: 36 Training loss: 0.27404099702835083 Validation loss: 0.3462586998939514\n",
      "Epoch: 37 Training loss: 0.2772093713283539 Validation loss: 0.3602610230445862\n",
      "Epoch: 38 Training loss: 0.2730940580368042 Validation loss: 0.3452569842338562\n",
      "Epoch: 39 Training loss: 0.2744825482368469 Validation loss: 0.3579949140548706\n",
      "Epoch: 40 Training loss: 0.2692377269268036 Validation loss: 0.34636396169662476\n",
      "Epoch: 41 Training loss: 0.2765965461730957 Validation loss: 0.35502585768699646\n",
      "Epoch: 42 Training loss: 0.2681952118873596 Validation loss: 0.34042179584503174\n",
      "Epoch: 43 Training loss: 0.2646104097366333 Validation loss: 0.35191836953163147\n",
      "Epoch: 44 Training loss: 0.26205939054489136 Validation loss: 0.34667354822158813\n",
      "Epoch: 45 Training loss: 0.27108705043792725 Validation loss: 0.3592844605445862\n",
      "Epoch: 46 Training loss: 0.2710382342338562 Validation loss: 0.35053351521492004\n",
      "Epoch: 47 Training loss: 0.280627965927124 Validation loss: 0.3586207330226898\n",
      "Epoch: 48 Training loss: 0.26799219846725464 Validation loss: 0.3508535921573639\n",
      "Epoch: 49 Training loss: 0.28341156244277954 Validation loss: 0.36278995871543884\n",
      "Epoch: 50 Training loss: 0.2727090120315552 Validation loss: 0.34960657358169556\n",
      "Epoch: 51 Training loss: 0.27737143635749817 Validation loss: 0.3625411093235016\n",
      "Epoch: 52 Training loss: 0.27022895216941833 Validation loss: 0.3525163233280182\n",
      "Epoch: 53 Training loss: 0.28418293595314026 Validation loss: 0.35221603512763977\n",
      "Epoch: 54 Training loss: 0.2597544491291046 Validation loss: 0.34069809317588806\n",
      "Epoch: 55 Training loss: 0.26926934719085693 Validation loss: 0.351998895406723\n",
      "Epoch: 56 Training loss: 0.26294174790382385 Validation loss: 0.33846625685691833\n",
      "Epoch: 57 Training loss: 0.26026198267936707 Validation loss: 0.352053701877594\n",
      "Epoch: 58 Training loss: 0.26044967770576477 Validation loss: 0.3389945328235626\n",
      "Epoch: 59 Training loss: 0.27010515332221985 Validation loss: 0.3546993136405945\n",
      "Epoch: 60 Training loss: 0.26208367943763733 Validation loss: 0.3409159183502197\n",
      "Epoch: 61 Training loss: 0.27139097452163696 Validation loss: 0.35747718811035156\n",
      "Epoch: 62 Training loss: 0.26130786538124084 Validation loss: 0.3420637845993042\n",
      "Epoch: 63 Training loss: 0.26874640583992004 Validation loss: 0.3575581908226013\n",
      "Epoch: 64 Training loss: 0.2629483640193939 Validation loss: 0.3402586579322815\n",
      "Epoch: 65 Training loss: 0.2629022002220154 Validation loss: 0.3578903079032898\n",
      "Epoch: 66 Training loss: 0.2646403908729553 Validation loss: 0.34307706356048584\n",
      "Epoch: 67 Training loss: 0.2759963572025299 Validation loss: 0.35821348428726196\n",
      "Epoch: 68 Training loss: 0.26339876651763916 Validation loss: 0.34147900342941284\n",
      "Epoch: 69 Training loss: 0.2693939208984375 Validation loss: 0.36106258630752563\n",
      "Epoch: 70 Training loss: 0.2647763788700104 Validation loss: 0.3468969464302063\n",
      "Epoch: 71 Training loss: 0.27608466148376465 Validation loss: 0.35651880502700806\n",
      "Epoch: 72 Training loss: 0.26214292645454407 Validation loss: 0.34237736463546753\n",
      "Epoch: 73 Training loss: 0.2673414945602417 Validation loss: 0.35892805457115173\n",
      "Epoch: 74 Training loss: 0.2634623348712921 Validation loss: 0.34785908460617065\n",
      "Epoch: 75 Training loss: 0.27367979288101196 Validation loss: 0.36255571246147156\n",
      "Epoch: 76 Training loss: 0.2665084898471832 Validation loss: 0.3477739989757538\n",
      "Epoch: 77 Training loss: 0.2730688452720642 Validation loss: 0.3610360324382782\n",
      "Epoch: 78 Training loss: 0.2671049237251282 Validation loss: 0.3490150570869446\n",
      "Epoch: 79 Training loss: 0.2794114947319031 Validation loss: 0.357418954372406\n",
      "Epoch: 80 Training loss: 0.26733314990997314 Validation loss: 0.3474487364292145\n",
      "Epoch: 81 Training loss: 0.2741287648677826 Validation loss: 0.3608924448490143\n",
      "Epoch: 82 Training loss: 0.2593272030353546 Validation loss: 0.3419513404369354\n",
      "Epoch: 83 Training loss: 0.26896360516548157 Validation loss: 0.35870760679244995\n",
      "Epoch: 84 Training loss: 0.26577556133270264 Validation loss: 0.33399853110313416\n",
      "Epoch: 85 Training loss: 0.2587535083293915 Validation loss: 0.35206371545791626\n",
      "Epoch: 86 Training loss: 0.25119656324386597 Validation loss: 0.3352168798446655\n",
      "Epoch: 87 Training loss: 0.254808634519577 Validation loss: 0.3592016100883484\n",
      "Epoch: 88 Training loss: 0.2585066854953766 Validation loss: 0.3386583626270294\n",
      "Epoch: 89 Training loss: 0.2568550407886505 Validation loss: 0.3569934666156769\n",
      "Epoch: 90 Training loss: 0.25706177949905396 Validation loss: 0.33493784070014954\n",
      "Epoch: 91 Training loss: 0.2544514536857605 Validation loss: 0.3549228310585022\n",
      "Epoch: 92 Training loss: 0.25552669167518616 Validation loss: 0.3408849835395813\n",
      "Epoch: 93 Training loss: 0.2656165063381195 Validation loss: 0.3637740910053253\n",
      "Epoch: 94 Training loss: 0.26103270053863525 Validation loss: 0.3458128571510315\n",
      "Epoch: 95 Training loss: 0.2736084461212158 Validation loss: 0.3573805093765259\n",
      "Epoch: 96 Training loss: 0.25652453303337097 Validation loss: 0.343496173620224\n",
      "Epoch: 97 Training loss: 0.26630908250808716 Validation loss: 0.3565642535686493\n",
      "Epoch: 98 Training loss: 0.25093188881874084 Validation loss: 0.34060388803482056\n",
      "Epoch: 99 Training loss: 0.2653079032897949 Validation loss: 0.3583284914493561\n"
     ]
    }
   ],
   "source": [
    "# CLR, L1:\n",
    "trainConstantLR(model_CLR_L1, optimizer_CLR_L1, criterion3, tau,  N_EPOCHS, lossList_CLR_L1, valList_CLR_L1, \"L1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training loss: 1.0420897006988525 Validation loss: 0.6447374224662781\n",
      "Epoch: 1 Training loss: 0.4142497479915619 Validation loss: 0.5014262795448303\n",
      "Epoch: 2 Training loss: 0.2974858283996582 Validation loss: 0.3997075855731964\n",
      "Epoch: 3 Training loss: 0.1614500731229782 Validation loss: 0.3470926880836487\n",
      "Epoch: 4 Training loss: 0.1439753919839859 Validation loss: 0.3529699742794037\n",
      "Epoch: 5 Training loss: 0.12938371300697327 Validation loss: 0.3355365991592407\n",
      "Epoch: 6 Training loss: 0.12923920154571533 Validation loss: 0.34225794672966003\n",
      "Epoch: 7 Training loss: 0.12948743999004364 Validation loss: 0.3331418037414551\n",
      "Epoch: 8 Training loss: 0.12699486315250397 Validation loss: 0.3381032347679138\n",
      "Epoch: 9 Training loss: 0.12308933585882187 Validation loss: 0.33202800154685974\n",
      "Epoch: 10 Training loss: 0.11968373507261276 Validation loss: 0.3339344263076782\n",
      "Epoch: 11 Training loss: 0.12034507095813751 Validation loss: 0.33242306113243103\n",
      "Epoch: 12 Training loss: 0.11935826390981674 Validation loss: 0.33178630471229553\n",
      "Epoch: 13 Training loss: 0.11991014331579208 Validation loss: 0.3318600356578827\n",
      "Epoch: 14 Training loss: 0.11855162680149078 Validation loss: 0.33020251989364624\n",
      "Epoch: 15 Training loss: 0.11620602011680603 Validation loss: 0.3301036059856415\n",
      "Epoch: 16 Training loss: 0.11533714085817337 Validation loss: 0.32912394404411316\n",
      "Epoch: 17 Training loss: 0.11401581019163132 Validation loss: 0.32963016629219055\n",
      "Epoch: 18 Training loss: 0.1130351722240448 Validation loss: 0.3291100561618805\n",
      "Epoch: 19 Training loss: 0.11059479415416718 Validation loss: 0.3276144564151764\n",
      "Epoch: 20 Training loss: 0.11203273385763168 Validation loss: 0.3310241997241974\n",
      "Epoch: 21 Training loss: 0.11511684954166412 Validation loss: 0.3273770809173584\n",
      "Epoch: 22 Training loss: 0.11496327072381973 Validation loss: 0.32830581068992615\n",
      "Epoch: 23 Training loss: 0.11158321797847748 Validation loss: 0.3268625736236572\n",
      "Epoch: 24 Training loss: 0.1106611043214798 Validation loss: 0.32658764719963074\n",
      "Epoch: 25 Training loss: 0.10920416563749313 Validation loss: 0.32731547951698303\n",
      "Epoch: 26 Training loss: 0.10980170220136642 Validation loss: 0.3274352550506592\n",
      "Epoch: 27 Training loss: 0.1090371310710907 Validation loss: 0.32801881432533264\n",
      "Epoch: 28 Training loss: 0.11088930070400238 Validation loss: 0.32540982961654663\n",
      "Epoch: 29 Training loss: 0.10711656510829926 Validation loss: 0.32647398114204407\n",
      "Epoch: 30 Training loss: 0.10873927175998688 Validation loss: 0.32670480012893677\n",
      "Epoch: 31 Training loss: 0.10660292208194733 Validation loss: 0.3256887197494507\n",
      "Epoch: 32 Training loss: 0.10834445059299469 Validation loss: 0.3269205093383789\n",
      "Epoch: 33 Training loss: 0.11050111055374146 Validation loss: 0.32574930787086487\n",
      "Epoch: 34 Training loss: 0.10580962896347046 Validation loss: 0.3269864618778229\n",
      "Epoch: 35 Training loss: 0.1045873835682869 Validation loss: 0.3257509768009186\n",
      "Epoch: 36 Training loss: 0.10751640796661377 Validation loss: 0.32602423429489136\n",
      "Epoch: 37 Training loss: 0.10611093789339066 Validation loss: 0.32574042677879333\n",
      "Epoch: 38 Training loss: 0.10767355561256409 Validation loss: 0.3263683617115021\n",
      "Epoch: 39 Training loss: 0.10492690652608871 Validation loss: 0.32664573192596436\n",
      "Epoch: 40 Training loss: 0.10457777231931686 Validation loss: 0.3265621066093445\n",
      "Epoch: 41 Training loss: 0.10594402998685837 Validation loss: 0.3246549367904663\n",
      "Epoch: 42 Training loss: 0.1026235818862915 Validation loss: 0.32543274760246277\n",
      "Epoch: 43 Training loss: 0.10376128554344177 Validation loss: 0.3259883522987366\n",
      "Epoch: 44 Training loss: 0.1038704514503479 Validation loss: 0.32428818941116333\n",
      "Epoch: 45 Training loss: 0.10805289447307587 Validation loss: 0.32549571990966797\n",
      "Epoch: 46 Training loss: 0.1018318235874176 Validation loss: 0.32548803091049194\n",
      "Epoch: 47 Training loss: 0.10629774630069733 Validation loss: 0.325104683637619\n",
      "Epoch: 48 Training loss: 0.1032819002866745 Validation loss: 0.3247002065181732\n",
      "Epoch: 49 Training loss: 0.10166862607002258 Validation loss: 0.32578346133232117\n",
      "Epoch: 50 Training loss: 0.10454332828521729 Validation loss: 0.3253675103187561\n",
      "Epoch: 51 Training loss: 0.10216653347015381 Validation loss: 0.32580095529556274\n",
      "Epoch: 52 Training loss: 0.10173149406909943 Validation loss: 0.3259929120540619\n",
      "Epoch: 53 Training loss: 0.10205390304327011 Validation loss: 0.32405394315719604\n",
      "Epoch: 54 Training loss: 0.10202167183160782 Validation loss: 0.32504957914352417\n",
      "Epoch: 55 Training loss: 0.1009521484375 Validation loss: 0.3264443874359131\n",
      "Epoch: 56 Training loss: 0.0984092503786087 Validation loss: 0.32400497794151306\n",
      "Epoch: 57 Training loss: 0.10448097437620163 Validation loss: 0.32739564776420593\n",
      "Epoch: 58 Training loss: 0.10240001976490021 Validation loss: 0.3233720660209656\n",
      "Epoch: 59 Training loss: 0.1009671688079834 Validation loss: 0.32880035042762756\n",
      "Epoch: 60 Training loss: 0.10011594742536545 Validation loss: 0.3247199058532715\n",
      "Epoch: 61 Training loss: 0.09968205541372299 Validation loss: 0.3278709650039673\n",
      "Epoch: 62 Training loss: 0.10225043445825577 Validation loss: 0.32608309388160706\n",
      "Epoch: 63 Training loss: 0.09997625648975372 Validation loss: 0.3275626599788666\n",
      "Epoch: 64 Training loss: 0.1043255627155304 Validation loss: 0.3258371949195862\n",
      "Epoch: 65 Training loss: 0.10024581104516983 Validation loss: 0.3256675899028778\n",
      "Epoch: 66 Training loss: 0.0993456020951271 Validation loss: 0.32593125104904175\n",
      "Epoch: 67 Training loss: 0.10262886434793472 Validation loss: 0.32545584440231323\n",
      "Epoch: 68 Training loss: 0.10318587720394135 Validation loss: 0.32466885447502136\n",
      "Epoch: 69 Training loss: 0.09990578144788742 Validation loss: 0.32548484206199646\n",
      "Epoch: 70 Training loss: 0.09965966641902924 Validation loss: 0.3253108263015747\n",
      "Epoch: 71 Training loss: 0.10263892263174057 Validation loss: 0.32568657398223877\n",
      "Epoch: 72 Training loss: 0.100262850522995 Validation loss: 0.3256407380104065\n",
      "Epoch: 73 Training loss: 0.0987074002623558 Validation loss: 0.32640504837036133\n",
      "Epoch: 74 Training loss: 0.09981764853000641 Validation loss: 0.32651078701019287\n",
      "Epoch: 75 Training loss: 0.09881874173879623 Validation loss: 0.3265698552131653\n",
      "Epoch: 76 Training loss: 0.10095781832933426 Validation loss: 0.32622095942497253\n",
      "Epoch: 77 Training loss: 0.09989093989133835 Validation loss: 0.32649049162864685\n",
      "Epoch: 78 Training loss: 0.09948898106813431 Validation loss: 0.3277221918106079\n",
      "Epoch: 79 Training loss: 0.09732658416032791 Validation loss: 0.32552897930145264\n",
      "Epoch: 80 Training loss: 0.09899111837148666 Validation loss: 0.32568037509918213\n",
      "Epoch: 81 Training loss: 0.10079813003540039 Validation loss: 0.3256710469722748\n",
      "Epoch: 82 Training loss: 0.09746775031089783 Validation loss: 0.3256193697452545\n",
      "Epoch: 83 Training loss: 0.09649846702814102 Validation loss: 0.32533907890319824\n",
      "Epoch: 84 Training loss: 0.09976734966039658 Validation loss: 0.3259737193584442\n",
      "Epoch: 85 Training loss: 0.09990489482879639 Validation loss: 0.3263307511806488\n",
      "Epoch: 86 Training loss: 0.09676697850227356 Validation loss: 0.32562658190727234\n",
      "Epoch: 87 Training loss: 0.09848868101835251 Validation loss: 0.3271639347076416\n",
      "Epoch: 88 Training loss: 0.10196571797132492 Validation loss: 0.32451510429382324\n",
      "Epoch: 89 Training loss: 0.09778472036123276 Validation loss: 0.32852572202682495\n",
      "Epoch: 90 Training loss: 0.10095693916082382 Validation loss: 0.3242481052875519\n",
      "Epoch: 91 Training loss: 0.09870538115501404 Validation loss: 0.3282015025615692\n",
      "Epoch: 92 Training loss: 0.09802895039319992 Validation loss: 0.3245047926902771\n",
      "Epoch: 93 Training loss: 0.09543952345848083 Validation loss: 0.32910019159317017\n",
      "Epoch: 94 Training loss: 0.09657885879278183 Validation loss: 0.3246401846408844\n",
      "Epoch: 95 Training loss: 0.09660431742668152 Validation loss: 0.3284911811351776\n",
      "Epoch: 96 Training loss: 0.09753848612308502 Validation loss: 0.3240957260131836\n",
      "Epoch: 97 Training loss: 0.09872888028621674 Validation loss: 0.3262408375740051\n",
      "Epoch: 98 Training loss: 0.098225899040699 Validation loss: 0.32391273975372314\n",
      "Epoch: 99 Training loss: 0.09752355515956879 Validation loss: 0.32807406783103943\n"
     ]
    }
   ],
   "source": [
    "# CLR, MSE\n",
    "trainConstantLR(model_CLR_MSE, optimizer_CLR_MSE, criterion2, tau,  N_EPOCHS, lossList_CLR_MSE, valList_CLR_MSE, \"MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training loss: 0.19018615782260895 Validation loss: 0.7493488788604736\n",
      "Epoch: 1 Training loss: 0.12839804589748383 Validation loss: 0.6156922578811646\n",
      "Epoch: 2 Training loss: 0.09027354419231415 Validation loss: 0.5286970138549805\n",
      "Epoch: 3 Training loss: 0.0692489817738533 Validation loss: 0.4743185341358185\n",
      "Epoch: 4 Training loss: 0.056910667568445206 Validation loss: 0.4396529495716095\n",
      "Epoch: 5 Training loss: 0.04894758760929108 Validation loss: 0.4157683849334717\n",
      "Epoch: 6 Training loss: 0.04393447935581207 Validation loss: 0.3987039029598236\n",
      "Epoch: 7 Training loss: 0.04140431061387062 Validation loss: 0.3854612410068512\n",
      "Epoch: 8 Training loss: 0.03732926398515701 Validation loss: 0.37547749280929565\n",
      "Epoch: 9 Training loss: 0.03561697527766228 Validation loss: 0.3676394820213318\n",
      "Epoch: 10 Training loss: 0.03312951326370239 Validation loss: 0.36142393946647644\n",
      "Epoch: 11 Training loss: 0.03224359080195427 Validation loss: 0.3566060960292816\n",
      "Epoch: 12 Training loss: 0.03157735615968704 Validation loss: 0.3530149459838867\n",
      "Epoch: 13 Training loss: 0.03080679103732109 Validation loss: 0.3503906726837158\n",
      "Epoch: 14 Training loss: 0.03048526495695114 Validation loss: 0.34814244508743286\n",
      "Epoch: 15 Training loss: 0.030523527413606644 Validation loss: 0.34635958075523376\n",
      "Epoch: 16 Training loss: 0.030086569488048553 Validation loss: 0.3447510004043579\n",
      "Epoch: 17 Training loss: 0.02857343479990959 Validation loss: 0.3436550498008728\n",
      "Epoch: 18 Training loss: 0.0299379825592041 Validation loss: 0.3426457643508911\n",
      "Epoch: 19 Training loss: 0.02905050292611122 Validation loss: 0.3420475423336029\n",
      "Epoch: 20 Training loss: 0.0288966316729784 Validation loss: 0.34138941764831543\n",
      "Epoch: 21 Training loss: 0.028952591121196747 Validation loss: 0.34085091948509216\n",
      "Epoch: 22 Training loss: 0.028650255873799324 Validation loss: 0.34038665890693665\n",
      "Epoch: 23 Training loss: 0.02850111573934555 Validation loss: 0.3404349088668823\n",
      "Epoch: 24 Training loss: 0.028376659378409386 Validation loss: 0.3398726284503937\n",
      "Epoch: 25 Training loss: 0.028507743030786514 Validation loss: 0.3395251929759979\n",
      "Epoch: 26 Training loss: 0.028366876766085625 Validation loss: 0.33936381340026855\n",
      "Epoch: 27 Training loss: 0.027912652119994164 Validation loss: 0.33930090069770813\n",
      "Epoch: 28 Training loss: 0.028089331462979317 Validation loss: 0.339171826839447\n",
      "Epoch: 29 Training loss: 0.02875952422618866 Validation loss: 0.3389701843261719\n",
      "Epoch: 30 Training loss: 0.02804454416036606 Validation loss: 0.3388044536113739\n",
      "Epoch: 31 Training loss: 0.027424558997154236 Validation loss: 0.3387492597103119\n",
      "Epoch: 32 Training loss: 0.027565497905015945 Validation loss: 0.33864057064056396\n",
      "Epoch: 33 Training loss: 0.02811998501420021 Validation loss: 0.33812546730041504\n",
      "Epoch: 34 Training loss: 0.0285271517932415 Validation loss: 0.3382621109485626\n",
      "Epoch: 35 Training loss: 0.027821723371744156 Validation loss: 0.3377879858016968\n",
      "Epoch: 36 Training loss: 0.027463048696517944 Validation loss: 0.3375445008277893\n",
      "Epoch: 37 Training loss: 0.027910029515624046 Validation loss: 0.3373860716819763\n",
      "Epoch: 38 Training loss: 0.028121892362833023 Validation loss: 0.3373236358165741\n",
      "Epoch: 39 Training loss: 0.027780110016465187 Validation loss: 0.33712682127952576\n",
      "Epoch: 40 Training loss: 0.027158113196492195 Validation loss: 0.3370814621448517\n",
      "Epoch: 41 Training loss: 0.028468921780586243 Validation loss: 0.33681005239486694\n",
      "Epoch: 42 Training loss: 0.02660406567156315 Validation loss: 0.3367803692817688\n",
      "Epoch: 43 Training loss: 0.028284648433327675 Validation loss: 0.3364579379558563\n",
      "Epoch: 44 Training loss: 0.027171026915311813 Validation loss: 0.33644863963127136\n",
      "Epoch: 45 Training loss: 0.02801038883626461 Validation loss: 0.3365232050418854\n",
      "Epoch: 46 Training loss: 0.02711493894457817 Validation loss: 0.33634045720100403\n",
      "Epoch: 47 Training loss: 0.02788136713206768 Validation loss: 0.3363414704799652\n",
      "Epoch: 48 Training loss: 0.027205901220440865 Validation loss: 0.3364270329475403\n",
      "Epoch: 49 Training loss: 0.02669011428952217 Validation loss: 0.3362394869327545\n",
      "Epoch: 50 Training loss: 0.027113793417811394 Validation loss: 0.33587631583213806\n",
      "Epoch: 51 Training loss: 0.02693904936313629 Validation loss: 0.3358115255832672\n",
      "Epoch: 52 Training loss: 0.027342816814780235 Validation loss: 0.3355218768119812\n",
      "Epoch: 53 Training loss: 0.02725856378674507 Validation loss: 0.33517372608184814\n",
      "Epoch: 54 Training loss: 0.027444029226899147 Validation loss: 0.33522865176200867\n",
      "Epoch: 55 Training loss: 0.027108309790492058 Validation loss: 0.33522865176200867\n",
      "Epoch: 56 Training loss: 0.027135059237480164 Validation loss: 0.33496084809303284\n",
      "Epoch: 57 Training loss: 0.027412936091423035 Validation loss: 0.3347345292568207\n",
      "Epoch: 58 Training loss: 0.026835164055228233 Validation loss: 0.33467382192611694\n",
      "Epoch: 59 Training loss: 0.02700442634522915 Validation loss: 0.3347148001194\n",
      "Epoch: 60 Training loss: 0.027184443548321724 Validation loss: 0.33464759588241577\n",
      "Epoch: 61 Training loss: 0.027003629133105278 Validation loss: 0.3345212936401367\n",
      "Epoch: 62 Training loss: 0.02721272222697735 Validation loss: 0.33441659808158875\n",
      "Epoch: 63 Training loss: 0.027138765901327133 Validation loss: 0.33456963300704956\n",
      "Epoch: 64 Training loss: 0.026868147775530815 Validation loss: 0.33429595828056335\n",
      "Epoch: 65 Training loss: 0.026689438149333 Validation loss: 0.33443334698677063\n",
      "Epoch: 66 Training loss: 0.026683000847697258 Validation loss: 0.3344891667366028\n",
      "Epoch: 67 Training loss: 0.02704804576933384 Validation loss: 0.3340390920639038\n",
      "Epoch: 68 Training loss: 0.027245141565799713 Validation loss: 0.3334542214870453\n",
      "Epoch: 69 Training loss: 0.027065148577094078 Validation loss: 0.3335632383823395\n",
      "Epoch: 70 Training loss: 0.02662036381661892 Validation loss: 0.33336710929870605\n",
      "Epoch: 71 Training loss: 0.026704590767621994 Validation loss: 0.332981675863266\n",
      "Epoch: 72 Training loss: 0.026564935222268105 Validation loss: 0.3329375088214874\n",
      "Epoch: 73 Training loss: 0.026068974286317825 Validation loss: 0.3327382504940033\n",
      "Epoch: 74 Training loss: 0.026033684611320496 Validation loss: 0.33297038078308105\n",
      "Epoch: 75 Training loss: 0.026196518912911415 Validation loss: 0.33292198181152344\n",
      "Epoch: 76 Training loss: 0.026505183428525925 Validation loss: 0.3326427638530731\n",
      "Epoch: 77 Training loss: 0.026789497584104538 Validation loss: 0.332383930683136\n",
      "Epoch: 78 Training loss: 0.02628198266029358 Validation loss: 0.33291494846343994\n",
      "Epoch: 79 Training loss: 0.02657271735370159 Validation loss: 0.33250942826271057\n",
      "Epoch: 80 Training loss: 0.025975672528147697 Validation loss: 0.33262622356414795\n",
      "Epoch: 81 Training loss: 0.026335449889302254 Validation loss: 0.33262771368026733\n",
      "Epoch: 82 Training loss: 0.02604966051876545 Validation loss: 0.3324351906776428\n",
      "Epoch: 83 Training loss: 0.026124613359570503 Validation loss: 0.33221784234046936\n",
      "Epoch: 84 Training loss: 0.026351431384682655 Validation loss: 0.33243465423583984\n",
      "Epoch: 85 Training loss: 0.025792092084884644 Validation loss: 0.3321477174758911\n",
      "Epoch: 86 Training loss: 0.02610662207007408 Validation loss: 0.3324222266674042\n",
      "Epoch: 87 Training loss: 0.026251696050167084 Validation loss: 0.3322935402393341\n",
      "Epoch: 88 Training loss: 0.026527121663093567 Validation loss: 0.3319762349128723\n",
      "Epoch: 89 Training loss: 0.025740336626768112 Validation loss: 0.33181846141815186\n",
      "Epoch: 90 Training loss: 0.026060007512569427 Validation loss: 0.3316773474216461\n",
      "Epoch: 91 Training loss: 0.02574198506772518 Validation loss: 0.33174094557762146\n",
      "Epoch: 92 Training loss: 0.02573666349053383 Validation loss: 0.33131715655326843\n",
      "Epoch: 93 Training loss: 0.026098446920514107 Validation loss: 0.3314044773578644\n",
      "Epoch: 94 Training loss: 0.025637269020080566 Validation loss: 0.33132925629615784\n",
      "Epoch: 95 Training loss: 0.026052402332425117 Validation loss: 0.3310436010360718\n",
      "Epoch: 96 Training loss: 0.02529492788016796 Validation loss: 0.33116579055786133\n",
      "Epoch: 97 Training loss: 0.025661952793598175 Validation loss: 0.3311900496482849\n",
      "Epoch: 98 Training loss: 0.025599852204322815 Validation loss: 0.33094483613967896\n",
      "Epoch: 99 Training loss: 0.0259596798568964 Validation loss: 0.3309697210788727\n"
     ]
    }
   ],
   "source": [
    "# CLR, LC\n",
    "trainConstantLR(model_CLR_LC, optimizer_CLR_LC, criterion1, tau,  N_EPOCHS, lossList_CLR_LC, valList_CLR_LC, \"LC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f82589298e0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(100), valList_LBFGS_LC, label=\"Log-Cosh-LBFGS\")\n",
    "plt.plot(range(100), valList_LBFGS_MSE, label=\"MSE-LBFGS\")\n",
    "plt.plot(range(100), valList_CLR_MSE, label=\"CLR-MSE\")\n",
    "plt.plot(range(100), valList_CLR_L1, label=\"CLR-L1\")\n",
    "plt.plot(range(100), valList_CLR_LC, label=\"CLR-LC\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
