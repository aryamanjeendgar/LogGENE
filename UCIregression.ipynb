{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from adahessian import Adahessian, get_params_grad\n",
    "import torch.utils.data as data_utils\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from UCI_loader import UCIDatasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'UCI/CCPP/Folds5x2_pp.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_101229/2554036781.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataClass\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mUCIDatasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"power\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# has a field named 'data' that contains the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainData\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdataClass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrainLoader\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# creating a loader with full batch size to ensure LBFGS works\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrainLoaderLBFGS\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdataClass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# creating a loader with full batch size to ensure LBFGS works\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Code/tmpBQR/UCI_loader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, data_path, n_splits)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Code/tmpBQR/UCI_loader.py\u001b[0m in \u001b[0;36m_load_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"power\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\"UCI/CCPP.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\"UCI/CCPP/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'UCI/CCPP/Folds5x2_pp.xlsx'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1189\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1192\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m                 )\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1070\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1071\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m     ) as handle:\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'UCI/CCPP/Folds5x2_pp.xlsx'"
     ]
    }
   ],
   "source": [
    "dataClass= UCIDatasets(\"power\", data_path=\"\", n_splits=10) # has a field named 'data' that contains the data\n",
    "\n",
    "trainData= dataClass.get_split(train=True)\n",
    "trainLoader= torch.utils.data.DataLoader(trainData, batch_size=64, shuffle= True) # creating a loader with full batch size to ensure LBFGS works\n",
    "trainLoaderLBFGS= torch.utils.data.DataLoader(trainData, batch_size= dataClass.data.shape[0], shuffle= True) # creating a loader with full batch size to ensure LBFGS works\n",
    "testData= dataClass.get_split(train= False)\n",
    "testLoader= torch.utils.data.DataLoader(testData, batch_size= 64, shuffle= True)\n",
    "testLoaderLBFGS= torch.utils.data.DataLoader(testData, batch_size= dataClass.data.shape[0], shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_154214/3605115037.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_X_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_X_unscaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcreate_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mditch_head\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mremove_head\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0mshap_x_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mshap_x_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_154214/3605115037.py\u001b[0m in \u001b[0;36mcreate_xy\u001b[0;34m(dataset, attribute_columns, target_column, delim, split_ratio, ditch_head)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_columns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_columns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_column\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mX_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mX_unscaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "dataset= \"./Datasets/Regression/california.csv\"\n",
    "x_cols = list(range(9))\n",
    "y_col = 9\n",
    "separator = \",\"\n",
    "remove_head = False\n",
    "\n",
    "\n",
    "# attribute_index =   # This controls which attribute is allowed to vary, 7,5\n",
    "attribute_name = \"BMI\" # Name of the attribute, used in the plots, max heart rate\n",
    "latent_name = \"Diabetes\" # Name of the function, used in the plots\n",
    "# The other attributes are replaced by the median value of the attribute\n",
    "Scaler= StandardScaler()\n",
    "batch_is= 64\n",
    "\n",
    "total_epochs = 20\n",
    "\n",
    "### \n",
    "def create_xy(dataset, attribute_columns, target_column, delim, split_ratio, ditch_head=True):\n",
    "    with open(dataset, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    if ditch_head:\n",
    "        lines = lines[1:]\n",
    "    X = []\n",
    "    Y = []\n",
    "    for line in lines:\n",
    "        while len(line) > 0 and line[-1] == \"\\n\":\n",
    "            line = line[:len(line)-1]\n",
    "        split_array = line.split(delim)\n",
    "        all_columns = []\n",
    "        for value in split_array:\n",
    "            if value !=\"\" and value !=\" \":\n",
    "                all_columns.append(value)\n",
    "        if len(all_columns)==0:\n",
    "            break\n",
    "        point = []\n",
    "        for i in attribute_columns:\n",
    "            point.append(float(all_columns[i]))\n",
    "        X.append(point)\n",
    "        Y.append(float(all_columns[target_column]))\n",
    "    X_arr = np.asarray(X)\n",
    "    X_unscaled = np.asarray(X)\n",
    "    Scaler.fit(X_arr)\n",
    "    X_arr = Scaler.transform(X_arr)\n",
    "    Y_arr = np.asarray(Y)\n",
    "    thresh = 0\n",
    "    # Y_arr_binary = np.where(Y_arr<=thresh,0,1)\n",
    "    # unique, counts = np.unique(Y_arr_binary, return_counts=True)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X_arr, Y_arr, test_size = split_ratio)\n",
    "    return x_train, x_test, y_train, y_test, Y_arr, X_arr, X_unscaled, X_arr.shape[0]\n",
    "\n",
    "###\n",
    "X_train,X_val,y_train,y_val, data_Y, data_X_scaled, data_X_unscaled, full= create_xy(dataset, x_cols, y_col, separator, 0.4, ditch_head= remove_head)\n",
    "shap_x_train = X_train.copy()\n",
    "shap_x_val = X_val.copy()\n",
    "X_train = torch.Tensor(X_train)\n",
    "y_train = torch.Tensor(y_train)\n",
    "# y_train= F.one_hot(y_train.to(torch.int64), num_classes=2)\n",
    "X_val = torch.Tensor(X_val)\n",
    "y_val = torch.Tensor(y_val)\n",
    "# y_val= F.one_hot(y_val.to(torch.int64), num_classes=2)\n",
    "train_dataset = data_utils.TensorDataset(X_train, y_train)\n",
    "test_dataset = data_utils.TensorDataset(X_val, y_val)\n",
    "trainLoader= data_utils.DataLoader(train_dataset, batch_size= batch_is, pin_memory=True,shuffle=True,num_workers = 1)\n",
    "trainLoaderLBFGS= data_utils.DataLoader(train_dataset, batch_size=full, pin_memory=True,shuffle=True,num_workers = 1)\n",
    "testLoader= data_utils.DataLoader(test_dataset,batch_size= batch_is,pin_memory=True,shuffle = False,num_workers = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_batch, labels_batch = next(iter(trainLoader))\n",
    "try:\n",
    "    inDim, outDim= data_batch.shape[1], labels_batch.shape[1]\n",
    "except IndexError:\n",
    "    inDim, outDim= data_batch.shape[1], 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# defining all the criterions to be used in the following experiments:\n",
    "def tiltedLC(x, y, tau, h):\n",
    "    e= y-x # errors\n",
    "    ind= (torch.sign(e)+1)/2 # the division in the log-cosh is only about the origin\n",
    "    quantFactor= (1-tau)*(1-ind) + tau*ind\n",
    "    loss= quantFactor*torch.log(torch.cosh(e))\n",
    "    loss= torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "class TiltedLC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TiltedLC, self).__init__()\n",
    "    def forward(self, x, y, tau, h):\n",
    "        return tiltedLC(x, y, tau, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# global initialisations:\n",
    "h= 0.4 # smoothing parameter for the log-cosh \n",
    "tau= 0.5\n",
    "device= ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion1= TiltedLC()\n",
    "criterion2= nn.MSELoss()\n",
    "criterion3= nn.L1Loss()\n",
    "N_EPOCHS= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LALRnetwork(nn.Module):\n",
    "    def __init__(self, size1, size2, inDim, outDim, drop):\n",
    "        super(LALRnetwork, self).__init__()\n",
    "        self.l1= nn.Linear(inDim, size1)\n",
    "        self.l2= nn.Dropout(p= drop)\n",
    "        self.l3= nn.Linear(size1, size2)\n",
    "        self.l4= nn.Dropout(p= drop)\n",
    "        self.l5= nn.Linear(size2, outDim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x= F.tanh(self.l1(x))\n",
    "        x= F.tanh(self.l3(self.l2(x)))\n",
    "        x= self.l5(self.l4(x))\n",
    "        return x\n",
    "    \n",
    "    def penU(self, x):\n",
    "        x= F.tanh(self.l2(self.l1(x)))\n",
    "        x= F.tanh(self.l4(self.l3(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "size1, size2= 300, 300\n",
    "model_LBFGS_LC= LALRnetwork(size1, size2, inDim, outDim, 0.1).to(device)\n",
    "optimizer_LBFGS_LC= optim.LBFGS(model_LBFGS_LC.parameters())\n",
    "lossList_LBFGS_LC= []\n",
    "valList_LBFGS_LC= []\n",
    "\n",
    "model_LBFGS_MSE= LALRnetwork(size1, size2, inDim, outDim, 0.1).to(device)\n",
    "optimizer_LBFGS_MSE= optim.LBFGS(model_LBFGS_MSE.parameters())\n",
    "lossList_LBFGS_MSE= []\n",
    "valList_LBFGS_MSE= []\n",
    "\n",
    "model_CLR_L1= LALRnetwork(size1, size2, inDim, outDim, 0.1).to(device)\n",
    "optimizer_CLR_L1= optim.Adam(model_CLR_L1.parameters(), lr= 0.1)\n",
    "lossList_CLR_L1= []\n",
    "valList_CLR_L1= []\n",
    "\n",
    "model_CLR_LC= LALRnetwork(size1, size2, inDim, outDim, 0.1).to(device)\n",
    "optimizer_CLR_LC= optim.Adam(model_CLR_LC.parameters(), lr= 0.1)\n",
    "lossList_CLR_LC= []\n",
    "valList_CLR_LC= []\n",
    "\n",
    "model_CLR_MSE= LALRnetwork(size1, size2, inDim, outDim, 0.1).to(device)\n",
    "optimizer_CLR_MSE= optim.Adam(model_CLR_MSE.parameters(), lr= 0.1)\n",
    "lossList_CLR_MSE= []\n",
    "valList_CLR_MSE= []\n",
    "\n",
    "model_LALR_L1= LALRnetwork(size1, size2, inDim, outDim, 0.1).to(device)\n",
    "optimizer_LALR_L1= optim.Adam(model_LALR_L1.parameters(), lr= 0.1)\n",
    "lossList_LALR_L1= []\n",
    "valList_LALR_L1= []\n",
    "\n",
    "model_LALR_LC= LALRnetwork(size1, size2, inDim, outDim, 0.1).to(device)\n",
    "optimizer_LALR_LC= optim.Adam(model_LALR_LC.parameters(), lr= 0.1)\n",
    "lossList_LALR_LC= []\n",
    "valList_LALR_LC= []\n",
    "\n",
    "model_LALR_MSE= LALRnetwork(size1, size2, inDim, outDim, 0.1).to(device)\n",
    "optimizer_LALR_MSE= optim.Adam(model_LALR_MSE.parameters(), lr= 0.1)\n",
    "lossList_LALR_MSE= []\n",
    "valList_LALR_MSE= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trainLBFGS(model, optimizer, criterion, tau, epochs, ls_list, valList, loss_name):\n",
    "    \"\"\"\n",
    "    Training loop used for LBFGS and conjugate gradient training\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss= 0.0\n",
    "        # training loop\n",
    "        model.train()\n",
    "        for inputs, labels in trainLoaderLBFGS: \n",
    "            inputs= inputs.to(device) \n",
    "            labels= labels.to(device)\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                outputs= model(inputs)\n",
    "                if loss_name== \"MSE\":\n",
    "                    loss= criterion(outputs, labels)\n",
    "                else:\n",
    "                    loss= criterion(outputs, labels, tau, h)\n",
    "                loss.backward()\n",
    "                return loss\n",
    "            optimizer.step(closure) \n",
    "        # ls_list.append(epoch_loss/len(trainLoader))\n",
    "\n",
    "        # validation loop\n",
    "        val_loss= 0.0\n",
    "        model.eval()\n",
    "        for inputs, labels in testLoader:\n",
    "            inputs= inputs.to(device)\n",
    "            labels= labels.to(device)\n",
    "            outputs= model(inputs)\n",
    "            loss= torch.sqrt(criterion2(outputs, labels))\n",
    "            val_loss+= loss.item()\n",
    "        valList.append(val_loss/len(testLoader))\n",
    "        print(\"Epoch: {} Training loss: {} Validation loss: {}\".format(epoch, epoch_loss/len(trainLoader), val_loss/len(testLoader)))\n",
    "        \n",
    "def trainConstantLR(model, optimizer, criterion, tau, epochs, ls_list, valList, loss_name):\n",
    "    \"\"\"\n",
    "    Training loop used for constantLR\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss= 0.0\n",
    "        # training loop\n",
    "        model.train()\n",
    "        for inputs, labels in trainLoader: \n",
    "            inputs= inputs.to(device) \n",
    "            labels= labels.to(device)\n",
    "            optimizer.zero_grad() \n",
    "            outputs= model(inputs) \n",
    "            if loss_name== \"LC\":\n",
    "                loss= criterion(outputs, labels, tau, h)\n",
    "            else:\n",
    "                loss= criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "            epoch_loss+= loss.item()\n",
    "        ls_list.append(epoch_loss/len(trainLoader))\n",
    "\n",
    "        # validation loop\n",
    "        val_loss= 0.0\n",
    "        model.eval()\n",
    "        for inputs, labels in testLoader:\n",
    "            inputs= inputs.to(device)\n",
    "            labels= labels.to(device)\n",
    "            outputs= model(inputs)\n",
    "            loss= torch.sqrt(criterion2(outputs, labels))\n",
    "            val_loss+= loss.item()\n",
    "        valList.append(val_loss/len(testLoader))\n",
    "        print(\"Epoch: {} Training loss: {} Validation loss: {}\".format(epoch, epoch_loss/len(trainLoader), val_loss/len(testLoader)))\n",
    "\n",
    "def trainLALR(model,optimizer, criterion,  tau, epochs, ls_list, valList, loss_name):\n",
    "    \"\"\"\n",
    "    Training loop used for LALR training\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss= 0.0\n",
    "        lr_val= computeLR(model, loss_name, bSize=16)\n",
    "        optimizer.param_groups[0]['lr']= lr_val\n",
    "        # training loop\n",
    "        model.train()\n",
    "        for inputs, labels in trainLoader:\n",
    "            inputs= inputs.to(device)\n",
    "            labels= labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs= model(inputs)\n",
    "            if loss_name == \"LC\":\n",
    "                loss= criterion(outputs, labels, tau, h)\n",
    "            else:\n",
    "                loss= criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss+= loss.item()\n",
    "        ls_list.append(epoch_loss/len(trainLoader))\n",
    "\n",
    "        # validation loop\n",
    "        val_loss= 0.0\n",
    "        model.eval()\n",
    "        for inputs, labels in testLoader:\n",
    "            inputs= inputs.to(device)\n",
    "            labels= labels.to(device)\n",
    "            outputs= model(inputs)\n",
    "            loss= torch.sqrt(criterion2(outputs, labels))\n",
    "            val_loss+= loss.item()\n",
    "        valList.append(val_loss/len(testLoader))\n",
    "        print(\"Epoch: {} Training Loss: {} Validation loss: {} LR: {}\".format(epoch, epoch_loss/len(trainLoader), val_loss/len(testLoader), optimizer.param_groups[0]['lr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Learning rate computation functions:\n",
    "def computeKa(x):\n",
    "    maxNorm= 0.0\n",
    "    for vector in x:\n",
    "        if (maxNorm < torch.linalg.vector_norm(vector)):\n",
    "            maxNorm= torch.linalg.vector_norm(vector)\n",
    "    return maxNorm\n",
    "\n",
    "def computeLR(model, ls, bSize= 16):\n",
    "    \"\"\"\n",
    "    Takes in a network of the LALRnetwork class(during some arbitrary EPOCH of training) and the current input, and returns Kz for the EPOCH\n",
    "    \"\"\"\n",
    "    Kz = 0.0\n",
    "    Ka= 0.0\n",
    "    Y= 0.0\n",
    "    z_k= 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i,j in enumerate(trainLoader):\n",
    "            inputs,labels= j[0],j[1]\n",
    "            inputs= inputs.to(device)\n",
    "            labels= labels.to(device)\n",
    "            op1= model.penU(inputs)\n",
    "            op2= model(inputs)\n",
    "            # first taking the max and min for each batch\n",
    "            val1= torch.linalg.norm(op1)\n",
    "            # activ1, arg1= torch.max(op1, dim= 1)\n",
    "            activ2, arg2= torch.min(op2, dim= 1)\n",
    "            # now, we take the max and min across batches\n",
    "            # val1, indx1= torch.max(activ1, dim= 0)\n",
    "            val2, indx2= torch.min(activ2, dim= 0)\n",
    "            val3= computeKa(op2)\n",
    "            val4= computeKa(labels)\n",
    "            # print(indx, i)\n",
    "            if val1 > Kz:\n",
    "                # in the case of K_z, we do not need the index where the max occurs, hence only deal with the value\n",
    "                Kz= val1 \n",
    "            z_k= val2\n",
    "            if val3 > Ka:\n",
    "                Ka= val3\n",
    "            if val3 > Y:\n",
    "                Y= val4 \n",
    "            argMin= arg2[indx2]\n",
    "\n",
    "    LR= 1\n",
    "    factor= 1\n",
    "    if ls == \"LC\":\n",
    "        # LR= (1/bSize)*torch.tanh(-op2[int(indx2)][int(argMin)])*Kz\n",
    "        LR= (1/bSize)*Kz*torch.tanh(val4)\n",
    "        factor= 0.1\n",
    "    elif ls == \"L1\":\n",
    "        LR= Kz/bSize\n",
    "    elif ls == \"MSE\":\n",
    "        LR= (1/bSize)*(Ka+Y)*Kz\n",
    "\n",
    "    if LR==0:\n",
    "        return 0.1\n",
    "    return (1/LR)*factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training loss: 0.0 Validation loss: 9.895125031471252\n",
      "Epoch: 1 Training loss: 0.0 Validation loss: 9.872456908226013\n",
      "Epoch: 2 Training loss: 0.0 Validation loss: 9.870267629623413\n",
      "Epoch: 3 Training loss: 0.0 Validation loss: 9.861825466156006\n",
      "Epoch: 4 Training loss: 0.0 Validation loss: 9.860758781433105\n",
      "Epoch: 5 Training loss: 0.0 Validation loss: 9.855895757675171\n",
      "Epoch: 6 Training loss: 0.0 Validation loss: 9.861916422843933\n",
      "Epoch: 7 Training loss: 0.0 Validation loss: 9.854797959327698\n",
      "Epoch: 8 Training loss: 0.0 Validation loss: 9.859606266021729\n",
      "Epoch: 9 Training loss: 0.0 Validation loss: 9.857123136520386\n",
      "Epoch: 10 Training loss: 0.0 Validation loss: 9.873890280723572\n",
      "Epoch: 11 Training loss: 0.0 Validation loss: 9.864315509796143\n",
      "Epoch: 12 Training loss: 0.0 Validation loss: 9.869473695755005\n",
      "Epoch: 13 Training loss: 0.0 Validation loss: 9.854023456573486\n",
      "Epoch: 14 Training loss: 0.0 Validation loss: 9.860597372055054\n",
      "Epoch: 15 Training loss: 0.0 Validation loss: 9.863126277923584\n",
      "Epoch: 16 Training loss: 0.0 Validation loss: 9.855231404304504\n",
      "Epoch: 17 Training loss: 0.0 Validation loss: 9.851680636405945\n",
      "Epoch: 18 Training loss: 0.0 Validation loss: 9.866995453834534\n",
      "Epoch: 19 Training loss: 0.0 Validation loss: 9.867739915847778\n",
      "Epoch: 20 Training loss: 0.0 Validation loss: 9.867049932479858\n",
      "Epoch: 21 Training loss: 0.0 Validation loss: 9.866807222366333\n",
      "Epoch: 22 Training loss: 0.0 Validation loss: 9.864142656326294\n",
      "Epoch: 23 Training loss: 0.0 Validation loss: 9.866203308105469\n",
      "Epoch: 24 Training loss: 0.0 Validation loss: 9.862122058868408\n",
      "Epoch: 25 Training loss: 0.0 Validation loss: 9.860100388526917\n",
      "Epoch: 26 Training loss: 0.0 Validation loss: 9.864813685417175\n",
      "Epoch: 27 Training loss: 0.0 Validation loss: 9.860542178153992\n",
      "Epoch: 28 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 29 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 30 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 31 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 32 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 33 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 34 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 35 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 36 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 37 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 38 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 39 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 40 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 41 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 42 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 43 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 44 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 45 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 46 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 47 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 48 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 49 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 50 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 51 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 52 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 53 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 54 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 55 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 56 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 57 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 58 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 59 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 60 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 61 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 62 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 63 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 64 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 65 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 66 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 67 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 68 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 69 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 70 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 71 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 72 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 73 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 74 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 75 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 76 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 77 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 78 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 79 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 80 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 81 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 82 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 83 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 84 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 85 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 86 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 87 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 88 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 89 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 90 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 91 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 92 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 93 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 94 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 95 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 96 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 97 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 98 Training loss: 0.0 Validation loss: nan\n",
      "Epoch: 99 Training loss: 0.0 Validation loss: nan\n"
     ]
    }
   ],
   "source": [
    "# LBFGS, LC\n",
    "trainLBFGS(model_LBFGS_LC, optimizer_LBFGS_LC, criterion1, tau,  N_EPOCHS, lossList_LBFGS_LC, valList_LBFGS_LC, \"LC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aryamanj/miniconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([303])) that is different to the input size (torch.Size([303, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training loss: 0.0 Validation loss: 9.771556854248047\n",
      "Epoch: 1 Training loss: 0.0 Validation loss: 9.753222107887268\n",
      "Epoch: 2 Training loss: 0.0 Validation loss: 9.713550209999084\n",
      "Epoch: 3 Training loss: 0.0 Validation loss: 9.706040620803833\n",
      "Epoch: 4 Training loss: 0.0 Validation loss: 9.725767135620117\n",
      "Epoch: 5 Training loss: 0.0 Validation loss: 9.719310760498047\n",
      "Epoch: 6 Training loss: 0.0 Validation loss: 9.724807024002075\n",
      "Epoch: 7 Training loss: 0.0 Validation loss: 9.70846140384674\n",
      "Epoch: 8 Training loss: 0.0 Validation loss: 9.709206938743591\n",
      "Epoch: 9 Training loss: 0.0 Validation loss: 9.714798331260681\n",
      "Epoch: 10 Training loss: 0.0 Validation loss: 9.717642068862915\n",
      "Epoch: 11 Training loss: 0.0 Validation loss: 9.718194127082825\n",
      "Epoch: 12 Training loss: 0.0 Validation loss: 9.716617703437805\n",
      "Epoch: 13 Training loss: 0.0 Validation loss: 9.714474201202393\n",
      "Epoch: 14 Training loss: 0.0 Validation loss: 9.71360445022583\n",
      "Epoch: 15 Training loss: 0.0 Validation loss: 9.713230013847351\n",
      "Epoch: 16 Training loss: 0.0 Validation loss: 9.714352369308472\n",
      "Epoch: 17 Training loss: 0.0 Validation loss: 9.712207198143005\n",
      "Epoch: 18 Training loss: 0.0 Validation loss: 9.702961683273315\n",
      "Epoch: 19 Training loss: 0.0 Validation loss: 9.714700818061829\n",
      "Epoch: 20 Training loss: 0.0 Validation loss: 9.718355298042297\n",
      "Epoch: 21 Training loss: 0.0 Validation loss: 9.713766932487488\n",
      "Epoch: 22 Training loss: 0.0 Validation loss: 9.717797875404358\n",
      "Epoch: 23 Training loss: 0.0 Validation loss: 9.713817954063416\n",
      "Epoch: 24 Training loss: 0.0 Validation loss: 9.738117456436157\n",
      "Epoch: 25 Training loss: 0.0 Validation loss: 9.714775204658508\n",
      "Epoch: 26 Training loss: 0.0 Validation loss: 9.71725845336914\n",
      "Epoch: 27 Training loss: 0.0 Validation loss: 9.715347051620483\n",
      "Epoch: 28 Training loss: 0.0 Validation loss: 9.715725779533386\n",
      "Epoch: 29 Training loss: 0.0 Validation loss: 9.71774411201477\n",
      "Epoch: 30 Training loss: 0.0 Validation loss: 9.717331051826477\n",
      "Epoch: 31 Training loss: 0.0 Validation loss: 9.715468287467957\n",
      "Epoch: 32 Training loss: 0.0 Validation loss: 9.714699864387512\n",
      "Epoch: 33 Training loss: 0.0 Validation loss: 9.715403437614441\n",
      "Epoch: 34 Training loss: 0.0 Validation loss: 9.715083956718445\n",
      "Epoch: 35 Training loss: 0.0 Validation loss: 9.715360522270203\n",
      "Epoch: 36 Training loss: 0.0 Validation loss: 9.715301036834717\n",
      "Epoch: 37 Training loss: 0.0 Validation loss: 9.715440034866333\n",
      "Epoch: 38 Training loss: 0.0 Validation loss: 9.715404868125916\n",
      "Epoch: 39 Training loss: 0.0 Validation loss: 9.715336561203003\n",
      "Epoch: 40 Training loss: 0.0 Validation loss: 9.715420603752136\n",
      "Epoch: 41 Training loss: 0.0 Validation loss: 9.715499758720398\n",
      "Epoch: 42 Training loss: 0.0 Validation loss: 9.715425491333008\n",
      "Epoch: 43 Training loss: 0.0 Validation loss: 9.715546607971191\n",
      "Epoch: 44 Training loss: 0.0 Validation loss: 9.715401530265808\n",
      "Epoch: 45 Training loss: 0.0 Validation loss: 9.715409278869629\n",
      "Epoch: 46 Training loss: 0.0 Validation loss: 9.715431094169617\n",
      "Epoch: 47 Training loss: 0.0 Validation loss: 9.715443849563599\n",
      "Epoch: 48 Training loss: 0.0 Validation loss: 9.715444922447205\n",
      "Epoch: 49 Training loss: 0.0 Validation loss: 9.715435147285461\n",
      "Epoch: 50 Training loss: 0.0 Validation loss: 9.715439438819885\n",
      "Epoch: 51 Training loss: 0.0 Validation loss: 9.715435266494751\n",
      "Epoch: 52 Training loss: 0.0 Validation loss: 9.715447545051575\n",
      "Epoch: 53 Training loss: 0.0 Validation loss: 9.715456008911133\n",
      "Epoch: 54 Training loss: 0.0 Validation loss: 9.71543264389038\n",
      "Epoch: 55 Training loss: 0.0 Validation loss: 9.715425252914429\n",
      "Epoch: 56 Training loss: 0.0 Validation loss: 9.715392708778381\n",
      "Epoch: 57 Training loss: 0.0 Validation loss: 9.715402960777283\n",
      "Epoch: 58 Training loss: 0.0 Validation loss: 9.715418219566345\n",
      "Epoch: 59 Training loss: 0.0 Validation loss: 9.715419888496399\n",
      "Epoch: 60 Training loss: 0.0 Validation loss: 9.71543562412262\n",
      "Epoch: 61 Training loss: 0.0 Validation loss: 9.715435028076172\n",
      "Epoch: 62 Training loss: 0.0 Validation loss: 9.715425729751587\n",
      "Epoch: 63 Training loss: 0.0 Validation loss: 9.715405106544495\n",
      "Epoch: 64 Training loss: 0.0 Validation loss: 9.715441346168518\n",
      "Epoch: 65 Training loss: 0.0 Validation loss: 9.715432167053223\n",
      "Epoch: 66 Training loss: 0.0 Validation loss: 9.715430617332458\n",
      "Epoch: 67 Training loss: 0.0 Validation loss: 9.715424537658691\n",
      "Epoch: 68 Training loss: 0.0 Validation loss: 9.715427160263062\n",
      "Epoch: 69 Training loss: 0.0 Validation loss: 9.71543538570404\n",
      "Epoch: 70 Training loss: 0.0 Validation loss: 9.71541976928711\n",
      "Epoch: 71 Training loss: 0.0 Validation loss: 9.71542203426361\n",
      "Epoch: 72 Training loss: 0.0 Validation loss: 9.715428709983826\n",
      "Epoch: 73 Training loss: 0.0 Validation loss: 9.715421795845032\n",
      "Epoch: 74 Training loss: 0.0 Validation loss: 9.715420961380005\n",
      "Epoch: 75 Training loss: 0.0 Validation loss: 9.715423345565796\n",
      "Epoch: 76 Training loss: 0.0 Validation loss: 9.7154221534729\n",
      "Epoch: 77 Training loss: 0.0 Validation loss: 9.715421915054321\n",
      "Epoch: 78 Training loss: 0.0 Validation loss: 9.71541965007782\n",
      "Epoch: 79 Training loss: 0.0 Validation loss: 9.715423703193665\n",
      "Epoch: 80 Training loss: 0.0 Validation loss: 9.715418338775635\n",
      "Epoch: 81 Training loss: 0.0 Validation loss: 9.715418934822083\n",
      "Epoch: 82 Training loss: 0.0 Validation loss: 9.715420484542847\n",
      "Epoch: 83 Training loss: 0.0 Validation loss: 9.715420007705688\n",
      "Epoch: 84 Training loss: 0.0 Validation loss: 9.715420722961426\n",
      "Epoch: 85 Training loss: 0.0 Validation loss: 9.715419888496399\n",
      "Epoch: 86 Training loss: 0.0 Validation loss: 9.71541690826416\n",
      "Epoch: 87 Training loss: 0.0 Validation loss: 9.715418696403503\n",
      "Epoch: 88 Training loss: 0.0 Validation loss: 9.715426087379456\n",
      "Epoch: 89 Training loss: 0.0 Validation loss: 9.715418815612793\n",
      "Epoch: 90 Training loss: 0.0 Validation loss: 9.715418815612793\n",
      "Epoch: 91 Training loss: 0.0 Validation loss: 9.71541965007782\n",
      "Epoch: 92 Training loss: 0.0 Validation loss: 9.71541965007782\n",
      "Epoch: 93 Training loss: 0.0 Validation loss: 9.715420722961426\n",
      "Epoch: 94 Training loss: 0.0 Validation loss: 9.715420007705688\n",
      "Epoch: 95 Training loss: 0.0 Validation loss: 9.715419888496399\n",
      "Epoch: 96 Training loss: 0.0 Validation loss: 9.715420365333557\n",
      "Epoch: 97 Training loss: 0.0 Validation loss: 9.715420126914978\n",
      "Epoch: 98 Training loss: 0.0 Validation loss: 9.715420126914978\n",
      "Epoch: 99 Training loss: 0.0 Validation loss: 9.715420365333557\n"
     ]
    }
   ],
   "source": [
    "# LBFGS, MSE\n",
    "trainLBFGS(model_LBFGS_MSE, optimizer_LBFGS_MSE, criterion2, tau,  N_EPOCHS, lossList_LBFGS_MSE, valList_LBFGS_MSE, \"MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aryamanj/miniconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/aryamanj/miniconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([47])) that is different to the input size (torch.Size([47, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training loss: 20.22290382385254 Validation loss: 16.227161407470703\n",
      "Epoch: 1 Training loss: 15.574099159240722 Validation loss: 18.1910080909729\n",
      "Epoch: 2 Training loss: 9.396281147003174 Validation loss: 14.96127986907959\n",
      "Epoch: 3 Training loss: 7.345251846313476 Validation loss: 10.014607906341553\n",
      "Epoch: 4 Training loss: 6.702865695953369 Validation loss: 9.800524353981018\n",
      "Epoch: 5 Training loss: 6.807854270935058 Validation loss: 10.254472255706787\n",
      "Epoch: 6 Training loss: 6.653886985778809 Validation loss: 9.740045189857483\n",
      "Epoch: 7 Training loss: 6.675432777404785 Validation loss: 9.743155837059021\n",
      "Epoch: 8 Training loss: 6.670226287841797 Validation loss: 9.89651894569397\n",
      "Epoch: 9 Training loss: 6.605729484558106 Validation loss: 9.676557540893555\n",
      "Epoch: 10 Training loss: 6.638999938964844 Validation loss: 9.690563440322876\n",
      "Epoch: 11 Training loss: 7.35370454788208 Validation loss: 10.988174676895142\n",
      "Epoch: 12 Training loss: 8.260419845581055 Validation loss: 9.741422414779663\n",
      "Epoch: 13 Training loss: 7.177040958404541 Validation loss: 10.778972506523132\n",
      "Epoch: 14 Training loss: 7.095961856842041 Validation loss: 11.372615694999695\n",
      "Epoch: 15 Training loss: 7.0942614555358885 Validation loss: 10.108859300613403\n",
      "Epoch: 16 Training loss: 6.481289005279541 Validation loss: 10.265636086463928\n",
      "Epoch: 17 Training loss: 6.545309543609619 Validation loss: 9.733968496322632\n",
      "Epoch: 18 Training loss: 6.885394477844239 Validation loss: 9.74980640411377\n",
      "Epoch: 19 Training loss: 7.525680160522461 Validation loss: 9.924005627632141\n",
      "Epoch: 20 Training loss: 7.5236564636230465 Validation loss: 11.074146389961243\n",
      "Epoch: 21 Training loss: 6.907787799835205 Validation loss: 11.501198530197144\n",
      "Epoch: 22 Training loss: 7.330956649780274 Validation loss: 10.796401739120483\n",
      "Epoch: 23 Training loss: 7.337116622924805 Validation loss: 10.290401458740234\n",
      "Epoch: 24 Training loss: 7.520589065551758 Validation loss: 10.131287336349487\n",
      "Epoch: 25 Training loss: 7.204399871826172 Validation loss: 9.744611740112305\n",
      "Epoch: 26 Training loss: 7.441807460784912 Validation loss: 11.170970678329468\n",
      "Epoch: 27 Training loss: 7.160587978363037 Validation loss: 11.091833233833313\n",
      "Epoch: 28 Training loss: 6.915519046783447 Validation loss: 10.026443719863892\n",
      "Epoch: 29 Training loss: 6.7152478218078615 Validation loss: 10.111201643943787\n",
      "Epoch: 30 Training loss: 6.653305435180664 Validation loss: 10.030815839767456\n",
      "Epoch: 31 Training loss: 7.563762378692627 Validation loss: 9.956730723381042\n",
      "Epoch: 32 Training loss: 6.890133857727051 Validation loss: 10.990234375\n",
      "Epoch: 33 Training loss: 7.064905834197998 Validation loss: 11.39178204536438\n",
      "Epoch: 34 Training loss: 7.247744560241699 Validation loss: 10.549900650978088\n",
      "Epoch: 35 Training loss: 6.816983985900879 Validation loss: 9.973016023635864\n",
      "Epoch: 36 Training loss: 6.5804009437561035 Validation loss: 9.988422513008118\n",
      "Epoch: 37 Training loss: 6.614526844024658 Validation loss: 10.036576986312866\n",
      "Epoch: 38 Training loss: 6.570254707336426 Validation loss: 10.236567974090576\n",
      "Epoch: 39 Training loss: 6.438094520568848 Validation loss: 10.153003811836243\n",
      "Epoch: 40 Training loss: 6.584095573425293 Validation loss: 10.323350429534912\n",
      "Epoch: 41 Training loss: 6.4865851402282715 Validation loss: 10.105763792991638\n",
      "Epoch: 42 Training loss: 6.563245964050293 Validation loss: 9.969096660614014\n",
      "Epoch: 43 Training loss: 6.450299644470215 Validation loss: 10.02467668056488\n",
      "Epoch: 44 Training loss: 6.95823564529419 Validation loss: 11.056968331336975\n",
      "Epoch: 45 Training loss: 6.867373657226563 Validation loss: 11.460166454315186\n",
      "Epoch: 46 Training loss: 6.904235363006592 Validation loss: 10.349903225898743\n",
      "Epoch: 47 Training loss: 6.773708057403565 Validation loss: 10.219021558761597\n",
      "Epoch: 48 Training loss: 6.4792228698730465 Validation loss: 10.353447318077087\n",
      "Epoch: 49 Training loss: 6.645155525207519 Validation loss: 10.243228554725647\n",
      "Epoch: 50 Training loss: 6.407347583770752 Validation loss: 10.111375570297241\n",
      "Epoch: 51 Training loss: 6.671512222290039 Validation loss: 10.279231309890747\n",
      "Epoch: 52 Training loss: 6.496379280090332 Validation loss: 10.229401230812073\n",
      "Epoch: 53 Training loss: 6.56040325164795 Validation loss: 11.406107187271118\n",
      "Epoch: 54 Training loss: 6.88208646774292 Validation loss: 10.991440415382385\n",
      "Epoch: 55 Training loss: 6.794806671142578 Validation loss: 9.968900680541992\n",
      "Epoch: 56 Training loss: 6.903285026550293 Validation loss: 9.965156316757202\n",
      "Epoch: 57 Training loss: 6.75947904586792 Validation loss: 10.036831974983215\n",
      "Epoch: 58 Training loss: 6.594618797302246 Validation loss: 10.188150644302368\n",
      "Epoch: 59 Training loss: 6.588605785369873 Validation loss: 10.090266585350037\n",
      "Epoch: 60 Training loss: 6.626045513153076 Validation loss: 10.325050950050354\n",
      "Epoch: 61 Training loss: 6.4460344314575195 Validation loss: 10.032791376113892\n",
      "Epoch: 62 Training loss: 6.7519042015075685 Validation loss: 10.05000901222229\n",
      "Epoch: 63 Training loss: 6.7798186302185055 Validation loss: 10.265009045600891\n",
      "Epoch: 64 Training loss: 6.514317893981934 Validation loss: 10.703653931617737\n",
      "Epoch: 65 Training loss: 6.569282913208008 Validation loss: 10.541477680206299\n",
      "Epoch: 66 Training loss: 6.774255466461182 Validation loss: 11.664457321166992\n",
      "Epoch: 67 Training loss: 7.2594749450683596 Validation loss: 10.028080582618713\n",
      "Epoch: 68 Training loss: 6.763509368896484 Validation loss: 9.98270297050476\n",
      "Epoch: 69 Training loss: 6.583490371704102 Validation loss: 10.089150190353394\n",
      "Epoch: 70 Training loss: 6.766254901885986 Validation loss: 10.483604550361633\n",
      "Epoch: 71 Training loss: 6.608798885345459 Validation loss: 11.212233662605286\n",
      "Epoch: 72 Training loss: 6.757774066925049 Validation loss: 10.50691545009613\n",
      "Epoch: 73 Training loss: 6.924210548400879 Validation loss: 10.535899639129639\n",
      "Epoch: 74 Training loss: 7.05695276260376 Validation loss: 10.461608290672302\n",
      "Epoch: 75 Training loss: 7.0291139602661135 Validation loss: 10.974411606788635\n",
      "Epoch: 76 Training loss: 6.647385597229004 Validation loss: 11.260218620300293\n",
      "Epoch: 77 Training loss: 7.025747299194336 Validation loss: 9.814592599868774\n",
      "Epoch: 78 Training loss: 6.725930118560791 Validation loss: 9.739405155181885\n",
      "Epoch: 79 Training loss: 6.720133972167969 Validation loss: 9.759260058403015\n",
      "Epoch: 80 Training loss: 6.814278697967529 Validation loss: 10.033261060714722\n",
      "Epoch: 81 Training loss: 6.537761592864991 Validation loss: 9.789244890213013\n",
      "Epoch: 82 Training loss: 6.527280807495117 Validation loss: 9.874582171440125\n",
      "Epoch: 83 Training loss: 6.452180671691894 Validation loss: 9.698625564575195\n",
      "Epoch: 84 Training loss: 6.6115490913391115 Validation loss: 10.857081174850464\n",
      "Epoch: 85 Training loss: 6.71492919921875 Validation loss: 9.86777925491333\n",
      "Epoch: 86 Training loss: 6.477707862854004 Validation loss: 10.057749509811401\n",
      "Epoch: 87 Training loss: 7.148929405212402 Validation loss: 10.796397089958191\n",
      "Epoch: 88 Training loss: 6.806785583496094 Validation loss: 10.471696376800537\n",
      "Epoch: 89 Training loss: 6.674400424957275 Validation loss: 9.983778238296509\n",
      "Epoch: 90 Training loss: 6.78223762512207 Validation loss: 9.89093828201294\n",
      "Epoch: 91 Training loss: 6.896099948883057 Validation loss: 9.971767663955688\n",
      "Epoch: 92 Training loss: 6.747477245330811 Validation loss: 10.480905175209045\n",
      "Epoch: 93 Training loss: 6.696097183227539 Validation loss: 10.233322501182556\n",
      "Epoch: 94 Training loss: 6.621694564819336 Validation loss: 9.887199640274048\n",
      "Epoch: 95 Training loss: 7.088613510131836 Validation loss: 9.944644570350647\n",
      "Epoch: 96 Training loss: 6.857325553894043 Validation loss: 10.91003692150116\n",
      "Epoch: 97 Training loss: 7.150406742095948 Validation loss: 10.201717853546143\n",
      "Epoch: 98 Training loss: 6.858237934112549 Validation loss: 9.884052038192749\n",
      "Epoch: 99 Training loss: 6.629067993164062 Validation loss: 10.139498591423035\n"
     ]
    }
   ],
   "source": [
    "# CLR, MAE:\n",
    "trainConstantLR(model_CLR_L1, optimizer_CLR_L1, criterion3, tau,  N_EPOCHS, lossList_CLR_L1, valList_CLR_L1, \"L1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aryamanj/miniconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([47])) that is different to the input size (torch.Size([47, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training loss: 628.5410583496093 Validation loss: 20.348630905151367\n",
      "Epoch: 1 Training loss: 269.8549865722656 Validation loss: 13.101255178451538\n",
      "Epoch: 2 Training loss: 157.23284301757812 Validation loss: 11.00379753112793\n",
      "Epoch: 3 Training loss: 112.60870666503907 Validation loss: 10.223612546920776\n",
      "Epoch: 4 Training loss: 93.40291442871094 Validation loss: 9.845509767532349\n",
      "Epoch: 5 Training loss: 90.85623779296876 Validation loss: 9.76801073551178\n",
      "Epoch: 6 Training loss: 87.01457977294922 Validation loss: 9.863399505615234\n",
      "Epoch: 7 Training loss: 86.20083312988281 Validation loss: 9.767314910888672\n",
      "Epoch: 8 Training loss: 84.21981506347656 Validation loss: 9.690276026725769\n",
      "Epoch: 9 Training loss: 84.03750610351562 Validation loss: 9.701805233955383\n",
      "Epoch: 10 Training loss: 85.85609893798828 Validation loss: 9.772770166397095\n",
      "Epoch: 11 Training loss: 86.06941070556641 Validation loss: 9.683846592903137\n",
      "Epoch: 12 Training loss: 84.8620590209961 Validation loss: 9.792319655418396\n",
      "Epoch: 13 Training loss: 86.68803100585937 Validation loss: 9.742738485336304\n",
      "Epoch: 14 Training loss: 88.10383605957031 Validation loss: 9.770995140075684\n",
      "Epoch: 15 Training loss: 87.30280456542968 Validation loss: 9.68057632446289\n",
      "Epoch: 16 Training loss: 87.24770965576172 Validation loss: 9.874447226524353\n",
      "Epoch: 17 Training loss: 85.08300018310547 Validation loss: 9.695562958717346\n",
      "Epoch: 18 Training loss: 83.97438049316406 Validation loss: 9.785626530647278\n",
      "Epoch: 19 Training loss: 87.75349273681641 Validation loss: 9.726605296134949\n",
      "Epoch: 20 Training loss: 85.12551727294922 Validation loss: 10.059983253479004\n",
      "Epoch: 21 Training loss: 88.83213653564454 Validation loss: 9.775424718856812\n",
      "Epoch: 22 Training loss: 90.05903396606445 Validation loss: 9.841997385025024\n",
      "Epoch: 23 Training loss: 88.53915405273438 Validation loss: 9.691898465156555\n",
      "Epoch: 24 Training loss: 90.53747100830078 Validation loss: 10.549014568328857\n",
      "Epoch: 25 Training loss: 91.57658538818359 Validation loss: 9.816779375076294\n",
      "Epoch: 26 Training loss: 88.75977020263672 Validation loss: 10.1661536693573\n",
      "Epoch: 27 Training loss: 95.40981903076172 Validation loss: 10.506701588630676\n",
      "Epoch: 28 Training loss: 94.50755310058594 Validation loss: 10.091352105140686\n",
      "Epoch: 29 Training loss: 88.21875915527343 Validation loss: 9.909594535827637\n",
      "Epoch: 30 Training loss: 86.5792839050293 Validation loss: 9.68319582939148\n",
      "Epoch: 31 Training loss: 84.13976745605468 Validation loss: 9.976280093193054\n",
      "Epoch: 32 Training loss: 89.96827087402343 Validation loss: 10.112186908721924\n",
      "Epoch: 33 Training loss: 88.95165710449218 Validation loss: 9.76971983909607\n",
      "Epoch: 34 Training loss: 89.8198013305664 Validation loss: 9.85706901550293\n",
      "Epoch: 35 Training loss: 86.42822875976563 Validation loss: 9.989901304244995\n",
      "Epoch: 36 Training loss: 89.67813720703126 Validation loss: 10.10971713066101\n",
      "Epoch: 37 Training loss: 89.94598693847657 Validation loss: 9.6823890209198\n",
      "Epoch: 38 Training loss: 86.0228515625 Validation loss: 10.218123435974121\n",
      "Epoch: 39 Training loss: 89.14840545654297 Validation loss: 9.766927123069763\n",
      "Epoch: 40 Training loss: 89.15394439697266 Validation loss: 9.716319799423218\n",
      "Epoch: 41 Training loss: 90.96754760742188 Validation loss: 10.067768216133118\n",
      "Epoch: 42 Training loss: 89.27595977783203 Validation loss: 10.126367568969727\n",
      "Epoch: 43 Training loss: 91.85122528076172 Validation loss: 9.715340375900269\n",
      "Epoch: 44 Training loss: 90.264990234375 Validation loss: 9.763319492340088\n",
      "Epoch: 45 Training loss: 86.86798858642578 Validation loss: 9.699788570404053\n",
      "Epoch: 46 Training loss: 86.02704315185547 Validation loss: 10.19320523738861\n",
      "Epoch: 47 Training loss: 86.07808532714844 Validation loss: 9.703871369361877\n",
      "Epoch: 48 Training loss: 84.65721588134765 Validation loss: 9.71481990814209\n",
      "Epoch: 49 Training loss: 84.9911735534668 Validation loss: 9.916462779045105\n",
      "Epoch: 50 Training loss: 83.99797439575195 Validation loss: 10.072400569915771\n",
      "Epoch: 51 Training loss: 88.9519416809082 Validation loss: 9.854352831840515\n",
      "Epoch: 52 Training loss: 84.94663619995117 Validation loss: 9.900345087051392\n",
      "Epoch: 53 Training loss: 86.36500701904296 Validation loss: 9.75395679473877\n",
      "Epoch: 54 Training loss: 85.98481597900391 Validation loss: 9.690802097320557\n",
      "Epoch: 55 Training loss: 86.2969871520996 Validation loss: 9.844345092773438\n",
      "Epoch: 56 Training loss: 88.84933776855469 Validation loss: 10.738654136657715\n",
      "Epoch: 57 Training loss: 88.30416564941406 Validation loss: 9.764966487884521\n",
      "Epoch: 58 Training loss: 87.68554992675782 Validation loss: 9.709466457366943\n",
      "Epoch: 59 Training loss: 91.51811752319335 Validation loss: 9.960541605949402\n",
      "Epoch: 60 Training loss: 85.30349578857422 Validation loss: 9.857571363449097\n",
      "Epoch: 61 Training loss: 86.48235092163085 Validation loss: 9.880860805511475\n",
      "Epoch: 62 Training loss: 84.73206176757813 Validation loss: 9.693606734275818\n",
      "Epoch: 63 Training loss: 86.86467742919922 Validation loss: 10.224165916442871\n",
      "Epoch: 64 Training loss: 90.0990966796875 Validation loss: 10.080146312713623\n",
      "Epoch: 65 Training loss: 95.04983978271484 Validation loss: 9.76656699180603\n",
      "Epoch: 66 Training loss: 93.52710571289063 Validation loss: 9.83324122428894\n",
      "Epoch: 67 Training loss: 85.65595092773438 Validation loss: 9.971754431724548\n",
      "Epoch: 68 Training loss: 85.69552612304688 Validation loss: 9.78733491897583\n",
      "Epoch: 69 Training loss: 86.20098724365235 Validation loss: 9.764349699020386\n",
      "Epoch: 70 Training loss: 85.01257781982422 Validation loss: 10.237816572189331\n",
      "Epoch: 71 Training loss: 85.58353729248047 Validation loss: 9.74633276462555\n",
      "Epoch: 72 Training loss: 87.98952941894531 Validation loss: 9.922641038894653\n",
      "Epoch: 73 Training loss: 82.73509140014649 Validation loss: 10.78528892993927\n",
      "Epoch: 74 Training loss: 95.4859848022461 Validation loss: 10.149006962776184\n",
      "Epoch: 75 Training loss: 92.30991821289062 Validation loss: 9.988205671310425\n",
      "Epoch: 76 Training loss: 86.45100402832031 Validation loss: 9.783888578414917\n",
      "Epoch: 77 Training loss: 86.5419189453125 Validation loss: 9.838358998298645\n",
      "Epoch: 78 Training loss: 84.4692367553711 Validation loss: 9.696850538253784\n",
      "Epoch: 79 Training loss: 84.42927398681641 Validation loss: 9.750929355621338\n",
      "Epoch: 80 Training loss: 85.76753768920898 Validation loss: 9.91409981250763\n",
      "Epoch: 81 Training loss: 85.86557159423828 Validation loss: 10.003162145614624\n",
      "Epoch: 82 Training loss: 96.8360610961914 Validation loss: 9.697683095932007\n",
      "Epoch: 83 Training loss: 88.2580551147461 Validation loss: 9.809378862380981\n",
      "Epoch: 84 Training loss: 83.66148071289062 Validation loss: 9.671820163726807\n",
      "Epoch: 85 Training loss: 83.3414306640625 Validation loss: 9.692171454429626\n",
      "Epoch: 86 Training loss: 82.44811401367187 Validation loss: 9.799012541770935\n",
      "Epoch: 87 Training loss: 85.89155426025391 Validation loss: 9.703295946121216\n",
      "Epoch: 88 Training loss: 81.65147399902344 Validation loss: 10.37016773223877\n",
      "Epoch: 89 Training loss: 90.39989776611328 Validation loss: 10.378878355026245\n",
      "Epoch: 90 Training loss: 99.92111358642578 Validation loss: 9.730305075645447\n",
      "Epoch: 91 Training loss: 86.1885856628418 Validation loss: 10.127562761306763\n",
      "Epoch: 92 Training loss: 89.02354888916015 Validation loss: 9.931899785995483\n",
      "Epoch: 93 Training loss: 93.70267791748047 Validation loss: 10.451104044914246\n",
      "Epoch: 94 Training loss: 89.8592544555664 Validation loss: 9.966558694839478\n",
      "Epoch: 95 Training loss: 91.95084228515626 Validation loss: 9.78027892112732\n",
      "Epoch: 96 Training loss: 88.42934875488281 Validation loss: 9.849265336990356\n",
      "Epoch: 97 Training loss: 86.90043182373047 Validation loss: 9.763118028640747\n",
      "Epoch: 98 Training loss: 83.5081771850586 Validation loss: 10.113282918930054\n",
      "Epoch: 99 Training loss: 88.45131072998046 Validation loss: 9.741243124008179\n"
     ]
    }
   ],
   "source": [
    "# CLR, MSE\n",
    "trainConstantLR(model_CLR_MSE, optimizer_CLR_MSE, criterion2, tau,  N_EPOCHS, lossList_CLR_MSE, valList_CLR_MSE, \"MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training loss: 11.746970844268798 Validation loss: 28.15874147415161\n",
      "Epoch: 1 Training loss: 7.524914121627807 Validation loss: 14.270958423614502\n",
      "Epoch: 2 Training loss: 4.464273262023926 Validation loss: 13.351861953735352\n",
      "Epoch: 3 Training loss: 4.1195387840271 Validation loss: 11.22227668762207\n",
      "Epoch: 4 Training loss: 3.428277063369751 Validation loss: 10.634599208831787\n",
      "Epoch: 5 Training loss: 3.3093474388122557 Validation loss: 10.996006846427917\n",
      "Epoch: 6 Training loss: 3.2050366401672363 Validation loss: 10.244851112365723\n",
      "Epoch: 7 Training loss: 3.185597467422485 Validation loss: 9.831474542617798\n",
      "Epoch: 8 Training loss: 3.1512866973876954 Validation loss: 10.135895252227783\n",
      "Epoch: 9 Training loss: 3.0762972831726074 Validation loss: 10.427598118782043\n",
      "Epoch: 10 Training loss: 3.002963066101074 Validation loss: 9.82954216003418\n",
      "Epoch: 11 Training loss: 2.9512307167053224 Validation loss: 9.854008674621582\n",
      "Epoch: 12 Training loss: 2.9240466594696044 Validation loss: 9.905982732772827\n",
      "Epoch: 13 Training loss: 3.0090781688690185 Validation loss: 9.730573177337646\n",
      "Epoch: 14 Training loss: 2.9945887088775636 Validation loss: 10.19955599308014\n",
      "Epoch: 15 Training loss: 2.983576250076294 Validation loss: 10.396231651306152\n",
      "Epoch: 16 Training loss: 3.1003997325897217 Validation loss: 10.019736528396606\n",
      "Epoch: 17 Training loss: 2.9655128479003907 Validation loss: 10.000518679618835\n",
      "Epoch: 18 Training loss: 2.9575639724731446 Validation loss: 9.919042229652405\n",
      "Epoch: 19 Training loss: 2.919442653656006 Validation loss: 9.832482933998108\n",
      "Epoch: 20 Training loss: 3.049221229553223 Validation loss: 9.92860460281372\n",
      "Epoch: 21 Training loss: 3.1685226917266847 Validation loss: 9.834990382194519\n",
      "Epoch: 22 Training loss: 3.0205389499664306 Validation loss: 10.013429760932922\n",
      "Epoch: 23 Training loss: 3.11159610748291 Validation loss: 10.18095052242279\n",
      "Epoch: 24 Training loss: 3.03223237991333 Validation loss: 11.755118608474731\n",
      "Epoch: 25 Training loss: 3.394319486618042 Validation loss: 9.935908555984497\n",
      "Epoch: 26 Training loss: 3.072992467880249 Validation loss: 9.877054452896118\n",
      "Epoch: 27 Training loss: 3.1264411926269533 Validation loss: 9.966853976249695\n",
      "Epoch: 28 Training loss: 3.1146078586578367 Validation loss: 10.662569522857666\n",
      "Epoch: 29 Training loss: 3.0768386363983153 Validation loss: 10.919756770133972\n",
      "Epoch: 30 Training loss: 3.202900457382202 Validation loss: 9.88815152645111\n",
      "Epoch: 31 Training loss: 3.0530786991119383 Validation loss: 10.26598048210144\n",
      "Epoch: 32 Training loss: 3.0130778312683106 Validation loss: 9.980873107910156\n",
      "Epoch: 33 Training loss: 3.016408824920654 Validation loss: 9.809929013252258\n",
      "Epoch: 34 Training loss: 2.9950288772583007 Validation loss: 10.159879446029663\n",
      "Epoch: 35 Training loss: 3.294906663894653 Validation loss: 9.760970830917358\n",
      "Epoch: 36 Training loss: 3.3127482891082765 Validation loss: 12.605674743652344\n",
      "Epoch: 37 Training loss: 3.562653589248657 Validation loss: 10.065941333770752\n",
      "Epoch: 38 Training loss: 3.3276914596557616 Validation loss: 9.86616039276123\n",
      "Epoch: 39 Training loss: 3.342625141143799 Validation loss: 10.79425311088562\n",
      "Epoch: 40 Training loss: 3.1995450019836427 Validation loss: 10.225446820259094\n",
      "Epoch: 41 Training loss: 3.1076622009277344 Validation loss: 9.922355651855469\n",
      "Epoch: 42 Training loss: 2.9773425102233886 Validation loss: 9.811742186546326\n",
      "Epoch: 43 Training loss: 3.0768736362457276 Validation loss: 10.720230102539062\n",
      "Epoch: 44 Training loss: 3.1505627155303957 Validation loss: 9.87889301776886\n",
      "Epoch: 45 Training loss: 3.0703965187072755 Validation loss: 9.75887680053711\n",
      "Epoch: 46 Training loss: 3.0551825046539305 Validation loss: 9.958263993263245\n",
      "Epoch: 47 Training loss: 3.0263722419738768 Validation loss: 10.32223117351532\n",
      "Epoch: 48 Training loss: 3.002573823928833 Validation loss: 9.835772037506104\n",
      "Epoch: 49 Training loss: 3.081135559082031 Validation loss: 10.737008452415466\n",
      "Epoch: 50 Training loss: 3.0674910068511965 Validation loss: 9.900555610656738\n",
      "Epoch: 51 Training loss: 3.1354252338409423 Validation loss: 10.076129913330078\n",
      "Epoch: 52 Training loss: 3.129445028305054 Validation loss: 10.196405410766602\n",
      "Epoch: 53 Training loss: 3.204345226287842 Validation loss: 10.16269600391388\n",
      "Epoch: 54 Training loss: 3.108314275741577 Validation loss: 10.026268482208252\n",
      "Epoch: 55 Training loss: 3.011075830459595 Validation loss: 9.933154463768005\n",
      "Epoch: 56 Training loss: 3.140997362136841 Validation loss: 11.268019676208496\n",
      "Epoch: 57 Training loss: 3.269296979904175 Validation loss: 9.782020330429077\n",
      "Epoch: 58 Training loss: 3.208839511871338 Validation loss: 9.878478765487671\n",
      "Epoch: 59 Training loss: 3.031931924819946 Validation loss: 9.902221083641052\n",
      "Epoch: 60 Training loss: 2.956513452529907 Validation loss: 10.289338946342468\n",
      "Epoch: 61 Training loss: 2.96205587387085 Validation loss: 9.783816576004028\n",
      "Epoch: 62 Training loss: 3.082486295700073 Validation loss: 9.709391713142395\n",
      "Epoch: 63 Training loss: 2.9450838565826416 Validation loss: 10.439335823059082\n",
      "Epoch: 64 Training loss: 2.990177536010742 Validation loss: 9.747327208518982\n",
      "Epoch: 65 Training loss: 2.9979562759399414 Validation loss: 9.823126316070557\n",
      "Epoch: 66 Training loss: 3.0079839706420897 Validation loss: 10.585346341133118\n",
      "Epoch: 67 Training loss: 3.0333423614501953 Validation loss: 10.0043306350708\n",
      "Epoch: 68 Training loss: 2.8972285747528077 Validation loss: 9.867233276367188\n",
      "Epoch: 69 Training loss: 2.956309366226196 Validation loss: 10.135635495185852\n",
      "Epoch: 70 Training loss: 2.928514337539673 Validation loss: 9.811055064201355\n",
      "Epoch: 71 Training loss: 3.0036923408508303 Validation loss: 9.838405966758728\n",
      "Epoch: 72 Training loss: 2.970043182373047 Validation loss: 9.942534446716309\n",
      "Epoch: 73 Training loss: 2.938625764846802 Validation loss: 9.733376264572144\n",
      "Epoch: 74 Training loss: 2.969851016998291 Validation loss: 10.325493454933167\n",
      "Epoch: 75 Training loss: 2.9849605560302734 Validation loss: 9.781335949897766\n",
      "Epoch: 76 Training loss: 2.9332215785980225 Validation loss: 9.816847443580627\n",
      "Epoch: 77 Training loss: 3.070828104019165 Validation loss: 10.09857964515686\n",
      "Epoch: 78 Training loss: 3.0104729652404787 Validation loss: 10.295557022094727\n",
      "Epoch: 79 Training loss: 3.045897340774536 Validation loss: 9.689203023910522\n",
      "Epoch: 80 Training loss: 3.043326807022095 Validation loss: 9.98749053478241\n",
      "Epoch: 81 Training loss: 2.988633918762207 Validation loss: 9.898948311805725\n",
      "Epoch: 82 Training loss: 3.0042620658874513 Validation loss: 9.87796688079834\n",
      "Epoch: 83 Training loss: 2.9937530994415282 Validation loss: 9.893470883369446\n",
      "Epoch: 84 Training loss: 2.9587599277496337 Validation loss: 9.864921569824219\n",
      "Epoch: 85 Training loss: 2.8844831943511964 Validation loss: 10.187042117118835\n",
      "Epoch: 86 Training loss: 2.941697835922241 Validation loss: 9.849030375480652\n",
      "Epoch: 87 Training loss: 2.920533609390259 Validation loss: 9.877183675765991\n",
      "Epoch: 88 Training loss: 2.939420461654663 Validation loss: 9.802702188491821\n",
      "Epoch: 89 Training loss: 2.97384033203125 Validation loss: 9.940253973007202\n",
      "Epoch: 90 Training loss: 2.971854639053345 Validation loss: 9.906688690185547\n",
      "Epoch: 91 Training loss: 2.9776981353759764 Validation loss: 9.82302737236023\n",
      "Epoch: 92 Training loss: 2.996127653121948 Validation loss: 9.79042637348175\n",
      "Epoch: 93 Training loss: 2.979233980178833 Validation loss: 10.081594467163086\n",
      "Epoch: 94 Training loss: 2.956363010406494 Validation loss: 10.494338512420654\n",
      "Epoch: 95 Training loss: 3.045041084289551 Validation loss: 9.683265566825867\n",
      "Epoch: 96 Training loss: 2.9511057853698732 Validation loss: 10.37239682674408\n",
      "Epoch: 97 Training loss: 3.0945835590362547 Validation loss: 9.746676921844482\n",
      "Epoch: 98 Training loss: 3.161836862564087 Validation loss: 9.774168014526367\n",
      "Epoch: 99 Training loss: 3.011658477783203 Validation loss: 10.708223342895508\n"
     ]
    }
   ],
   "source": [
    "# CLR, LC\n",
    "trainConstantLR(model_CLR_LC, optimizer_CLR_LC, criterion1, tau,  N_EPOCHS, lossList_CLR_LC, valList_CLR_LC, \"LC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training Loss: 6.296551513671875 Validation loss: 11.224769830703735 LR: 0.045352753251791\n",
      "Epoch: 1 Training Loss: 3.6325222492218017 Validation loss: 10.091965556144714 LR: 0.011749116703867912\n",
      "Epoch: 2 Training Loss: 3.1882354259490966 Validation loss: 10.866821885108948 LR: 0.011659768410027027\n",
      "Epoch: 3 Training Loss: 3.0330012321472166 Validation loss: 9.74666976928711 LR: 0.01162084098905325\n",
      "Epoch: 4 Training Loss: 3.0073118209838867 Validation loss: 9.757845282554626 LR: 0.011616628617048264\n",
      "Epoch: 5 Training Loss: 2.9212725162506104 Validation loss: 10.048588633537292 LR: 0.011630753055214882\n",
      "Epoch: 6 Training Loss: 2.923072576522827 Validation loss: 9.993490099906921 LR: 0.011627797968685627\n",
      "Epoch: 7 Training Loss: 2.937275505065918 Validation loss: 9.719716429710388 LR: 0.011625432409346104\n",
      "Epoch: 8 Training Loss: 2.9435542583465577 Validation loss: 9.957775473594666 LR: 0.011621377430856228\n",
      "Epoch: 9 Training Loss: 2.9190475940704346 Validation loss: 9.997872471809387 LR: 0.011613293550908566\n",
      "Epoch: 10 Training Loss: 2.930929660797119 Validation loss: 9.750987768173218 LR: 0.011607082560658455\n",
      "Epoch: 11 Training Loss: 2.9272725105285646 Validation loss: 9.867623805999756 LR: 0.011608420871198177\n",
      "Epoch: 12 Training Loss: 2.8902299880981444 Validation loss: 9.868321418762207 LR: 0.011604907922446728\n",
      "Epoch: 13 Training Loss: 2.9004493713378907 Validation loss: 9.782397866249084 LR: 0.011603808030486107\n",
      "Epoch: 14 Training Loss: 2.932882022857666 Validation loss: 9.891339302062988 LR: 0.01159962173551321\n",
      "Epoch: 15 Training Loss: 2.9335748195648192 Validation loss: 9.911101937294006 LR: 0.011600492522120476\n",
      "Epoch: 16 Training Loss: 2.9055511474609377 Validation loss: 9.8479585647583 LR: 0.011605429463088512\n",
      "Epoch: 17 Training Loss: 2.9163904666900633 Validation loss: 9.804237723350525 LR: 0.011604159139096737\n",
      "Epoch: 18 Training Loss: 2.902370834350586 Validation loss: 10.02785062789917 LR: 0.011599662713706493\n",
      "Epoch: 19 Training Loss: 2.9090989112854 Validation loss: 9.75178575515747 LR: 0.011600504629313946\n",
      "Epoch: 20 Training Loss: 2.926549530029297 Validation loss: 9.918221473693848 LR: 0.01159899216145277\n",
      "Epoch: 21 Training Loss: 2.923726511001587 Validation loss: 10.01297640800476 LR: 0.011600407771766186\n",
      "Epoch: 22 Training Loss: 2.935122346878052 Validation loss: 9.780917644500732 LR: 0.011601087637245655\n",
      "Epoch: 23 Training Loss: 2.8815805912017822 Validation loss: 9.968358278274536 LR: 0.01159850973635912\n",
      "Epoch: 24 Training Loss: 2.9411134243011476 Validation loss: 9.831577062606812 LR: 0.011599497869610786\n",
      "Epoch: 25 Training Loss: 2.950984764099121 Validation loss: 9.90591049194336 LR: 0.011591160669922829\n",
      "Epoch: 26 Training Loss: 2.882302236557007 Validation loss: 9.819397330284119 LR: 0.01158713735640049\n",
      "Epoch: 27 Training Loss: 2.911706972122192 Validation loss: 9.918116211891174 LR: 0.011598438955843449\n",
      "Epoch: 28 Training Loss: 2.905903625488281 Validation loss: 9.85631275177002 LR: 0.01159876212477684\n",
      "Epoch: 29 Training Loss: 2.8982035160064696 Validation loss: 9.806723594665527 LR: 0.011593504808843136\n",
      "Epoch: 30 Training Loss: 2.9116793632507325 Validation loss: 9.94813883304596 LR: 0.011592434719204903\n",
      "Epoch: 31 Training Loss: 2.9133277893066407 Validation loss: 9.812763810157776 LR: 0.011598721146583557\n",
      "Epoch: 32 Training Loss: 2.9297915935516357 Validation loss: 9.926505446434021 LR: 0.011594632640480995\n",
      "Epoch: 33 Training Loss: 2.9588700771331786 Validation loss: 9.859337449073792 LR: 0.011594335548579693\n",
      "Epoch: 34 Training Loss: 2.9230848789215087 Validation loss: 9.854042291641235 LR: 0.01159675233066082\n",
      "Epoch: 35 Training Loss: 2.917902135848999 Validation loss: 9.878632664680481 LR: 0.011595640331506729\n",
      "Epoch: 36 Training Loss: 2.9348957538604736 Validation loss: 9.897798776626587 LR: 0.011594041250646114\n",
      "Epoch: 37 Training Loss: 2.974424123764038 Validation loss: 9.799607992172241 LR: 0.01159602776169777\n",
      "Epoch: 38 Training Loss: 2.942843294143677 Validation loss: 9.931241750717163 LR: 0.01159681472927332\n",
      "Epoch: 39 Training Loss: 2.938540506362915 Validation loss: 9.88421881198883 LR: 0.011597524397075176\n",
      "Epoch: 40 Training Loss: 2.9003084182739256 Validation loss: 9.853518724441528 LR: 0.011592993512749672\n",
      "Epoch: 41 Training Loss: 2.983488178253174 Validation loss: 9.871049404144287 LR: 0.011594701558351517\n",
      "Epoch: 42 Training Loss: 2.962367534637451 Validation loss: 9.943689584732056 LR: 0.011591153219342232\n",
      "Epoch: 43 Training Loss: 2.905190658569336 Validation loss: 9.735273599624634 LR: 0.01159348152577877\n",
      "Epoch: 44 Training Loss: 2.917846441268921 Validation loss: 9.897369146347046 LR: 0.01159101165831089\n",
      "Epoch: 45 Training Loss: 2.960440731048584 Validation loss: 9.857316136360168 LR: 0.011591528542339802\n",
      "Epoch: 46 Training Loss: 2.950645685195923 Validation loss: 10.027741551399231 LR: 0.0115894116461277\n",
      "Epoch: 47 Training Loss: 2.93765549659729 Validation loss: 9.728077292442322 LR: 0.011590198613703251\n",
      "Epoch: 48 Training Loss: 2.935018301010132 Validation loss: 9.979079484939575 LR: 0.011593918316066265\n",
      "Epoch: 49 Training Loss: 2.927214765548706 Validation loss: 9.860296487808228 LR: 0.011592157185077667\n",
      "Epoch: 50 Training Loss: 2.9313145160675047 Validation loss: 9.825804829597473 LR: 0.011586477980017662\n",
      "Epoch: 51 Training Loss: 2.927216815948486 Validation loss: 9.792919039726257 LR: 0.011591642163693905\n",
      "Epoch: 52 Training Loss: 2.936635398864746 Validation loss: 9.867377042770386 LR: 0.01157976221293211\n",
      "Epoch: 53 Training Loss: 2.9792494773864746 Validation loss: 9.868878841400146 LR: 0.011594642885029316\n",
      "Epoch: 54 Training Loss: 2.9683385372161863 Validation loss: 9.7357816696167 LR: 0.011592879891395569\n",
      "Epoch: 55 Training Loss: 2.978108787536621 Validation loss: 10.148842215538025 LR: 0.011592475697398186\n",
      "Epoch: 56 Training Loss: 2.940463066101074 Validation loss: 9.676631808280945 LR: 0.011593768373131752\n",
      "Epoch: 57 Training Loss: 2.961105728149414 Validation loss: 10.036763548851013 LR: 0.011589260771870613\n",
      "Epoch: 58 Training Loss: 2.9831127643585207 Validation loss: 9.824079990386963 LR: 0.011590409092605114\n",
      "Epoch: 59 Training Loss: 3.0041858673095705 Validation loss: 9.71517038345337 LR: 0.01159009337425232\n",
      "Epoch: 60 Training Loss: 2.98197283744812 Validation loss: 10.20260202884674 LR: 0.011590875685214996\n",
      "Epoch: 61 Training Loss: 2.996193599700928 Validation loss: 9.697731971740723 LR: 0.011591039597988129\n",
      "Epoch: 62 Training Loss: 2.9190664291381836 Validation loss: 10.15116024017334 LR: 0.011595336720347404\n",
      "Epoch: 63 Training Loss: 2.974422311782837 Validation loss: 9.78893768787384 LR: 0.01158679649233818\n",
      "Epoch: 64 Training Loss: 2.9576234340667726 Validation loss: 9.790314078330994 LR: 0.011588074266910553\n",
      "Epoch: 65 Training Loss: 2.960011291503906 Validation loss: 9.993762135505676 LR: 0.011587110348045826\n",
      "Epoch: 66 Training Loss: 2.9240073680877687 Validation loss: 9.755890250205994 LR: 0.011585630476474762\n",
      "Epoch: 67 Training Loss: 2.997823143005371 Validation loss: 10.055550813674927 LR: 0.01158322673290968\n",
      "Epoch: 68 Training Loss: 2.931312847137451 Validation loss: 9.729534029960632 LR: 0.01158690731972456\n",
      "Epoch: 69 Training Loss: 2.900149774551392 Validation loss: 9.908661127090454 LR: 0.011580613441765308\n",
      "Epoch: 70 Training Loss: 2.941682052612305 Validation loss: 9.915963888168335 LR: 0.011578964069485664\n",
      "Epoch: 71 Training Loss: 2.99983811378479 Validation loss: 9.70235788822174 LR: 0.011576790362596512\n",
      "Epoch: 72 Training Loss: 2.9669413566589355 Validation loss: 10.07750380039215 LR: 0.011579415760934353\n",
      "Epoch: 73 Training Loss: 2.9468833446502685 Validation loss: 9.816959023475647 LR: 0.011582963168621063\n",
      "Epoch: 74 Training Loss: 2.9262364387512205 Validation loss: 9.813481450080872 LR: 0.011583621613681316\n",
      "Epoch: 75 Training Loss: 2.9380190849304197 Validation loss: 9.938704371452332 LR: 0.011582580395042896\n",
      "Epoch: 76 Training Loss: 2.924874782562256 Validation loss: 9.781706929206848 LR: 0.011583819054067135\n",
      "Epoch: 77 Training Loss: 2.903558683395386 Validation loss: 10.049890995025635 LR: 0.01157613005489111\n",
      "Epoch: 78 Training Loss: 2.9585144996643065 Validation loss: 9.826293468475342 LR: 0.011581472121179104\n",
      "Epoch: 79 Training Loss: 2.9243754863739015 Validation loss: 9.968797087669373 LR: 0.011578632518649101\n",
      "Epoch: 80 Training Loss: 2.9335267543792725 Validation loss: 9.788797378540039 LR: 0.011580507270991802\n",
      "Epoch: 81 Training Loss: 2.9149176120758056 Validation loss: 9.876031041145325 LR: 0.011585398577153683\n",
      "Epoch: 82 Training Loss: 2.926052284240723 Validation loss: 9.855871319770813 LR: 0.01158273033797741\n",
      "Epoch: 83 Training Loss: 2.9249569892883303 Validation loss: 10.000109076499939 LR: 0.011583990417420864\n",
      "Epoch: 84 Training Loss: 2.950524663925171 Validation loss: 9.777312874794006 LR: 0.011582649312913418\n",
      "Epoch: 85 Training Loss: 2.9108025074005126 Validation loss: 9.984241247177124 LR: 0.01158303115516901\n",
      "Epoch: 86 Training Loss: 2.9476580142974855 Validation loss: 9.856562972068787 LR: 0.011578909121453762\n",
      "Epoch: 87 Training Loss: 2.8915015697479247 Validation loss: 9.84064769744873 LR: 0.011581591330468655\n",
      "Epoch: 88 Training Loss: 2.9402201652526854 Validation loss: 9.841245651245117 LR: 0.011583219282329082\n",
      "Epoch: 89 Training Loss: 2.952243375778198 Validation loss: 9.888669490814209 LR: 0.011578872799873352\n",
      "Epoch: 90 Training Loss: 2.920370101928711 Validation loss: 9.75728702545166 LR: 0.01157293375581503\n",
      "Epoch: 91 Training Loss: 2.9008699893951415 Validation loss: 9.881141543388367 LR: 0.011583243496716022\n",
      "Epoch: 92 Training Loss: 2.9079169273376464 Validation loss: 9.904028534889221 LR: 0.011584402993321419\n",
      "Epoch: 93 Training Loss: 2.9049193382263185 Validation loss: 9.824848771095276 LR: 0.011582481674849987\n",
      "Epoch: 94 Training Loss: 2.9477556705474854 Validation loss: 9.771562933921814 LR: 0.011582235805690289\n",
      "Epoch: 95 Training Loss: 2.8779398679733275 Validation loss: 9.972380876541138 LR: 0.011580836959183216\n",
      "Epoch: 96 Training Loss: 2.9569427490234377 Validation loss: 9.738669037818909 LR: 0.011583801358938217\n",
      "Epoch: 97 Training Loss: 2.922836685180664 Validation loss: 10.048857569694519 LR: 0.011583412997424603\n",
      "Epoch: 98 Training Loss: 2.92474308013916 Validation loss: 9.718385815620422 LR: 0.011580554768443108\n",
      "Epoch: 99 Training Loss: 2.9341333866119386 Validation loss: 9.932047247886658 LR: 0.01158326119184494\n"
     ]
    }
   ],
   "source": [
    "# LALR, LC\n",
    "trainLALR(model_LALR_LC, optimizer_LALR_LC, criterion1, tau,  N_EPOCHS, lossList_LALR_LC, valList_LALR_LC, \"LC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training Loss: 390.4385192871094 Validation loss: 12.94198489189148 LR: 0.008804201148450375\n",
      "Epoch: 1 Training Loss: 119.33902740478516 Validation loss: 10.953763365745544 LR: 0.0019581054802984\n",
      "Epoch: 2 Training Loss: 93.26563262939453 Validation loss: 9.86756718158722 LR: 0.0021676404867321253\n",
      "Epoch: 3 Training Loss: 82.06964950561523 Validation loss: 9.680711507797241 LR: 0.0016715467208996415\n",
      "Epoch: 4 Training Loss: 82.95601119995118 Validation loss: 9.69076418876648 LR: 0.0016192805487662554\n",
      "Epoch: 5 Training Loss: 84.82884979248047 Validation loss: 9.699005603790283 LR: 0.0015972956316545606\n",
      "Epoch: 6 Training Loss: 85.85055694580078 Validation loss: 9.677596807479858 LR: 0.001593135530129075\n",
      "Epoch: 7 Training Loss: 83.572314453125 Validation loss: 9.677178025245667 LR: 0.0016006232472136617\n",
      "Epoch: 8 Training Loss: 85.22699584960938 Validation loss: 9.709229826927185 LR: 0.0016145985573530197\n",
      "Epoch: 9 Training Loss: 83.54543762207031 Validation loss: 9.744696736335754 LR: 0.0017878490034490824\n",
      "Epoch: 10 Training Loss: 84.03257293701172 Validation loss: 9.758920907974243 LR: 0.001636987435631454\n",
      "Epoch: 11 Training Loss: 82.71227951049805 Validation loss: 9.750182032585144 LR: 0.0016402106266468763\n",
      "Epoch: 12 Training Loss: 80.85184783935547 Validation loss: 9.727336525917053 LR: 0.0016384113114327192\n",
      "Epoch: 13 Training Loss: 81.92536849975586 Validation loss: 9.715900182723999 LR: 0.0016327420016750693\n",
      "Epoch: 14 Training Loss: 82.08221282958985 Validation loss: 9.702733516693115 LR: 0.001630317186936736\n",
      "Epoch: 15 Training Loss: 84.22955017089843 Validation loss: 9.710318684577942 LR: 0.0016255560331046581\n",
      "Epoch: 16 Training Loss: 86.10245819091797 Validation loss: 9.710626482963562 LR: 0.001628239406272769\n",
      "Epoch: 17 Training Loss: 83.8481231689453 Validation loss: 9.702078938484192 LR: 0.0016279076226055622\n",
      "Epoch: 18 Training Loss: 83.53326568603515 Validation loss: 9.703357458114624 LR: 0.001625940902158618\n",
      "Epoch: 19 Training Loss: 82.85727157592774 Validation loss: 9.713444590568542 LR: 0.001625221106223762\n",
      "Epoch: 20 Training Loss: 83.99921264648438 Validation loss: 9.711101651191711 LR: 0.0016289957566186786\n",
      "Epoch: 21 Training Loss: 84.19225616455078 Validation loss: 9.708335876464844 LR: 0.0016273106448352337\n",
      "Epoch: 22 Training Loss: 84.40057220458985 Validation loss: 9.715601325035095 LR: 0.001981698675081134\n",
      "Epoch: 23 Training Loss: 84.01455917358399 Validation loss: 9.71152114868164 LR: 0.001628626137971878\n",
      "Epoch: 24 Training Loss: 83.48930740356445 Validation loss: 9.708977937698364 LR: 0.0016259063268080354\n",
      "Epoch: 25 Training Loss: 83.08258209228515 Validation loss: 9.711482882499695 LR: 0.001625415519811213\n",
      "Epoch: 26 Training Loss: 83.81936950683594 Validation loss: 9.71422290802002 LR: 0.0016242905985563993\n",
      "Epoch: 27 Training Loss: 84.03213348388672 Validation loss: 9.707409620285034 LR: 0.0016258555697277188\n",
      "Epoch: 28 Training Loss: 82.78229675292968 Validation loss: 9.705369234085083 LR: 0.001624558586627245\n",
      "Epoch: 29 Training Loss: 81.4035774230957 Validation loss: 9.710914969444275 LR: 0.0016236063092947006\n",
      "Epoch: 30 Training Loss: 82.29483337402344 Validation loss: 9.713222026824951 LR: 0.0016246570739895105\n",
      "Epoch: 31 Training Loss: 81.69903869628907 Validation loss: 9.71989357471466 LR: 0.001626475714147091\n",
      "Epoch: 32 Training Loss: 82.23357162475585 Validation loss: 9.719803929328918 LR: 0.0016282580327242613\n",
      "Epoch: 33 Training Loss: 83.59189300537109 Validation loss: 9.723289728164673 LR: 0.0016267569735646248\n",
      "Epoch: 34 Training Loss: 82.44275665283203 Validation loss: 9.717294216156006 LR: 0.0016287572216242552\n",
      "Epoch: 35 Training Loss: 82.10183868408203 Validation loss: 9.721825003623962 LR: 0.0016274936497211456\n",
      "Epoch: 36 Training Loss: 80.86269073486328 Validation loss: 9.712603569030762 LR: 0.0016266951570287347\n",
      "Epoch: 37 Training Loss: 82.17684936523438 Validation loss: 9.702562808990479 LR: 0.0016250392654910684\n",
      "Epoch: 38 Training Loss: 84.8699577331543 Validation loss: 9.71665608882904 LR: 0.0016211076872423291\n",
      "Epoch: 39 Training Loss: 84.37376556396484 Validation loss: 9.71619725227356 LR: 0.0016259246040135622\n",
      "Epoch: 40 Training Loss: 82.9869873046875 Validation loss: 9.701715230941772 LR: 0.0016264512669295073\n",
      "Epoch: 41 Training Loss: 83.56912078857422 Validation loss: 9.695111393928528 LR: 0.0016212441259995103\n",
      "Epoch: 42 Training Loss: 81.69759826660156 Validation loss: 9.69834291934967 LR: 0.0016186954453587532\n",
      "Epoch: 43 Training Loss: 81.47400436401367 Validation loss: 9.714553594589233 LR: 0.0016199002275243402\n",
      "Epoch: 44 Training Loss: 83.56109466552735 Validation loss: 9.724434852600098 LR: 0.0016248150495812297\n",
      "Epoch: 45 Training Loss: 83.56991348266601 Validation loss: 9.721531629562378 LR: 0.0016271054046228528\n",
      "Epoch: 46 Training Loss: 84.23052597045898 Validation loss: 9.716386675834656 LR: 0.0016271778149530292\n",
      "Epoch: 47 Training Loss: 84.20364227294922 Validation loss: 9.722630739212036 LR: 0.001626000739634037\n",
      "Epoch: 48 Training Loss: 83.00583190917969 Validation loss: 9.713126420974731 LR: 0.0016266132006421685\n",
      "Epoch: 49 Training Loss: 83.27276306152343 Validation loss: 9.69868266582489 LR: 0.0016250039916485548\n",
      "Epoch: 50 Training Loss: 82.1779296875 Validation loss: 9.698426485061646 LR: 0.0016201183898374438\n",
      "Epoch: 51 Training Loss: 82.2737548828125 Validation loss: 9.710968375205994 LR: 0.0016199576202780008\n",
      "Epoch: 52 Training Loss: 81.63876495361328 Validation loss: 9.711868286132812 LR: 0.0016242369310930371\n",
      "Epoch: 53 Training Loss: 82.31917724609374 Validation loss: 9.713951230049133 LR: 0.0016243048012256622\n",
      "Epoch: 54 Training Loss: 81.8593994140625 Validation loss: 9.721088171005249 LR: 0.0016241976991295815\n",
      "Epoch: 55 Training Loss: 83.28016815185546 Validation loss: 9.718263983726501 LR: 0.0016257879324257374\n",
      "Epoch: 56 Training Loss: 81.97596817016601 Validation loss: 9.713467717170715 LR: 0.0016247135354205966\n",
      "Epoch: 57 Training Loss: 83.4835220336914 Validation loss: 9.696459293365479 LR: 0.00162336730863899\n",
      "Epoch: 58 Training Loss: 85.16649475097657 Validation loss: 9.69956350326538 LR: 0.0018376678926870227\n",
      "Epoch: 59 Training Loss: 83.63570861816406 Validation loss: 9.70130455493927 LR: 0.00161728635430336\n",
      "Epoch: 60 Training Loss: 82.37465057373046 Validation loss: 9.709867238998413 LR: 0.0016191649483516812\n",
      "Epoch: 61 Training Loss: 83.27232971191407 Validation loss: 9.728490948677063 LR: 0.001620981958694756\n",
      "Epoch: 62 Training Loss: 80.49205169677734 Validation loss: 9.72990870475769 LR: 0.0016246705781668425\n",
      "Epoch: 63 Training Loss: 83.06163177490234 Validation loss: 9.73551082611084 LR: 0.001664906507357955\n",
      "Epoch: 64 Training Loss: 84.17115936279296 Validation loss: 9.715718150138855 LR: 0.0016265164595097303\n",
      "Epoch: 65 Training Loss: 83.55345153808594 Validation loss: 9.689238905906677 LR: 0.0016226554289460182\n",
      "Epoch: 66 Training Loss: 84.41448974609375 Validation loss: 9.697848320007324 LR: 0.0016121013322845101\n",
      "Epoch: 67 Training Loss: 81.86328811645508 Validation loss: 9.691308379173279 LR: 0.001616846420802176\n",
      "Epoch: 68 Training Loss: 83.60078125 Validation loss: 9.709654927253723 LR: 0.0016149766743183136\n",
      "Epoch: 69 Training Loss: 82.52744216918946 Validation loss: 9.728390455245972 LR: 0.0016214594943448901\n",
      "Epoch: 70 Training Loss: 83.02354888916015 Validation loss: 9.722469925880432 LR: 0.0016251795459538698\n",
      "Epoch: 71 Training Loss: 82.1342399597168 Validation loss: 9.725443601608276 LR: 0.0016236396040767431\n",
      "Epoch: 72 Training Loss: 82.8379913330078 Validation loss: 9.725790143013 LR: 0.001663657953031361\n",
      "Epoch: 73 Training Loss: 83.00269165039063 Validation loss: 9.699826717376709 LR: 0.0016244900180026889\n",
      "Epoch: 74 Training Loss: 82.40448150634765 Validation loss: 9.701062440872192 LR: 0.0016160310478881001\n",
      "Epoch: 75 Training Loss: 83.28440017700196 Validation loss: 9.706726789474487 LR: 0.0016160542145371437\n",
      "Epoch: 76 Training Loss: 82.88472137451171 Validation loss: 9.69176459312439 LR: 0.0017117051174864173\n",
      "Epoch: 77 Training Loss: 84.0403549194336 Validation loss: 9.721651673316956 LR: 0.0016128814313560724\n",
      "Epoch: 78 Training Loss: 83.58128204345704 Validation loss: 9.726041913032532 LR: 0.0016225380823016167\n",
      "Epoch: 79 Training Loss: 84.55791473388672 Validation loss: 9.727986693382263 LR: 0.0016236979281529784\n",
      "Epoch: 80 Training Loss: 81.53419342041016 Validation loss: 9.719643831253052 LR: 0.0016235488001257181\n",
      "Epoch: 81 Training Loss: 83.0262237548828 Validation loss: 9.689868688583374 LR: 0.0016212334157899022\n",
      "Epoch: 82 Training Loss: 81.94266815185547 Validation loss: 9.69139552116394 LR: 0.0016115355538204312\n",
      "Epoch: 83 Training Loss: 84.19943542480469 Validation loss: 9.71119225025177 LR: 0.001650733407586813\n",
      "Epoch: 84 Training Loss: 82.26078643798829 Validation loss: 9.736798286437988 LR: 0.0016195393400266767\n",
      "Epoch: 85 Training Loss: 81.61123962402344 Validation loss: 9.742531776428223 LR: 0.0016247538151219487\n",
      "Epoch: 86 Training Loss: 81.24441223144531 Validation loss: 9.709330201148987 LR: 0.0016257999232038856\n",
      "Epoch: 87 Training Loss: 82.40858001708985 Validation loss: 9.71356451511383 LR: 0.0016185215208679438\n",
      "Epoch: 88 Training Loss: 82.22100448608398 Validation loss: 9.72243845462799 LR: 0.0017149762716144323\n",
      "Epoch: 89 Training Loss: 82.258544921875 Validation loss: 9.695678949356079 LR: 0.001622266718186438\n",
      "Epoch: 90 Training Loss: 82.66104431152344 Validation loss: 9.700392723083496 LR: 0.0016153083415701985\n",
      "Epoch: 91 Training Loss: 85.41013946533204 Validation loss: 9.709025740623474 LR: 0.0016159001970663667\n",
      "Epoch: 92 Training Loss: 83.36209564208984 Validation loss: 9.709130644798279 LR: 0.0016187032451853156\n",
      "Epoch: 93 Training Loss: 82.02705383300781 Validation loss: 9.70641553401947 LR: 0.0016184099949896336\n",
      "Epoch: 94 Training Loss: 83.83958435058594 Validation loss: 9.715046644210815 LR: 0.0016173217445611954\n",
      "Epoch: 95 Training Loss: 83.99296264648437 Validation loss: 9.71316659450531 LR: 0.0016192621551454067\n",
      "Epoch: 96 Training Loss: 83.00787582397462 Validation loss: 9.71088445186615 LR: 0.0019538840278983116\n",
      "Epoch: 97 Training Loss: 82.15097579956054 Validation loss: 9.694196462631226 LR: 0.001617722911760211\n",
      "Epoch: 98 Training Loss: 82.30884399414063 Validation loss: 9.70395302772522 LR: 0.0016124641988426447\n",
      "Epoch: 99 Training Loss: 84.76363677978516 Validation loss: 9.718127846717834 LR: 0.0016164456028491259\n"
     ]
    }
   ],
   "source": [
    "# LALR, MSE\n",
    "trainLALR(model_LALR_MSE, optimizer_LALR_MSE, criterion2, tau,  N_EPOCHS, lossList_LALR_MSE, valList_LALR_MSE, \"MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training Loss: 44.39178123474121 Validation loss: 32.33310413360596 LR: 0.4528968930244446\n",
      "Epoch: 1 Training Loss: 22.2152458190918 Validation loss: 19.454655170440674 LR: 0.11593209207057953\n",
      "Epoch: 2 Training Loss: 12.12131862640381 Validation loss: 16.36980938911438 LR: 0.1158185750246048\n",
      "Epoch: 3 Training Loss: 11.074598121643067 Validation loss: 13.057222843170166 LR: 0.11568094044923782\n",
      "Epoch: 4 Training Loss: 8.14787769317627 Validation loss: 11.451771259307861 LR: 0.11564849317073822\n",
      "Epoch: 5 Training Loss: 7.393148517608642 Validation loss: 11.277796030044556 LR: 0.11562307924032211\n",
      "Epoch: 6 Training Loss: 7.302708148956299 Validation loss: 11.026631712913513 LR: 0.11558429896831512\n",
      "Epoch: 7 Training Loss: 6.991545581817627 Validation loss: 10.72295618057251 LR: 0.11560175567865372\n",
      "Epoch: 8 Training Loss: 7.115063190460205 Validation loss: 10.191576361656189 LR: 0.11556382477283478\n",
      "Epoch: 9 Training Loss: 6.819791412353515 Validation loss: 10.176325678825378 LR: 0.11556950956583023\n",
      "Epoch: 10 Training Loss: 6.948115634918213 Validation loss: 10.031615495681763 LR: 0.1155758649110794\n",
      "Epoch: 11 Training Loss: 6.963522052764892 Validation loss: 10.106974363327026 LR: 0.11557348072528839\n",
      "Epoch: 12 Training Loss: 6.8428144454956055 Validation loss: 10.0228511095047 LR: 0.1155899167060852\n",
      "Epoch: 13 Training Loss: 6.799813270568848 Validation loss: 10.16542911529541 LR: 0.11556042730808258\n",
      "Epoch: 14 Training Loss: 6.897022724151611 Validation loss: 10.126524090766907 LR: 0.11557076871395111\n",
      "Epoch: 15 Training Loss: 6.812486934661865 Validation loss: 10.340908288955688 LR: 0.11558021605014801\n",
      "Epoch: 16 Training Loss: 6.792847156524658 Validation loss: 10.186498284339905 LR: 0.11556191742420197\n",
      "Epoch: 17 Training Loss: 6.869708919525147 Validation loss: 10.26981008052826 LR: 0.11553990095853806\n",
      "Epoch: 18 Training Loss: 6.979566192626953 Validation loss: 10.08567214012146 LR: 0.11554078012704849\n",
      "Epoch: 19 Training Loss: 6.744171524047852 Validation loss: 10.110865950584412 LR: 0.11554425209760666\n",
      "Epoch: 20 Training Loss: 6.754984951019287 Validation loss: 10.126593351364136 LR: 0.11555339395999908\n",
      "Epoch: 21 Training Loss: 6.730647373199463 Validation loss: 10.123604774475098 LR: 0.11555150896310806\n",
      "Epoch: 22 Training Loss: 6.776508712768555 Validation loss: 10.450922727584839 LR: 0.11554807424545288\n",
      "Epoch: 23 Training Loss: 7.106511211395263 Validation loss: 10.516488909721375 LR: 0.11556443572044373\n",
      "Epoch: 24 Training Loss: 6.700940895080566 Validation loss: 10.04274034500122 LR: 0.11555790156126022\n",
      "Epoch: 25 Training Loss: 6.73367280960083 Validation loss: 10.070976376533508 LR: 0.11555954813957214\n",
      "Epoch: 26 Training Loss: 6.849361801147461 Validation loss: 10.472737789154053 LR: 0.11557762324810028\n",
      "Epoch: 27 Training Loss: 6.929985618591308 Validation loss: 10.2883141040802 LR: 0.11555071920156479\n",
      "Epoch: 28 Training Loss: 6.647390937805175 Validation loss: 9.929477572441101 LR: 0.11556069552898407\n",
      "Epoch: 29 Training Loss: 6.865202140808106 Validation loss: 10.018498659133911 LR: 0.11554929614067078\n",
      "Epoch: 30 Training Loss: 6.850064373016357 Validation loss: 10.067701935768127 LR: 0.11556192487478256\n",
      "Epoch: 31 Training Loss: 6.572028923034668 Validation loss: 9.870957612991333 LR: 0.11554166674613953\n",
      "Epoch: 32 Training Loss: 6.709947776794434 Validation loss: 10.04564368724823 LR: 0.11557290703058243\n",
      "Epoch: 33 Training Loss: 6.696072959899903 Validation loss: 10.285478949546814 LR: 0.1155683696269989\n",
      "Epoch: 34 Training Loss: 6.7705848693847654 Validation loss: 9.992546558380127 LR: 0.11556287109851837\n",
      "Epoch: 35 Training Loss: 6.78554220199585 Validation loss: 10.00638461112976 LR: 0.1155686303973198\n",
      "Epoch: 36 Training Loss: 6.7895652770996096 Validation loss: 10.168256640434265 LR: 0.11555472016334534\n",
      "Epoch: 37 Training Loss: 6.716966247558593 Validation loss: 9.98088002204895 LR: 0.11552722007036209\n",
      "Epoch: 38 Training Loss: 6.744001007080078 Validation loss: 10.10597288608551 LR: 0.11554194986820221\n",
      "Epoch: 39 Training Loss: 6.690207862854004 Validation loss: 10.514585256576538 LR: 0.11554685980081558\n",
      "Epoch: 40 Training Loss: 6.889396381378174 Validation loss: 10.195736646652222 LR: 0.11553740501403809\n",
      "Epoch: 41 Training Loss: 6.8909937858581545 Validation loss: 9.94413959980011 LR: 0.11555010825395584\n",
      "Epoch: 42 Training Loss: 6.838023662567139 Validation loss: 10.162198066711426 LR: 0.11553246527910233\n",
      "Epoch: 43 Training Loss: 6.946329021453858 Validation loss: 10.514304876327515 LR: 0.11553111672401428\n",
      "Epoch: 44 Training Loss: 6.7606199264526365 Validation loss: 9.937160015106201 LR: 0.11552558839321136\n",
      "Epoch: 45 Training Loss: 6.669116592407226 Validation loss: 10.02754282951355 LR: 0.11552555114030838\n",
      "Epoch: 46 Training Loss: 6.610222911834716 Validation loss: 9.945276379585266 LR: 0.11553658545017242\n",
      "Epoch: 47 Training Loss: 6.8220664978027346 Validation loss: 10.104461908340454 LR: 0.11555881053209305\n",
      "Epoch: 48 Training Loss: 6.780013179779052 Validation loss: 10.161320805549622 LR: 0.1155710369348526\n",
      "Epoch: 49 Training Loss: 6.78722562789917 Validation loss: 10.098854303359985 LR: 0.11553877592086792\n",
      "Epoch: 50 Training Loss: 6.861084270477295 Validation loss: 10.154972076416016 LR: 0.11553370952606201\n",
      "Epoch: 51 Training Loss: 6.583490467071533 Validation loss: 9.957319498062134 LR: 0.11554154008626938\n",
      "Epoch: 52 Training Loss: 6.665278053283691 Validation loss: 9.978730320930481 LR: 0.11555833369493484\n",
      "Epoch: 53 Training Loss: 6.567391967773437 Validation loss: 9.769442677497864 LR: 0.11553554236888885\n",
      "Epoch: 54 Training Loss: 6.7190391540527346 Validation loss: 10.103676676750183 LR: 0.11554275453090668\n",
      "Epoch: 55 Training Loss: 6.6606403350830075 Validation loss: 10.000027775764465 LR: 0.11554065346717834\n",
      "Epoch: 56 Training Loss: 6.751672077178955 Validation loss: 10.01688814163208 LR: 0.11553812772035599\n",
      "Epoch: 57 Training Loss: 6.874551486968994 Validation loss: 9.943871974945068 LR: 0.11554594337940216\n",
      "Epoch: 58 Training Loss: 7.040597057342529 Validation loss: 9.92306649684906 LR: 0.11554174870252609\n",
      "Epoch: 59 Training Loss: 6.766096496582032 Validation loss: 10.312047123908997 LR: 0.1155175119638443\n",
      "Epoch: 60 Training Loss: 6.757372665405273 Validation loss: 10.06568706035614 LR: 0.11552231758832932\n",
      "Epoch: 61 Training Loss: 6.653595733642578 Validation loss: 10.094337701797485 LR: 0.11553514748811722\n",
      "Epoch: 62 Training Loss: 6.846739387512207 Validation loss: 10.011422872543335 LR: 0.1155516728758812\n",
      "Epoch: 63 Training Loss: 6.904367923736572 Validation loss: 10.052676439285278 LR: 0.11554069817066193\n",
      "Epoch: 64 Training Loss: 6.680256366729736 Validation loss: 10.034703373908997 LR: 0.11553329229354858\n",
      "Epoch: 65 Training Loss: 6.583039569854736 Validation loss: 9.876307249069214 LR: 0.11553207039833069\n",
      "Epoch: 66 Training Loss: 6.7130731582641605 Validation loss: 10.275082111358643 LR: 0.1155262291431427\n",
      "Epoch: 67 Training Loss: 7.109259128570557 Validation loss: 10.288249135017395 LR: 0.1155405342578888\n",
      "Epoch: 68 Training Loss: 6.9632903099060055 Validation loss: 10.203651547431946 LR: 0.1155436709523201\n",
      "Epoch: 69 Training Loss: 6.7145462989807125 Validation loss: 9.809666514396667 LR: 0.1155177652835846\n",
      "Epoch: 70 Training Loss: 6.850988864898682 Validation loss: 10.50310206413269 LR: 0.11552289128303528\n",
      "Epoch: 71 Training Loss: 6.936756992340088 Validation loss: 10.718725085258484 LR: 0.11553401499986649\n",
      "Epoch: 72 Training Loss: 7.155563831329346 Validation loss: 10.221386909484863 LR: 0.11552644520998001\n",
      "Epoch: 73 Training Loss: 7.114020156860351 Validation loss: 10.213772535324097 LR: 0.11553259193897247\n",
      "Epoch: 74 Training Loss: 6.925765705108643 Validation loss: 10.067055225372314 LR: 0.1155356615781784\n",
      "Epoch: 75 Training Loss: 6.764662361145019 Validation loss: 10.328728079795837 LR: 0.11552274972200394\n",
      "Epoch: 76 Training Loss: 7.064828300476075 Validation loss: 10.727853775024414 LR: 0.11553501337766647\n",
      "Epoch: 77 Training Loss: 7.33310546875 Validation loss: 11.740835428237915 LR: 0.11553727835416794\n",
      "Epoch: 78 Training Loss: 7.689965534210205 Validation loss: 10.94057309627533 LR: 0.11553505808115005\n",
      "Epoch: 79 Training Loss: 6.94543685913086 Validation loss: 10.930444240570068 LR: 0.11553667485713959\n",
      "Epoch: 80 Training Loss: 6.774472045898437 Validation loss: 9.960744380950928 LR: 0.11556898057460785\n",
      "Epoch: 81 Training Loss: 6.596753406524658 Validation loss: 9.949446201324463 LR: 0.11554218083620071\n",
      "Epoch: 82 Training Loss: 6.773103332519531 Validation loss: 10.284358382225037 LR: 0.11556017398834229\n",
      "Epoch: 83 Training Loss: 6.8069250106811525 Validation loss: 10.101469039916992 LR: 0.11556345969438553\n",
      "Epoch: 84 Training Loss: 6.86868839263916 Validation loss: 9.947083950042725 LR: 0.11555030196905136\n",
      "Epoch: 85 Training Loss: 7.070468521118164 Validation loss: 10.009043097496033 LR: 0.11554590612649918\n",
      "Epoch: 86 Training Loss: 6.8210978507995605 Validation loss: 10.361923456192017 LR: 0.11555854976177216\n",
      "Epoch: 87 Training Loss: 6.76038122177124 Validation loss: 10.444024801254272 LR: 0.11558588594198227\n",
      "Epoch: 88 Training Loss: 6.792303848266601 Validation loss: 10.151788115501404 LR: 0.11556500941514969\n",
      "Epoch: 89 Training Loss: 6.787831592559814 Validation loss: 10.05507743358612 LR: 0.11557599157094955\n",
      "Epoch: 90 Training Loss: 6.913935565948487 Validation loss: 10.540248274803162 LR: 0.1155562624335289\n",
      "Epoch: 91 Training Loss: 6.801990699768067 Validation loss: 10.487359762191772 LR: 0.11554310470819473\n",
      "Epoch: 92 Training Loss: 6.797969818115234 Validation loss: 10.635691046714783 LR: 0.11556414514780045\n",
      "Epoch: 93 Training Loss: 6.968052005767822 Validation loss: 9.949178099632263 LR: 0.11555220186710358\n",
      "Epoch: 94 Training Loss: 6.752036190032959 Validation loss: 9.984238147735596 LR: 0.11556929349899292\n",
      "Epoch: 95 Training Loss: 6.631590557098389 Validation loss: 9.996645331382751 LR: 0.11556637287139893\n",
      "Epoch: 96 Training Loss: 6.957976150512695 Validation loss: 10.262246370315552 LR: 0.11556590348482132\n",
      "Epoch: 97 Training Loss: 6.895978832244873 Validation loss: 10.190341234207153 LR: 0.11556372046470642\n",
      "Epoch: 98 Training Loss: 7.12374906539917 Validation loss: 10.236156225204468 LR: 0.11556479334831238\n",
      "Epoch: 99 Training Loss: 7.248931503295898 Validation loss: 10.827253818511963 LR: 0.11554926633834839\n"
     ]
    }
   ],
   "source": [
    "# LALR, MAE\n",
    "trainLALR(model_LALR_L1, optimizer_LALR_L1, criterion3, tau,  N_EPOCHS, lossList_LALR_L1, valList_LALR_L1, \"L1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f0406b12b20>"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.plot(range(100), valList_LBFGS_LC, label=\"Log-Cosh-LBFGS\")\n",
    "# plt.plot(range(100), valList_LBFGS_MSE, label=\"MSE-LBFGS\")\n",
    "# plt.plot(range(100), valList_CLR_MSE, label=\"CLR-MSE_adam\")\n",
    "# plt.plot(range(100), valList_LALR_MSE, label=\"LALR-MSE_adam\")\n",
    "# plt.plot(range(100), valList_CLR_L1, label=\"CLR-L1_adam\")\n",
    "# plt.plot(range(100), valList_LALR_L1, label=\"LALR-L1_adam\")\n",
    "# plt.plot(range(100), valList_LALR_LC, label=\"LALR-LC_adam\")\n",
    "# plt.plot(range(100), valList_CLR_LC, label=\"CLR-LC_adam\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.45429801940918"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valList_CLR_LC[-1]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "argv": [
    "/home/aryamanj/miniconda3/bin/python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (ipykernel)",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": {
    "debugger": true
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "name": "UCIregression.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
