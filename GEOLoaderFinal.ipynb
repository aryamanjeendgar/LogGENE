{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d217578b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34664ec1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GEOdataLoader(path, bSize= 64):\n",
    "    \"\"\"\n",
    "    Specify the path to the parent folder containing the .npy files\n",
    "    \"\"\"\n",
    "    X_tr= np.load(os.path.join(path, \"X_tr.npy\"))\n",
    "    Y_tr= np.load(os.path.join(path, \"Y_tr.npy\"))\n",
    "    X_va= np.load(os.path.join(path, \"X_va.npy\"))\n",
    "    Y_va= np.load(os.path.join(path, \"Y_va.npy\"))\n",
    "\n",
    "    test_data= torch.utils.data.TensorDataset(torch.from_numpy(X_tr).float(), torch.from_numpy(Y_tr).float())\n",
    "    val_data= torch.utils.data.TensorDataset(torch.from_numpy(X_va).float(), torch.from_numpy(Y_va).float())\n",
    "    trainLoader= torch.utils.data.DataLoader(test_data, batch_size=bSize, shuffle=True) \n",
    "    valLoader= torch.utils.data.DataLoader(val_data, batch_size=bSize, shuffle=True) \n",
    "    return (trainLoader, valLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377c5f6a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# defining all the criterions to be used in the following experiments:\n",
    "def tiltedLC(x, y, tau, h):\n",
    "    e= y-x # errors\n",
    "    ind= (torch.sign(e)+1)/2 # the division in the log-cosh is only about the origin\n",
    "    quantFactor= (1-tau)*(1-ind) + tau*ind\n",
    "    loss= quantFactor*torch.log(torch.cosh(e))\n",
    "    loss= torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "class TiltedLC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TiltedLC, self).__init__()\n",
    "    def forward(self, x, y, tau, h):\n",
    "        return tiltedLC(x, y, tau, h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc75bf3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# global initialisations:\n",
    "h= 0.4 # smoothing parameter for the log-cosh \n",
    "tau= 0.5\n",
    "device= ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "trainLoader, valLoader= GEOdataLoader(\"/home/aryamanj/Downloads/LGdata\")\n",
    "criterion1= TiltedLC()\n",
    "criterion2= nn.L1Loss()\n",
    "criterion3= nn.MSELoss()\n",
    "N_EPOCHS= 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b54e42",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A new network class for LALR training, that supports returning penultimate activations\n",
    "class LALRnetwork(nn.Module):\n",
    "    def __init__(self, size1, size2, drop):\n",
    "        super(LALRnetwork, self).__init__()\n",
    "        self.l1= nn.Linear(943, size1)\n",
    "        self.l2= nn.Dropout(p= drop)\n",
    "        self.l3= nn.Linear(size1, size2)\n",
    "        self.l4= nn.Dropout(p= drop)\n",
    "        self.l5= nn.Linear(size2, 4760)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x= F.tanh(self.l1(x))\n",
    "        x= F.tanh(self.l3(self.l2(x)))\n",
    "        x= self.l5(self.l4(x))\n",
    "        return x\n",
    "    \n",
    "    def penU(self, x):\n",
    "        x= F.tanh(self.l2(self.l1(x)))\n",
    "        x= F.tanh(self.l4(self.l3(x)))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b569f8a8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# instantiating objects for all constantLR tests:\n",
    "\n",
    "size1,size2= 300,300\n",
    "# LC initialisations\n",
    "model_CLR_LC= LALRnetwork(size1, size2, 0.1).to(device)\n",
    "optimizer_CLR_LC= optim.Adam(model_CLR_S.parameters(), lr= 0.1)\n",
    "lossList_CLR_LC= []\n",
    "valList_CLR_LC= []\n",
    "model_LALR_LC= LALRnetwork(size1, size2, 0.1).to(device)\n",
    "optimizer_LALR_LC= optim.Adam(model_CLR_S.parameters(), lr= 0.1)\n",
    "lossList_LALR_LC= []\n",
    "valList_LALR_LC= []\n",
    "model_LBFGS_LC= LALRnetwork(size1, size2, 0.1).to(device)\n",
    "optimizer_LBFGS_LC= optim.Adam(model_CLR_S.parameters(), lr= 0.1)\n",
    "lossList_LBFGS_LC= []\n",
    "valList_LBFGS_LC= []\n",
    "\n",
    "# L1 initialisations\n",
    "model_CLR_L1= LALRnetwork(size1, size2, 0.1).to(device)\n",
    "optimizer_CLR_L1= optim.Adam(model_CLR_S.parameters(), lr= 0.1)\n",
    "lossList_CLR_L1= []\n",
    "valList_CLR_L1= []\n",
    "model_LALR_L1= LALRnetwork(size1, size2, 0.1).to(device)\n",
    "optimizer_LALR_L1= optim.Adam(model_CLR_S.parameters(), lr= 0.1)\n",
    "lossList_LALR_L1= []\n",
    "valList_LALR_L1= []\n",
    "model_LBFGS_L1= LALRnetwork(size1, size2, 0.1).to(device)\n",
    "optimizer_LBFGS_L1= optim.Adam(model_CLR_S.parameters(), lr= 0.1)\n",
    "lossList_LBFGS_L1= []\n",
    "valList_LBFGS_L1= []\n",
    "\n",
    "# MSE initialisations:\n",
    "model_CLR_MSE= LALRnetwork(size1, size2, 0.1).to(device)\n",
    "optimizer_CLR_MSE= optim.Adam(model_CLR_S.parameters(), lr= 0.1)\n",
    "lossList_CLR_MSE= []\n",
    "valList_CLR_MSE= []\n",
    "model_LALR_MSE= LALRnetwork(size1, size2, 0.1).to(device)\n",
    "optimizer_LALR_MSE= optim.Adam(model_CLR_S.parameters(), lr= 0.1)\n",
    "lossList_LALR_MSE= []\n",
    "valList_LALR_MSE= []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8d4ec7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training loops:\n",
    "def trainConstantLR(model, optimizer, criterion, tau, epochs, ls_list, valList, loss_name):\n",
    "    \"\"\"\n",
    "    Training loop used for constantLR\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss= 0.0\n",
    "        # training loop\n",
    "        model.train()\n",
    "        for inputs, labels in trainLoader: \n",
    "            inputs= inputs.to(device) \n",
    "            labels= labels.to(device)\n",
    "            optimizer.zero_grad() \n",
    "            outputs= model(inputs) \n",
    "            if loss_name == \"LC\":\n",
    "                loss= criterion(outputs, labels, tau, h) \n",
    "            else:\n",
    "                loss= criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "            epoch_loss+= loss.item()\n",
    "        ls_list.append(epoch_loss/len(trainLoader))\n",
    "\n",
    "        # validation loop\n",
    "        val_loss= 0.0\n",
    "        model.eval()\n",
    "        for inputs, labels in valLoader:\n",
    "            inputs= inputs.to(device)\n",
    "            labels= labels.to(device)\n",
    "            outputs= model(inputs)\n",
    "            if loss_name == \"LC\":\n",
    "                loss= criterion(outputs, labels, tau, h) \n",
    "            else:\n",
    "                loss= criterion(outputs, labels)\n",
    "            val_loss+= loss.item()\n",
    "        valList.append(val_loss/len(valLoader))\n",
    "        print(\"Epoch: {} Training loss: {} Validation loss: {}\".format(epoch, epoch_loss/len(trainLoader), val_loss/len(valLoader)))\n",
    "\n",
    "def trainLBFGS(model, optimizer, criterion, tau, epochs, ls_list, valList, loss_name):\n",
    "    \"\"\"\n",
    "    Training loop used for LBFGS and conjugate gradient training\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss= 0.0\n",
    "        # training loop\n",
    "        model.train()\n",
    "        for inputs, labels in trainLoader: \n",
    "            inputs= inputs.to(device) \n",
    "            labels= labels.to(device)\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                outputs= model(inputs)\n",
    "                if loss_name== \"MSE\":\n",
    "                    loss= criterion(outputs, labels)\n",
    "                else:\n",
    "                    loss= criterion(outputs, labels, tau, h)\n",
    "                loss.backward()\n",
    "                return loss\n",
    "            optimizer.step(closure) \n",
    "        # ls_list.append(epoch_loss/len(trainLoader))\n",
    "\n",
    "        # validation loop\n",
    "        val_loss= 0.0\n",
    "        model.eval()\n",
    "        for inputs, labels in testLoader:\n",
    "            inputs= inputs.to(device)\n",
    "            labels= labels.to(device)\n",
    "            outputs= model(inputs)\n",
    "            loss= torch.sqrt(criterion2(outputs, labels))\n",
    "            val_loss+= loss.item()\n",
    "        valList.append(val_loss/len(testLoader))\n",
    "        print(\"Epoch: {} Training loss: {} Validation loss: {}\".format(epoch, epoch_loss/len(trainLoader), val_loss/len(testLoader)))\n",
    "\n",
    "def trainLALR(model,optimizer, criterion,  tau, epochs, ls_list, valList, loss_name):\n",
    "    \"\"\"\n",
    "    Training loop used for LALR training\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss= 0.0\n",
    "        lr_val= computeLR(model, ls, bSize=16)\n",
    "        optimizer.param_groups[0]['lr']= lr_val\n",
    "        # training loop\n",
    "        model.train()\n",
    "        for inputs, labels in trainLoader: \n",
    "            inputs= inputs.to(device) \n",
    "            labels= labels.to(device)\n",
    "            optimizer.zero_grad() \n",
    "            outputs= model(inputs) \n",
    "            if loss_name == \"LC\":\n",
    "                loss= criterion(outputs, labels, tau, h) \n",
    "            else:\n",
    "                loss= criterion(outputs, labels)\n",
    "            loss.backward() \n",
    "            optimizer.step() \n",
    "            epoch_loss+= loss.item()\n",
    "        ls_list.append(epoch_loss/len(trainLoader))\n",
    "\n",
    "        # validation loop\n",
    "        val_loss= 0.0\n",
    "        model.eval()\n",
    "        for inputs, labels in valLoader:\n",
    "            inputs= inputs.to(device)\n",
    "            labels= labels.to(device)\n",
    "            outputs= model(inputs)\n",
    "            if loss_name == \"LC\":\n",
    "                loss= criterion(outputs, labels, tau, h) \n",
    "            else:\n",
    "                loss= criterion(outputs, labels)\n",
    "            val_loss+= loss.item()\n",
    "        valList.append(val_loss/len(valLoader))\n",
    "        print(\"Epoch: {} Training Loss: {} Validation loss: {} LR: {}\".format(epoch, epoch_loss/len(trainLoader), val_loss/len(valLoader), optimizer.param_groups[0]['lr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c284b760",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Learning rate computation functions:\n",
    "def computeKa(x):\n",
    "    maxNorm= 0.0\n",
    "    for vector in x:\n",
    "        if (maxNorm < torch.linalg.vector_norm(vector)):\n",
    "            maxNorm= torch.linalg.vector_norm(vector)\n",
    "    return maxNorm\n",
    "\n",
    "def computeLR(model, ls, bSize= 16):\n",
    "    \"\"\"\n",
    "    Takes in a network of the LALRnetwork class(during some arbitrary EPOCH of training) and the current input, and returns Kz for the EPOCH\n",
    "    \"\"\"\n",
    "    Kz = 0.0\n",
    "    Ka= 0.0\n",
    "    Y= 0.0\n",
    "    z_k= 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i,j in enumerate(trainLoader):\n",
    "            inputs,labels= j[0],j[1]\n",
    "            inputs= inputs.to(device)\n",
    "            labels= labels.to(device)\n",
    "            op1= model.penU(inputs)\n",
    "            op2= model(inputs)\n",
    "            # first taking the max and min for each batch\n",
    "            activ1, arg1= torch.max(op1, dim= 1)\n",
    "            activ2, arg2= torch.min(op2, dim= 1)\n",
    "            # now, we take the max and min across batches\n",
    "            val1, indx1= torch.max(activ1, dim= 0)\n",
    "            val2, indx2= torch.min(activ2, dim= 0)\n",
    "            val3= computeKa(op2)\n",
    "            val4= computeKa(labels)\n",
    "            # print(indx, i)\n",
    "            if val1 > Kz:\n",
    "                # in the case of K_z, we do not need the index where the max occurs, hence only deal with the value\n",
    "                Kz= val1 \n",
    "            z_k= val2\n",
    "            if val3 > Ka:\n",
    "                Ka= val3\n",
    "            if val3 > Y:\n",
    "                Y= val4 \n",
    "            argMin= arg2[indx2]\n",
    "\n",
    "    LR= 1\n",
    "    if ls == \"LC\":\n",
    "        LR= (1/bSize)*torch.tanh(-op2[int(indx2)][int(argMin)])*Kz\n",
    "    elif ls == \"L1\":\n",
    "        LR= Kz/bSize\n",
    "    elif ls == \"MSE\":\n",
    "        LR= (1/bSize)*(Ka+Y)*Kz\n",
    "\n",
    "    if LR==0:\n",
    "        return 0.1\n",
    "    return 1/LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbd26e5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ConstantLR training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c4251f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainConstantLR(model_CLR_LC, optimizer_CLR_LC, criterion1, tau, N_EPOCHS, lossList_CLR_LC, valList_CLR_LC, \"LC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495b2465",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "In [ ]:\n",
    "trainConstantLR(model_CLR_L1, optimizer_CLR_L1, criterion2, tau, N_EPOCHS, lossList_CLR_L1, valList_CLR_L1, \"L1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9abde2f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainConstantLR(model_CLR_MSE, optimizer_CLR_MSE, criterion3, tau, N_EPOCHS, lossList_CLR_MSE, valList_CLR_MSE, \"MSE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a801908",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff5d87b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LALR training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7917cc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainLALR(model_LALR_LC, optimizer_LALR_LC, criterion1, tau, N_EPOCHS, lossList_LALR_LC, valList_LALR_LC, \"LC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bc20c9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainLALR(model_LALR_L1, optimizer_LALR_L1, criterion2, tau, N_EPOCHS, lossList_LALR_L1, valList_LALR_L1, \"L1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea710e1c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainLALR(model_LALR_MSE, optimizer_LALR_MSE, criterion3, tau, N_EPOCHS, lossList_LALR_MSE, valList_LALR_MSE, \"MSE\")In [ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93004e0f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LBFGS training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d08696a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainLBFGS(model_LBFGS_LC, optimizer_LBFGS_LC, criterion1, tau, N_EPOCHS, lossList_LBFGS_LC, valList_LBFGS_LC, \"LC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4762e8b7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainLBFGS(model_LBFGS_L1, optimizer_LBFGS_L1, criterion2, tau, N_EPOCHS, lossList_LBFGS_L1, valList_LBFGS_L1, \"L1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b945a1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainLBFGS(model_LBFGS_MSE, optimizer_LBFGS_MSE, criterion3, tau, N_EPOCHS, lossList_LBFGS_MSE, valList_LBFGS_MSE, \"MSE\")In [ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730eedcf",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Saving losses and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3ce18c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A= np.asarray(lossList_CLR_LC)\n",
    "B= np.asarray(lossList_LALR_LC)\n",
    "C= np.asarray(lossList_LBFGS_LC)\n",
    "\n",
    "A_= np.asarray(lossList_CLR_L1)\n",
    "B_= np.asarray(lossList_LALR_L1)\n",
    "C_= np.asarray(lossList_LBFGS_L1)\n",
    "\n",
    "A__= np.asarray(lossList_CLR_MSE)\n",
    "B__= np.asarray(lossList_LALR_MSE)\n",
    "C__= np.asarray(lossList_LBFGS_MSE)\n",
    "\n",
    "np.save(\"./D-GEX_checkpoint/LossLists/lossList_CLR_LC.npy\", A)\n",
    "np.save(\"./D-GEX_checkpoint/LossLists/lossList_LALR_LC.npy\", B)\n",
    "np.save(\"./D-GEX_checkpoint/LossLists/lossList_LBFGS_LC.npy\", C)\n",
    "\n",
    "\n",
    "np.save(\"./D-GEX_checkpoint/LossLists/lossList_CLR_L1.npy\", A_)\n",
    "np.save(\"./D-GEX_checkpoint/LossLists/lossList_LALR_L1.npy\", B_)\n",
    "np.save(\"./D-GEX_checkpoint/LossLists/lossList_LBFGS_L1.npy\", C_)\n",
    "\n",
    "np.save(\"./D-GEX_checkpoint/LossLists/lossList_CLR_MSE.npy\", A__)\n",
    "np.save(\"./D-GEX_checkpoint/LossLists/lossList_LALR_MSE.npy\", B__)\n",
    "np.save(\"./D-GEX_checkpoint/LossLists/lossList_LBFGS_MSE.npy\", C__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37831db9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A= np.asarray(valList_CLR_LC)\n",
    "B= np.asarray(valList_LALR_LC)\n",
    "C= np.asarray(valList_LBFGS_LC)\n",
    "\n",
    "A_= np.asarray(valList_CLR_L1)\n",
    "B_= np.asarray(valList_LALR_L1)\n",
    "C_= np.asarray(valList_LBFGS_L1)\n",
    "\n",
    "A__= np.asarray(valList_CLR_MSE)\n",
    "B__= np.asarray(valList_LALR_MSE)\n",
    "C__= np.asarray(valList_LBFGS_MSE)\n",
    "\n",
    "np.save(\"./D-GEX_checkpoint/ValLists/valList_CLR_LC.npy\", A)\n",
    "np.save(\"./D-GEX_checkpoint/ValLists/valList_LALR_LC.npy\", B)\n",
    "np.save(\"./D-GEX_checkpoint/ValLists/valList_LBFGS_LC.npy\", C)\n",
    "\n",
    "\n",
    "np.save(\"./D-GEX_checkpoint/ValLists/valList_CLR_L1.npy\", A_)\n",
    "np.save(\"./D-GEX_checkpoint/ValLists/valList_LALR_L1.npy\", B_)\n",
    "np.save(\"./D-GEX_checkpoint/ValLists/valList_LBFGS_L1.npy\", C_)\n",
    "\n",
    "np.save(\"./D-GEX_checkpoint/ValLists/valList_CLR_MSE.npy\", A__)\n",
    "np.save(\"./D-GEX_checkpoint/ValLists/valList_LALR_MSE.npy\", B__)\n",
    "np.save(\"./D-GEX_checkpoint/ValLists/valList_LBFGS_MSE.npy\", C__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6be2c09",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(model_CLR_LC.state_dict(), \"./D-GEX_checkpoint/model_params/CLR_LC.pt\")\n",
    "torch.save(model_LALR_LC.state_dict(), \"./D-GEX_checkpoint/model_params/LALR_LC.pt\")\n",
    "torch.save(model_LBFGS_LC.state_dict(), \"./D-GEX_checkpoint/model_params/LBFGS_LC.pt\")\n",
    "\n",
    "torch.save(model_CLR_L1.state_dict(), \"./D-GEX_checkpoint/model_params/CLR_L1.pt\")\n",
    "torch.save(model_LALR_L1.state_dict(), \"./D-GEX_checkpoint/model_params/LALR_L1.pt\")\n",
    "torch.save(model_LBFGS_L1.state_dict(), \"./D-GEX_checkpoint/model_params/LBFGS_L1.pt\")\n",
    "\n",
    "torch.save(model_CLR_MSE.state_dict(), \"./D-GEX_checkpoint/model_params/CLR_MSE.pt\")\n",
    "torch.save(model_LALR_MSE.state_dict(), \"./D-GEX_checkpoint/model_params/LALR_MSE.pt\")\n",
    "torch.save(model_LBFGS_MSE.state_dict(), \"./D-GEX_checkpoint/model_params/LBFGS_MSE.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/home/aryamanj/miniconda3/bin/python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (ipykernel)",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": {
    "debugger": true
   },
   "name": "python3"
  },
  "name": "GEOLoaderFinal.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
