{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from adahessian import Adahessian, get_params_grad\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from UCI_loader import UCIDatasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataClass= UCIDatasets(\"energy\", data_path=\"\", n_splits=10) # has a field named 'data' that contains the data\n",
    "\n",
    "trainData= dataClass.get_split(train=True)\n",
    "trainLoader= torch.utils.data.DataLoader(trainData, batch_size= dataClass.data.shape[0], shuffle= True) # creating a loader with full batch size to ensure LBFGS works\n",
    "testData= dataClass.get_split(train= False)\n",
    "testLoader= torch.utils.data.DataLoader(testData, batch_size= dataClass.data.shape[0], shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inDim, outDim= 8, 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# defining all the criterions to be used in the following experiments:\n",
    "def tiltedLC(x, y, tau, h):\n",
    "    e= y-x # errors\n",
    "    ind= (torch.sign(e)+1)/2 # the division in the log-cosh is only about the origin\n",
    "    quantFactor= (1-tau)*(1-ind) + tau*ind\n",
    "    loss= quantFactor*torch.log(torch.cosh(e))\n",
    "    loss= torch.mean(loss)\n",
    "    return loss\n",
    "\n",
    "class TiltedLC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TiltedLC, self).__init__()\n",
    "    def forward(self, x, y, tau, h):\n",
    "        return tiltedLC(x, y, tau, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# global initialisations:\n",
    "h= 0.4 # smoothing parameter for the log-cosh \n",
    "tau= 0.5\n",
    "device= ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion1= TiltedLC()\n",
    "criterion2= nn.MSELoss()\n",
    "criterion3= nn.L1Loss()\n",
    "N_EPOCHS= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LALRnetwork(nn.Module):\n",
    "    def __init__(self, size1, size2, inDim, outDim, drop):\n",
    "        super(LALRnetwork, self).__init__()\n",
    "        self.l1= nn.Linear(inDim, size1)\n",
    "        self.l2= nn.Dropout(p= drop)\n",
    "        self.l3= nn.Linear(size1, size2)\n",
    "        self.l4= nn.Dropout(p= drop)\n",
    "        self.l5= nn.Linear(size2, outDim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x= F.tanh(self.l1(x))\n",
    "        x= F.tanh(self.l3(self.l2(x)))\n",
    "        x= self.l5(self.l4(x))\n",
    "        return x\n",
    "    \n",
    "    def penU(self, x):\n",
    "        x= F.tanh(self.l2(self.l1(x)))\n",
    "        x= F.tanh(self.l4(self.l3(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "size1, size2= 300, 300\n",
    "model_LBFGS_LC= LALRnetwork(size1, size2, inDim, outDim, 0.1).to(device)\n",
    "optimizer_LBFGS_LC= optim.LBFGS(model_LBFGS_LC.parameters())\n",
    "lossList_LBFGS_LC= []\n",
    "valList_LBFGS_LC= []\n",
    "\n",
    "model_LBFGS_MSE= LALRnetwork(size1, size2, inDim, outDim, 0.1).to(device)\n",
    "optimizer_LBFGS_MSE= optim.LBFGS(model_LBFGS_MSE.parameters())\n",
    "lossList_LBFGS_MSE= []\n",
    "valList_LBFGS_MSE= []\n",
    "\n",
    "model_CLR_L1= LALRnetwork(size1, size2, inDim, outDim, 0.1).to(device)\n",
    "optimizer_CLR_L1= optim.Adam(model_CLR_L1.parameters(), lr= 0.1)\n",
    "lossList_CLR_L1= []\n",
    "valList_CLR_L1= []\n",
    "\n",
    "model_CLR_LC= LALRnetwork(size1, size2, inDim, outDim, 0.1).to(device)\n",
    "optimizer_CLR_LC= optim.Adam(model_CLR_LC.parameters(), lr= 0.1)\n",
    "lossList_CLR_LC= []\n",
    "valList_CLR_LC= []\n",
    "\n",
    "model_CLR_MSE= LALRnetwork(size1, size2, inDim, outDim, 0.1).to(device)\n",
    "optimizer_CLR_MSE= optim.Adam(model_CLR_MSE.parameters(), lr= 0.1)\n",
    "lossList_CLR_MSE= []\n",
    "valList_CLR_MSE= []\n",
    "\n",
    "model_LALR_L1= LALRnetwork(size1, size2, inDim, outDim, 0.1).to(device)\n",
    "optimizer_LALR_L1= optim.Adam(model_LALR_L1.parameters(), lr= 0.1)\n",
    "lossList_LALR_L1= []\n",
    "valList_LALR_L1= []\n",
    "\n",
    "model_LALR_LC= LALRnetwork(size1, size2, inDim, outDim, 0.1).to(device)\n",
    "optimizer_LALR_LC= optim.Adam(model_LALR_LC.parameters(), lr= 0.1)\n",
    "lossList_LALR_LC= []\n",
    "valList_LALR_LC= []\n",
    "\n",
    "model_LALR_MSE= LALRnetwork(size1, size2, inDim, outDim, 0.1).to(device)\n",
    "optimizer_LALR_MSE= optim.Adam(model_LALR_MSE.parameters(), lr= 0.1)\n",
    "lossList_LALR_MSE= []\n",
    "valList_LALR_MSE= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trainLBFGS(model, optimizer, criterion, tau, epochs, ls_list, valList, loss_name):\n",
    "    \"\"\"\n",
    "    Training loop used for LBFGS and conjugate gradient training\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss= 0.0\n",
    "        # training loop\n",
    "        model.train()\n",
    "        for inputs, labels in trainLoader: \n",
    "            inputs= inputs.to(device) \n",
    "            labels= labels.to(device)\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                outputs= model(inputs)\n",
    "                if loss_name== \"MSE\":\n",
    "                    loss= criterion(outputs, labels)\n",
    "                else:\n",
    "                    loss= criterion(outputs, labels, tau, h)\n",
    "                loss.backward()\n",
    "                return loss\n",
    "            optimizer.step(closure)\n",
    "        # ls_list.append(epoch_loss/len(trainLoader))\n",
    "\n",
    "        # validation loop\n",
    "        val_loss= 0.0\n",
    "        model.eval()\n",
    "        for inputs, labels in testLoader:\n",
    "            inputs= inputs.to(device)\n",
    "            labels= labels.to(device)\n",
    "            outputs= model(inputs)\n",
    "            loss= torch.sqrt(criterion2(outputs, labels))\n",
    "            val_loss+= loss.item()\n",
    "        valList.append(val_loss/len(testLoader))\n",
    "        print(\"Epoch: {} Training loss: {} Validation loss: {}\".format(epoch, epoch_loss/len(trainLoader), val_loss/len(testLoader)))\n",
    "        \n",
    "def trainConstantLR(model, optimizer, criterion, tau, epochs, ls_list, valList, loss_name):\n",
    "    \"\"\"\n",
    "    Training loop used for constantLR\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss= 0.0\n",
    "        # training loop\n",
    "        model.train()\n",
    "        for inputs, labels in trainLoader: \n",
    "            inputs= inputs.to(device) \n",
    "            labels= labels.to(device)\n",
    "            optimizer.zero_grad() \n",
    "            outputs= model(inputs) \n",
    "            if loss_name== \"LC\":\n",
    "                loss= criterion(outputs, labels, tau, h)\n",
    "            else:\n",
    "                loss= criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "            epoch_loss+= loss.item()\n",
    "        ls_list.append(epoch_loss/len(trainLoader))\n",
    "\n",
    "        # validation loop\n",
    "        val_loss= 0.0\n",
    "        model.eval()\n",
    "        for inputs, labels in testLoader:\n",
    "            inputs= inputs.to(device)\n",
    "            labels= labels.to(device)\n",
    "            outputs= model(inputs)\n",
    "            loss= torch.sqrt(criterion2(outputs, labels))\n",
    "            val_loss+= loss.item()\n",
    "        valList.append(val_loss/len(testLoader))\n",
    "        print(\"Epoch: {} Training loss: {} Validation loss: {}\".format(epoch, epoch_loss/len(trainLoader), val_loss/len(testLoader)))\n",
    "\n",
    "def trainLALR(model,optimizer, criterion,  tau, epochs, ls_list, valList, loss_name):\n",
    "    \"\"\"\n",
    "    Training loop used for LALR training\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss= 0.0\n",
    "        lr_val= computeLR(model, loss_name, bSize=16)\n",
    "        optimizer.param_groups[0]['lr']= lr_val\n",
    "        # training loop\n",
    "        model.train()\n",
    "        for inputs, labels in trainLoader:\n",
    "            inputs= inputs.to(device)\n",
    "            labels= labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs= model(inputs)\n",
    "            if loss_name == \"LC\":\n",
    "                loss= criterion(outputs, labels, tau, h)\n",
    "            else:\n",
    "                loss= criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss+= loss.item()\n",
    "        ls_list.append(epoch_loss/len(trainLoader))\n",
    "\n",
    "        # validation loop\n",
    "        val_loss= 0.0\n",
    "        model.eval()\n",
    "        for inputs, labels in valLoader:\n",
    "            inputs= inputs.to(device)\n",
    "            labels= labels.to(device)\n",
    "            outputs= model(inputs)\n",
    "            if loss_name == \"LC\":\n",
    "                loss= criterion(outputs, labels, tau, h)\n",
    "            else:\n",
    "                loss= criterion(outputs, labels)\n",
    "            val_loss+= loss.item()\n",
    "        valList.append(val_loss/len(valLoader))\n",
    "        print(\"Epoch: {} Training Loss: {} Validation loss: {} LR: {}\".format(epoch, epoch_loss/len(trainLoader), val_loss/len(valLoader), optimizer.param_groups[0]['lr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Learning rate computation functions:\n",
    "def computeKa(x):\n",
    "    maxNorm= 0.0\n",
    "    for vector in x:\n",
    "        if (maxNorm < torch.linalg.vector_norm(vector)):\n",
    "            maxNorm= torch.linalg.vector_norm(vector)\n",
    "    return maxNorm\n",
    "\n",
    "def computeLR(model, ls, bSize= 16):\n",
    "    \"\"\"\n",
    "    Takes in a network of the LALRnetwork class(during some arbitrary EPOCH of training) and the current input, and returns Kz for the EPOCH\n",
    "    \"\"\"\n",
    "    Kz = 0.0\n",
    "    Ka= 0.0\n",
    "    Y= 0.0\n",
    "    z_k= 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i,j in enumerate(trainLoader):\n",
    "            inputs,labels= j[0],j[1]\n",
    "            inputs= inputs.to(device)\n",
    "            labels= labels.to(device)\n",
    "            op1= model.penU(inputs)\n",
    "            op2= model(inputs)\n",
    "            # first taking the max and min for each batch\n",
    "            activ1, arg1= torch.max(op1, dim= 1)\n",
    "            activ2, arg2= torch.min(op2, dim= 1)\n",
    "            # now, we take the max and min across batches\n",
    "            val1, indx1= torch.max(activ1, dim= 0)\n",
    "            val2, indx2= torch.min(activ2, dim= 0)\n",
    "            val3= computeKa(op2)\n",
    "            val4= computeKa(labels)\n",
    "            # print(indx, i)\n",
    "            if val1 > Kz:\n",
    "                # in the case of K_z, we do not need the index where the max occurs, hence only deal with the value\n",
    "                Kz= val1 \n",
    "            z_k= val2\n",
    "            if val3 > Ka:\n",
    "                Ka= val3\n",
    "            if val3 > Y:\n",
    "                Y= val4 \n",
    "            argMin= arg2[indx2]\n",
    "\n",
    "    LR= 1\n",
    "    if ls == \"LC\":\n",
    "        LR= (1/bSize)*torch.tanh(-op2[int(indx2)][int(argMin)])*Kz\n",
    "    elif ls == \"L1\":\n",
    "        LR= Kz/bSize\n",
    "    elif ls == \"MSE\":\n",
    "        LR= (1/bSize)*(Ka+Y)*Kz\n",
    "\n",
    "    if LR==0:\n",
    "        return 0.1\n",
    "    return 1/LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training loss: 0.0 Validation loss: 0.24517226219177246\n",
      "Epoch: 1 Training loss: 0.0 Validation loss: 0.23169979453086853\n",
      "Epoch: 2 Training loss: 0.0 Validation loss: 0.22587649524211884\n",
      "Epoch: 3 Training loss: 0.0 Validation loss: 0.20880471169948578\n",
      "Epoch: 4 Training loss: 0.0 Validation loss: 0.16629497706890106\n",
      "Epoch: 5 Training loss: 0.0 Validation loss: 0.15121948719024658\n",
      "Epoch: 6 Training loss: 0.0 Validation loss: 0.13927170634269714\n",
      "Epoch: 7 Training loss: 0.0 Validation loss: 0.13508354127407074\n",
      "Epoch: 8 Training loss: 0.0 Validation loss: 0.13291971385478973\n",
      "Epoch: 9 Training loss: 0.0 Validation loss: 0.12844650447368622\n",
      "Epoch: 10 Training loss: 0.0 Validation loss: 0.12080609798431396\n",
      "Epoch: 11 Training loss: 0.0 Validation loss: 0.12135693430900574\n",
      "Epoch: 12 Training loss: 0.0 Validation loss: 0.11627184599637985\n",
      "Epoch: 13 Training loss: 0.0 Validation loss: 0.11195747554302216\n",
      "Epoch: 14 Training loss: 0.0 Validation loss: 0.11115211248397827\n",
      "Epoch: 15 Training loss: 0.0 Validation loss: 0.11135158687829971\n",
      "Epoch: 16 Training loss: 0.0 Validation loss: 0.10956413298845291\n",
      "Epoch: 17 Training loss: 0.0 Validation loss: 0.10934693366289139\n",
      "Epoch: 18 Training loss: 0.0 Validation loss: 0.10884541273117065\n",
      "Epoch: 19 Training loss: 0.0 Validation loss: 0.10713035613298416\n",
      "Epoch: 20 Training loss: 0.0 Validation loss: 0.10543421655893326\n",
      "Epoch: 21 Training loss: 0.0 Validation loss: 0.10394886881113052\n",
      "Epoch: 22 Training loss: 0.0 Validation loss: 0.10213232040405273\n",
      "Epoch: 23 Training loss: 0.0 Validation loss: 0.10235501080751419\n",
      "Epoch: 24 Training loss: 0.0 Validation loss: 0.10271871834993362\n",
      "Epoch: 25 Training loss: 0.0 Validation loss: 0.10295379161834717\n",
      "Epoch: 26 Training loss: 0.0 Validation loss: 0.10107412934303284\n",
      "Epoch: 27 Training loss: 0.0 Validation loss: 0.09938796609640121\n",
      "Epoch: 28 Training loss: 0.0 Validation loss: 0.0984274297952652\n",
      "Epoch: 29 Training loss: 0.0 Validation loss: 0.09776981920003891\n",
      "Epoch: 30 Training loss: 0.0 Validation loss: 0.09780818223953247\n",
      "Epoch: 31 Training loss: 0.0 Validation loss: 0.09466416388750076\n",
      "Epoch: 32 Training loss: 0.0 Validation loss: 0.09407059103250504\n",
      "Epoch: 33 Training loss: 0.0 Validation loss: 0.09539620578289032\n",
      "Epoch: 34 Training loss: 0.0 Validation loss: 0.09288858622312546\n",
      "Epoch: 35 Training loss: 0.0 Validation loss: 0.09615559130907059\n",
      "Epoch: 36 Training loss: 0.0 Validation loss: 0.09344936907291412\n",
      "Epoch: 37 Training loss: 0.0 Validation loss: 0.09213492274284363\n",
      "Epoch: 38 Training loss: 0.0 Validation loss: 0.09341460466384888\n",
      "Epoch: 39 Training loss: 0.0 Validation loss: 0.09155204892158508\n",
      "Epoch: 40 Training loss: 0.0 Validation loss: 0.08981914073228836\n",
      "Epoch: 41 Training loss: 0.0 Validation loss: 0.08900532871484756\n",
      "Epoch: 42 Training loss: 0.0 Validation loss: 0.08887699246406555\n",
      "Epoch: 43 Training loss: 0.0 Validation loss: 0.08840925246477127\n",
      "Epoch: 44 Training loss: 0.0 Validation loss: 0.08895537257194519\n",
      "Epoch: 45 Training loss: 0.0 Validation loss: 0.0862993523478508\n",
      "Epoch: 46 Training loss: 0.0 Validation loss: 0.08638571947813034\n",
      "Epoch: 47 Training loss: 0.0 Validation loss: 0.08694443851709366\n",
      "Epoch: 48 Training loss: 0.0 Validation loss: 0.08835304528474808\n",
      "Epoch: 49 Training loss: 0.0 Validation loss: 0.08684585988521576\n",
      "Epoch: 50 Training loss: 0.0 Validation loss: 0.08639904856681824\n",
      "Epoch: 51 Training loss: 0.0 Validation loss: 0.08697476983070374\n",
      "Epoch: 52 Training loss: 0.0 Validation loss: 0.08653946220874786\n",
      "Epoch: 53 Training loss: 0.0 Validation loss: 0.08544284850358963\n",
      "Epoch: 54 Training loss: 0.0 Validation loss: 0.0852341279387474\n",
      "Epoch: 55 Training loss: 0.0 Validation loss: 0.08497785031795502\n",
      "Epoch: 56 Training loss: 0.0 Validation loss: 0.08608664572238922\n",
      "Epoch: 57 Training loss: 0.0 Validation loss: 0.08570382744073868\n",
      "Epoch: 58 Training loss: 0.0 Validation loss: 0.08668192476034164\n",
      "Epoch: 59 Training loss: 0.0 Validation loss: 0.08659519255161285\n",
      "Epoch: 60 Training loss: 0.0 Validation loss: 0.086038738489151\n",
      "Epoch: 61 Training loss: 0.0 Validation loss: 0.0857851579785347\n",
      "Epoch: 62 Training loss: 0.0 Validation loss: 0.08527891337871552\n",
      "Epoch: 63 Training loss: 0.0 Validation loss: 0.08494299650192261\n",
      "Epoch: 64 Training loss: 0.0 Validation loss: 0.08520764857530594\n",
      "Epoch: 65 Training loss: 0.0 Validation loss: 0.08395759016275406\n",
      "Epoch: 66 Training loss: 0.0 Validation loss: 0.08412829041481018\n",
      "Epoch: 67 Training loss: 0.0 Validation loss: 0.0842878520488739\n",
      "Epoch: 68 Training loss: 0.0 Validation loss: 0.08382759243249893\n",
      "Epoch: 69 Training loss: 0.0 Validation loss: 0.08425351232290268\n",
      "Epoch: 70 Training loss: 0.0 Validation loss: 0.0840727835893631\n",
      "Epoch: 71 Training loss: 0.0 Validation loss: 0.0838681235909462\n",
      "Epoch: 72 Training loss: 0.0 Validation loss: 0.08437598496675491\n",
      "Epoch: 73 Training loss: 0.0 Validation loss: 0.08382148295640945\n",
      "Epoch: 74 Training loss: 0.0 Validation loss: 0.08417891710996628\n",
      "Epoch: 75 Training loss: 0.0 Validation loss: 0.08465318381786346\n",
      "Epoch: 76 Training loss: 0.0 Validation loss: 0.08398749679327011\n",
      "Epoch: 77 Training loss: 0.0 Validation loss: 0.0840492770075798\n",
      "Epoch: 78 Training loss: 0.0 Validation loss: 0.08383212238550186\n",
      "Epoch: 79 Training loss: 0.0 Validation loss: 0.08414408564567566\n",
      "Epoch: 80 Training loss: 0.0 Validation loss: 0.08404052257537842\n",
      "Epoch: 81 Training loss: 0.0 Validation loss: 0.08359652757644653\n",
      "Epoch: 82 Training loss: 0.0 Validation loss: 0.0835661068558693\n",
      "Epoch: 83 Training loss: 0.0 Validation loss: 0.08821168541908264\n",
      "Epoch: 84 Training loss: 0.0 Validation loss: 0.08356652408838272\n",
      "Epoch: 85 Training loss: 0.0 Validation loss: 0.08355162292718887\n",
      "Epoch: 86 Training loss: 0.0 Validation loss: 0.08397529274225235\n",
      "Epoch: 87 Training loss: 0.0 Validation loss: 0.08381575345993042\n",
      "Epoch: 88 Training loss: 0.0 Validation loss: 0.08413077145814896\n",
      "Epoch: 89 Training loss: 0.0 Validation loss: 0.08350353688001633\n",
      "Epoch: 90 Training loss: 0.0 Validation loss: 0.08337974548339844\n",
      "Epoch: 91 Training loss: 0.0 Validation loss: 0.08346621692180634\n",
      "Epoch: 92 Training loss: 0.0 Validation loss: 0.08278607577085495\n",
      "Epoch: 93 Training loss: 0.0 Validation loss: 0.08260855823755264\n",
      "Epoch: 94 Training loss: 0.0 Validation loss: 0.08217792212963104\n",
      "Epoch: 95 Training loss: 0.0 Validation loss: 0.08195966482162476\n",
      "Epoch: 96 Training loss: 0.0 Validation loss: 0.08233346045017242\n",
      "Epoch: 97 Training loss: 0.0 Validation loss: 1.486473560333252\n",
      "Epoch: 98 Training loss: 0.0 Validation loss: 0.0813608169555664\n",
      "Epoch: 99 Training loss: 0.0 Validation loss: 0.08117830008268356\n"
     ]
    }
   ],
   "source": [
    "# LBFGS, LC\n",
    "trainLBFGS(model_LBFGS_LC, optimizer_LBFGS_LC, criterion1, tau,  N_EPOCHS, lossList_LBFGS_LC, valList_LBFGS_LC, \"LC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training loss: 0.0 Validation loss: 0.2387649118900299\n",
      "Epoch: 1 Training loss: 0.0 Validation loss: 0.225356325507164\n",
      "Epoch: 2 Training loss: 0.0 Validation loss: 0.21755817532539368\n",
      "Epoch: 3 Training loss: 0.0 Validation loss: 0.20018866658210754\n",
      "Epoch: 4 Training loss: 0.0 Validation loss: 0.17192187905311584\n",
      "Epoch: 5 Training loss: 0.0 Validation loss: 0.15544241666793823\n",
      "Epoch: 6 Training loss: 0.0 Validation loss: 0.14712779223918915\n",
      "Epoch: 7 Training loss: 0.0 Validation loss: 0.13813161849975586\n",
      "Epoch: 8 Training loss: 0.0 Validation loss: 0.13789208233356476\n",
      "Epoch: 9 Training loss: 0.0 Validation loss: 0.133472740650177\n",
      "Epoch: 10 Training loss: 0.0 Validation loss: 0.12467052042484283\n",
      "Epoch: 11 Training loss: 0.0 Validation loss: 0.12344399094581604\n",
      "Epoch: 12 Training loss: 0.0 Validation loss: 0.11826372146606445\n",
      "Epoch: 13 Training loss: 0.0 Validation loss: 0.11252106726169586\n",
      "Epoch: 14 Training loss: 0.0 Validation loss: 0.1122872456908226\n",
      "Epoch: 15 Training loss: 0.0 Validation loss: 0.1095455139875412\n",
      "Epoch: 16 Training loss: 0.0 Validation loss: 0.10277754813432693\n",
      "Epoch: 17 Training loss: 0.0 Validation loss: 0.10199502855539322\n",
      "Epoch: 18 Training loss: 0.0 Validation loss: 0.10172171890735626\n",
      "Epoch: 19 Training loss: 0.0 Validation loss: 0.09888540953397751\n",
      "Epoch: 20 Training loss: 0.0 Validation loss: 0.10019399970769882\n",
      "Epoch: 21 Training loss: 0.0 Validation loss: 0.09885354340076447\n",
      "Epoch: 22 Training loss: 0.0 Validation loss: 0.09778682887554169\n",
      "Epoch: 23 Training loss: 0.0 Validation loss: 0.0919269546866417\n",
      "Epoch: 24 Training loss: 0.0 Validation loss: 0.09180402010679245\n",
      "Epoch: 25 Training loss: 0.0 Validation loss: 0.09114488214254379\n",
      "Epoch: 26 Training loss: 0.0 Validation loss: 0.09136340767145157\n",
      "Epoch: 27 Training loss: 0.0 Validation loss: 0.09204055368900299\n",
      "Epoch: 28 Training loss: 0.0 Validation loss: 0.09619946777820587\n",
      "Epoch: 29 Training loss: 0.0 Validation loss: 0.09209475666284561\n",
      "Epoch: 30 Training loss: 0.0 Validation loss: 0.09200983494520187\n",
      "Epoch: 31 Training loss: 0.0 Validation loss: 0.09171315282583237\n",
      "Epoch: 32 Training loss: 0.0 Validation loss: 0.09063751995563507\n",
      "Epoch: 33 Training loss: 0.0 Validation loss: 0.08919720351696014\n",
      "Epoch: 34 Training loss: 0.0 Validation loss: 0.08828245848417282\n",
      "Epoch: 35 Training loss: 0.0 Validation loss: 0.09982968121767044\n",
      "Epoch: 36 Training loss: 0.0 Validation loss: 0.08874189853668213\n",
      "Epoch: 37 Training loss: 0.0 Validation loss: 0.08886394649744034\n",
      "Epoch: 38 Training loss: 0.0 Validation loss: 0.08889193087816238\n",
      "Epoch: 39 Training loss: 0.0 Validation loss: 0.08914382010698318\n",
      "Epoch: 40 Training loss: 0.0 Validation loss: 0.08938382565975189\n",
      "Epoch: 41 Training loss: 0.0 Validation loss: 0.08945699781179428\n",
      "Epoch: 42 Training loss: 0.0 Validation loss: 0.0885828360915184\n",
      "Epoch: 43 Training loss: 0.0 Validation loss: 0.08852005004882812\n",
      "Epoch: 44 Training loss: 0.0 Validation loss: 0.08684451878070831\n",
      "Epoch: 45 Training loss: 0.0 Validation loss: 0.08646977692842484\n",
      "Epoch: 46 Training loss: 0.0 Validation loss: 0.08656574040651321\n",
      "Epoch: 47 Training loss: 0.0 Validation loss: 0.08756076544523239\n",
      "Epoch: 48 Training loss: 0.0 Validation loss: 0.08798684924840927\n",
      "Epoch: 49 Training loss: 0.0 Validation loss: 0.08802315592765808\n",
      "Epoch: 50 Training loss: 0.0 Validation loss: 0.0886436179280281\n",
      "Epoch: 51 Training loss: 0.0 Validation loss: 0.08830058574676514\n",
      "Epoch: 52 Training loss: 0.0 Validation loss: 0.08834421634674072\n",
      "Epoch: 53 Training loss: 0.0 Validation loss: 0.09048614650964737\n",
      "Epoch: 54 Training loss: 0.0 Validation loss: 0.08904754370450974\n",
      "Epoch: 55 Training loss: 0.0 Validation loss: 0.08843127638101578\n",
      "Epoch: 56 Training loss: 0.0 Validation loss: 0.08840810507535934\n",
      "Epoch: 57 Training loss: 0.0 Validation loss: 0.08754505962133408\n",
      "Epoch: 58 Training loss: 0.0 Validation loss: 0.08670605719089508\n",
      "Epoch: 59 Training loss: 0.0 Validation loss: 0.08647981286048889\n",
      "Epoch: 60 Training loss: 0.0 Validation loss: 0.08585663884878159\n",
      "Epoch: 61 Training loss: 0.0 Validation loss: 0.08613336831331253\n",
      "Epoch: 62 Training loss: 0.0 Validation loss: 0.08578047901391983\n",
      "Epoch: 63 Training loss: 0.0 Validation loss: 0.08628236502408981\n",
      "Epoch: 64 Training loss: 0.0 Validation loss: 0.08638318628072739\n",
      "Epoch: 65 Training loss: 0.0 Validation loss: 0.08724232017993927\n",
      "Epoch: 66 Training loss: 0.0 Validation loss: 0.08665992319583893\n",
      "Epoch: 67 Training loss: 0.0 Validation loss: 0.08621716499328613\n",
      "Epoch: 68 Training loss: 0.0 Validation loss: 0.0871059000492096\n",
      "Epoch: 69 Training loss: 0.0 Validation loss: 0.08775032311677933\n",
      "Epoch: 70 Training loss: 0.0 Validation loss: 0.08807972073554993\n",
      "Epoch: 71 Training loss: 0.0 Validation loss: 0.08736059814691544\n",
      "Epoch: 72 Training loss: 0.0 Validation loss: 0.0876508504152298\n",
      "Epoch: 73 Training loss: 0.0 Validation loss: 0.0870121493935585\n",
      "Epoch: 74 Training loss: 0.0 Validation loss: 0.08690211921930313\n",
      "Epoch: 75 Training loss: 0.0 Validation loss: 0.08603224158287048\n",
      "Epoch: 76 Training loss: 0.0 Validation loss: 0.08620484918355942\n",
      "Epoch: 77 Training loss: 0.0 Validation loss: 0.08661704510450363\n",
      "Epoch: 78 Training loss: 0.0 Validation loss: 0.08622808009386063\n",
      "Epoch: 79 Training loss: 0.0 Validation loss: 0.0907215029001236\n",
      "Epoch: 80 Training loss: 0.0 Validation loss: 0.08650572597980499\n",
      "Epoch: 81 Training loss: 0.0 Validation loss: 0.08698834478855133\n",
      "Epoch: 82 Training loss: 0.0 Validation loss: 0.08724106103181839\n",
      "Epoch: 83 Training loss: 0.0 Validation loss: 0.0873958170413971\n",
      "Epoch: 84 Training loss: 0.0 Validation loss: 0.08705846965312958\n",
      "Epoch: 85 Training loss: 0.0 Validation loss: 0.0867261216044426\n",
      "Epoch: 86 Training loss: 0.0 Validation loss: 0.08649860322475433\n",
      "Epoch: 87 Training loss: 0.0 Validation loss: 0.08665300160646439\n",
      "Epoch: 88 Training loss: 0.0 Validation loss: 0.08619891107082367\n",
      "Epoch: 89 Training loss: 0.0 Validation loss: 0.0871165469288826\n",
      "Epoch: 90 Training loss: 0.0 Validation loss: 0.08606548607349396\n",
      "Epoch: 91 Training loss: 0.0 Validation loss: 0.08583717793226242\n",
      "Epoch: 92 Training loss: 0.0 Validation loss: 0.08576956391334534\n",
      "Epoch: 93 Training loss: 0.0 Validation loss: 0.08511091768741608\n",
      "Epoch: 94 Training loss: 0.0 Validation loss: 0.08575186878442764\n",
      "Epoch: 95 Training loss: 0.0 Validation loss: 0.08467395603656769\n",
      "Epoch: 96 Training loss: 0.0 Validation loss: 0.08562786877155304\n",
      "Epoch: 97 Training loss: 0.0 Validation loss: 0.0855075940489769\n",
      "Epoch: 98 Training loss: 0.0 Validation loss: 0.08449810743331909\n",
      "Epoch: 99 Training loss: 0.0 Validation loss: 0.08516694605350494\n"
     ]
    }
   ],
   "source": [
    "# LBFGS, MSE\n",
    "trainLBFGS(model_LBFGS_MSE, optimizer_LBFGS_MSE, criterion2, tau,  N_EPOCHS, lossList_LBFGS_MSE, valList_LBFGS_MSE, \"MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training loss: 0.9241100549697876 Validation loss: 5.381256103515625\n",
      "Epoch: 1 Training loss: 5.262235641479492 Validation loss: 17.399717330932617\n",
      "Epoch: 2 Training loss: 17.405353546142578 Validation loss: 15.532523155212402\n",
      "Epoch: 3 Training loss: 15.537973403930664 Validation loss: 3.683785915374756\n",
      "Epoch: 4 Training loss: 3.0896637439727783 Validation loss: 10.440313339233398\n",
      "Epoch: 5 Training loss: 9.475266456604004 Validation loss: 9.42457103729248\n",
      "Epoch: 6 Training loss: 8.425718307495117 Validation loss: 11.574254989624023\n",
      "Epoch: 7 Training loss: 10.809696197509766 Validation loss: 5.48992395401001\n",
      "Epoch: 8 Training loss: 5.159193515777588 Validation loss: 4.207498550415039\n",
      "Epoch: 9 Training loss: 3.3881947994232178 Validation loss: 4.553592681884766\n",
      "Epoch: 10 Training loss: 4.051962852478027 Validation loss: 2.412257194519043\n",
      "Epoch: 11 Training loss: 1.9787932634353638 Validation loss: 3.820408344268799\n",
      "Epoch: 12 Training loss: 3.679368257522583 Validation loss: 2.762477159500122\n",
      "Epoch: 13 Training loss: 2.5015902519226074 Validation loss: 2.3914542198181152\n",
      "Epoch: 14 Training loss: 2.1191186904907227 Validation loss: 2.664344310760498\n",
      "Epoch: 15 Training loss: 2.430776596069336 Validation loss: 1.6595994234085083\n",
      "Epoch: 16 Training loss: 1.462465763092041 Validation loss: 2.7977375984191895\n",
      "Epoch: 17 Training loss: 2.5163886547088623 Validation loss: 1.357396125793457\n",
      "Epoch: 18 Training loss: 1.266139268875122 Validation loss: 2.3628108501434326\n",
      "Epoch: 19 Training loss: 2.2764296531677246 Validation loss: 1.6494404077529907\n",
      "Epoch: 20 Training loss: 1.614643931388855 Validation loss: 2.34432315826416\n",
      "Epoch: 21 Training loss: 2.113781690597534 Validation loss: 2.475330114364624\n",
      "Epoch: 22 Training loss: 2.1682331562042236 Validation loss: 1.422072172164917\n",
      "Epoch: 23 Training loss: 1.4531947374343872 Validation loss: 1.8975507020950317\n",
      "Epoch: 24 Training loss: 1.8601441383361816 Validation loss: 1.229560375213623\n",
      "Epoch: 25 Training loss: 1.2284067869186401 Validation loss: 1.7930711507797241\n",
      "Epoch: 26 Training loss: 1.8141789436340332 Validation loss: 1.3341240882873535\n",
      "Epoch: 27 Training loss: 1.3309353590011597 Validation loss: 1.5027356147766113\n",
      "Epoch: 28 Training loss: 1.423323631286621 Validation loss: 1.3338005542755127\n",
      "Epoch: 29 Training loss: 1.3679107427597046 Validation loss: 1.467633843421936\n",
      "Epoch: 30 Training loss: 1.5049947500228882 Validation loss: 1.5600091218948364\n",
      "Epoch: 31 Training loss: 1.4681345224380493 Validation loss: 1.1218324899673462\n",
      "Epoch: 32 Training loss: 1.1671254634857178 Validation loss: 1.3994319438934326\n",
      "Epoch: 33 Training loss: 1.4134385585784912 Validation loss: 1.1878637075424194\n",
      "Epoch: 34 Training loss: 1.2913099527359009 Validation loss: 1.4949952363967896\n",
      "Epoch: 35 Training loss: 1.4318532943725586 Validation loss: 1.3116588592529297\n",
      "Epoch: 36 Training loss: 1.342215657234192 Validation loss: 1.275565266609192\n",
      "Epoch: 37 Training loss: 1.2808696031570435 Validation loss: 1.4652291536331177\n",
      "Epoch: 38 Training loss: 1.3768177032470703 Validation loss: 1.1124441623687744\n",
      "Epoch: 39 Training loss: 1.1292189359664917 Validation loss: 1.2675644159317017\n",
      "Epoch: 40 Training loss: 1.2887794971466064 Validation loss: 0.9850499033927917\n",
      "Epoch: 41 Training loss: 1.0735180377960205 Validation loss: 1.0925498008728027\n",
      "Epoch: 42 Training loss: 1.1782711744308472 Validation loss: 1.2065677642822266\n",
      "Epoch: 43 Training loss: 1.1652202606201172 Validation loss: 0.9886109232902527\n",
      "Epoch: 44 Training loss: 0.9901756644248962 Validation loss: 1.3407998085021973\n",
      "Epoch: 45 Training loss: 1.3080708980560303 Validation loss: 1.0336618423461914\n",
      "Epoch: 46 Training loss: 1.0841217041015625 Validation loss: 1.4018259048461914\n",
      "Epoch: 47 Training loss: 1.3873611688613892 Validation loss: 1.0521682500839233\n",
      "Epoch: 48 Training loss: 1.1157796382904053 Validation loss: 1.254953145980835\n",
      "Epoch: 49 Training loss: 1.2784438133239746 Validation loss: 1.1617902517318726\n",
      "Epoch: 50 Training loss: 1.1558464765548706 Validation loss: 0.8824601173400879\n",
      "Epoch: 51 Training loss: 0.8753087520599365 Validation loss: 0.9899212121963501\n",
      "Epoch: 52 Training loss: 1.0158103704452515 Validation loss: 1.013642430305481\n",
      "Epoch: 53 Training loss: 1.0057069063186646 Validation loss: 0.8781546354293823\n",
      "Epoch: 54 Training loss: 0.9245041608810425 Validation loss: 0.9862669110298157\n",
      "Epoch: 55 Training loss: 0.9993169903755188 Validation loss: 0.8870567083358765\n",
      "Epoch: 56 Training loss: 0.9102460741996765 Validation loss: 1.0793033838272095\n",
      "Epoch: 57 Training loss: 1.0811265707015991 Validation loss: 0.893928050994873\n",
      "Epoch: 58 Training loss: 0.9353362917900085 Validation loss: 1.2080445289611816\n",
      "Epoch: 59 Training loss: 1.0931850671768188 Validation loss: 1.0166246891021729\n",
      "Epoch: 60 Training loss: 1.0027515888214111 Validation loss: 0.9956963658332825\n",
      "Epoch: 61 Training loss: 0.986790657043457 Validation loss: 1.0497292280197144\n",
      "Epoch: 62 Training loss: 1.0291141271591187 Validation loss: 0.9821943044662476\n",
      "Epoch: 63 Training loss: 0.9906814694404602 Validation loss: 0.9329159259796143\n",
      "Epoch: 64 Training loss: 0.9115529656410217 Validation loss: 0.840663731098175\n",
      "Epoch: 65 Training loss: 0.8823311924934387 Validation loss: 0.8899779319763184\n",
      "Epoch: 66 Training loss: 0.9300026893615723 Validation loss: 0.9552667737007141\n",
      "Epoch: 67 Training loss: 0.9109105467796326 Validation loss: 0.6734082698822021\n",
      "Epoch: 68 Training loss: 0.7915762066841125 Validation loss: 1.0557621717453003\n",
      "Epoch: 69 Training loss: 0.98561030626297 Validation loss: 0.7274229526519775\n",
      "Epoch: 70 Training loss: 0.8078485727310181 Validation loss: 1.420823097229004\n",
      "Epoch: 71 Training loss: 1.3008415699005127 Validation loss: 1.045841932296753\n",
      "Epoch: 72 Training loss: 0.9445522427558899 Validation loss: 1.0192692279815674\n",
      "Epoch: 73 Training loss: 0.9771609902381897 Validation loss: 1.1915204524993896\n",
      "Epoch: 74 Training loss: 1.185813069343567 Validation loss: 0.8988069295883179\n",
      "Epoch: 75 Training loss: 0.9232032299041748 Validation loss: 0.9046604037284851\n",
      "Epoch: 76 Training loss: 0.880955159664154 Validation loss: 0.7220563888549805\n",
      "Epoch: 77 Training loss: 0.7441290020942688 Validation loss: 0.7440822720527649\n",
      "Epoch: 78 Training loss: 0.7688371539115906 Validation loss: 1.0527350902557373\n",
      "Epoch: 79 Training loss: 1.018139362335205 Validation loss: 0.7872328162193298\n",
      "Epoch: 80 Training loss: 0.7648420929908752 Validation loss: 0.8887799978256226\n",
      "Epoch: 81 Training loss: 0.8314145803451538 Validation loss: 0.59193354845047\n",
      "Epoch: 82 Training loss: 0.6931519508361816 Validation loss: 1.2496185302734375\n",
      "Epoch: 83 Training loss: 1.1383469104766846 Validation loss: 0.9366579055786133\n",
      "Epoch: 84 Training loss: 0.8676474094390869 Validation loss: 1.109848141670227\n",
      "Epoch: 85 Training loss: 0.9550279378890991 Validation loss: 1.0596004724502563\n",
      "Epoch: 86 Training loss: 0.9644218683242798 Validation loss: 0.855559766292572\n",
      "Epoch: 87 Training loss: 0.7959547638893127 Validation loss: 0.9991913437843323\n",
      "Epoch: 88 Training loss: 0.9744941592216492 Validation loss: 0.8763974905014038\n",
      "Epoch: 89 Training loss: 0.8820016384124756 Validation loss: 0.7216210961341858\n",
      "Epoch: 90 Training loss: 0.6934030652046204 Validation loss: 0.9181287884712219\n",
      "Epoch: 91 Training loss: 0.8183183670043945 Validation loss: 0.8004390597343445\n",
      "Epoch: 92 Training loss: 0.765623927116394 Validation loss: 1.0972803831100464\n",
      "Epoch: 93 Training loss: 1.0798636674880981 Validation loss: 0.8660824298858643\n",
      "Epoch: 94 Training loss: 0.8116413950920105 Validation loss: 0.940878689289093\n",
      "Epoch: 95 Training loss: 0.8607852458953857 Validation loss: 0.8717814087867737\n",
      "Epoch: 96 Training loss: 0.8344332575798035 Validation loss: 0.9888871908187866\n",
      "Epoch: 97 Training loss: 0.9680465459823608 Validation loss: 0.8132885694503784\n",
      "Epoch: 98 Training loss: 0.7753220796585083 Validation loss: 0.8986571431159973\n",
      "Epoch: 99 Training loss: 0.822679877281189 Validation loss: 0.8515828251838684\n"
     ]
    }
   ],
   "source": [
    "# CLR, L1:\n",
    "trainConstantLR(model_CLR_L1, optimizer_CLR_L1, criterion3, tau,  N_EPOCHS, lossList_CLR_L1, valList_CLR_L1, \"L1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training loss: 1.0041735172271729 Validation loss: 4.681914806365967\n",
      "Epoch: 1 Training loss: 21.427425384521484 Validation loss: 18.400632858276367\n",
      "Epoch: 2 Training loss: 339.50445556640625 Validation loss: 4.075799465179443\n",
      "Epoch: 3 Training loss: 17.054285049438477 Validation loss: 11.05278205871582\n",
      "Epoch: 4 Training loss: 123.93434143066406 Validation loss: 13.311402320861816\n",
      "Epoch: 5 Training loss: 176.67462158203125 Validation loss: 7.093291282653809\n",
      "Epoch: 6 Training loss: 50.385772705078125 Validation loss: 2.11114764213562\n",
      "Epoch: 7 Training loss: 4.937455177307129 Validation loss: 8.935091972351074\n",
      "Epoch: 8 Training loss: 80.37435150146484 Validation loss: 10.003596305847168\n",
      "Epoch: 9 Training loss: 100.85337829589844 Validation loss: 5.941281795501709\n",
      "Epoch: 10 Training loss: 35.8873291015625 Validation loss: 0.6416414380073547\n",
      "Epoch: 11 Training loss: 0.8866572380065918 Validation loss: 5.926745891571045\n",
      "Epoch: 12 Training loss: 34.652889251708984 Validation loss: 7.86367654800415\n",
      "Epoch: 13 Training loss: 61.56867218017578 Validation loss: 6.060233116149902\n",
      "Epoch: 14 Training loss: 36.90711975097656 Validation loss: 1.8939424753189087\n",
      "Epoch: 15 Training loss: 3.992173910140991 Validation loss: 2.9108726978302\n",
      "Epoch: 16 Training loss: 9.109396934509277 Validation loss: 5.716068267822266\n",
      "Epoch: 17 Training loss: 34.14955139160156 Validation loss: 5.655829429626465\n",
      "Epoch: 18 Training loss: 33.04170608520508 Validation loss: 3.1115081310272217\n",
      "Epoch: 19 Training loss: 10.531431198120117 Validation loss: 1.0857096910476685\n",
      "Epoch: 20 Training loss: 1.6930497884750366 Validation loss: 3.8601746559143066\n",
      "Epoch: 21 Training loss: 14.916486740112305 Validation loss: 4.898211479187012\n",
      "Epoch: 22 Training loss: 24.431806564331055 Validation loss: 3.6644978523254395\n",
      "Epoch: 23 Training loss: 13.904380798339844 Validation loss: 1.1765581369400024\n",
      "Epoch: 24 Training loss: 1.8821688890457153 Validation loss: 2.228468656539917\n",
      "Epoch: 25 Training loss: 5.535956859588623 Validation loss: 3.7407491207122803\n",
      "Epoch: 26 Training loss: 15.032962799072266 Validation loss: 3.4249446392059326\n",
      "Epoch: 27 Training loss: 12.367253303527832 Validation loss: 1.688029170036316\n",
      "Epoch: 28 Training loss: 3.492074966430664 Validation loss: 1.3311166763305664\n",
      "Epoch: 29 Training loss: 2.347853660583496 Validation loss: 2.8044824600219727\n",
      "Epoch: 30 Training loss: 8.229424476623535 Validation loss: 3.013033390045166\n",
      "Epoch: 31 Training loss: 9.564737319946289 Validation loss: 1.8898444175720215\n",
      "Epoch: 32 Training loss: 4.073063850402832 Validation loss: 0.9434686899185181\n",
      "Epoch: 33 Training loss: 1.4720690250396729 Validation loss: 2.017270803451538\n",
      "Epoch: 34 Training loss: 4.6699137687683105 Validation loss: 2.41955304145813\n",
      "Epoch: 35 Training loss: 6.598231315612793 Validation loss: 1.7066844701766968\n",
      "Epoch: 36 Training loss: 3.560797691345215 Validation loss: 0.859220027923584\n",
      "Epoch: 37 Training loss: 1.3121113777160645 Validation loss: 1.5985726118087769\n",
      "Epoch: 38 Training loss: 2.9818685054779053 Validation loss: 2.0267980098724365\n",
      "Epoch: 39 Training loss: 4.646103858947754 Validation loss: 1.5459208488464355\n",
      "Epoch: 40 Training loss: 2.861416816711426 Validation loss: 0.8513704538345337\n",
      "Epoch: 41 Training loss: 1.2594759464263916 Validation loss: 1.249647855758667\n",
      "Epoch: 42 Training loss: 2.1547696590423584 Validation loss: 1.5516103506088257\n",
      "Epoch: 43 Training loss: 3.0366923809051514 Validation loss: 1.2084897756576538\n",
      "Epoch: 44 Training loss: 2.104506015777588 Validation loss: 0.7869722843170166\n",
      "Epoch: 45 Training loss: 1.1713154315948486 Validation loss: 1.1216896772384644\n",
      "Epoch: 46 Training loss: 1.7153728008270264 Validation loss: 1.3419864177703857\n",
      "Epoch: 47 Training loss: 2.154951810836792 Validation loss: 1.0717204809188843\n",
      "Epoch: 48 Training loss: 1.7415051460266113 Validation loss: 0.737787663936615\n",
      "Epoch: 49 Training loss: 1.088494896888733 Validation loss: 0.9827044010162354\n",
      "Epoch: 50 Training loss: 1.5553271770477295 Validation loss: 1.091447114944458\n",
      "Epoch: 51 Training loss: 1.7640150785446167 Validation loss: 0.8295751214027405\n",
      "Epoch: 52 Training loss: 1.2401612997055054 Validation loss: 0.7132321000099182\n",
      "Epoch: 53 Training loss: 0.9980198740959167 Validation loss: 0.9360105395317078\n",
      "Epoch: 54 Training loss: 1.3974730968475342 Validation loss: 0.9312667846679688\n",
      "Epoch: 55 Training loss: 1.2677754163742065 Validation loss: 0.7262681126594543\n",
      "Epoch: 56 Training loss: 1.088016152381897 Validation loss: 0.6646566987037659\n",
      "Epoch: 57 Training loss: 1.0410658121109009 Validation loss: 0.7703642845153809\n",
      "Epoch: 58 Training loss: 1.20710027217865 Validation loss: 0.7147631645202637\n",
      "Epoch: 59 Training loss: 1.131719946861267 Validation loss: 0.6132887601852417\n",
      "Epoch: 60 Training loss: 0.9103659391403198 Validation loss: 0.6572221517562866\n",
      "Epoch: 61 Training loss: 0.9104486107826233 Validation loss: 0.6626161336898804\n",
      "Epoch: 62 Training loss: 0.8968403339385986 Validation loss: 0.618236780166626\n",
      "Epoch: 63 Training loss: 0.8574512004852295 Validation loss: 0.5781104564666748\n",
      "Epoch: 64 Training loss: 0.85484778881073 Validation loss: 0.5631640553474426\n",
      "Epoch: 65 Training loss: 0.7761049866676331 Validation loss: 0.5445073843002319\n",
      "Epoch: 66 Training loss: 0.900972843170166 Validation loss: 0.5335612893104553\n",
      "Epoch: 67 Training loss: 0.7680227756500244 Validation loss: 0.5753583908081055\n",
      "Epoch: 68 Training loss: 0.8090217709541321 Validation loss: 0.5768065452575684\n",
      "Epoch: 69 Training loss: 0.7680714130401611 Validation loss: 0.5444427728652954\n",
      "Epoch: 70 Training loss: 0.7825990915298462 Validation loss: 0.5156250596046448\n",
      "Epoch: 71 Training loss: 0.7348744869232178 Validation loss: 0.5132381916046143\n",
      "Epoch: 72 Training loss: 0.730644166469574 Validation loss: 0.5067681074142456\n",
      "Epoch: 73 Training loss: 0.7205781936645508 Validation loss: 0.4985072612762451\n",
      "Epoch: 74 Training loss: 0.7204740047454834 Validation loss: 0.5005698800086975\n",
      "Epoch: 75 Training loss: 0.7255937457084656 Validation loss: 0.5078347325325012\n",
      "Epoch: 76 Training loss: 0.736116349697113 Validation loss: 0.49944689869880676\n",
      "Epoch: 77 Training loss: 0.7031050324440002 Validation loss: 0.47918546199798584\n",
      "Epoch: 78 Training loss: 0.6907990574836731 Validation loss: 0.45935580134391785\n",
      "Epoch: 79 Training loss: 0.6929707527160645 Validation loss: 0.4397737681865692\n",
      "Epoch: 80 Training loss: 0.6403272151947021 Validation loss: 0.46116402745246887\n",
      "Epoch: 81 Training loss: 0.6978744864463806 Validation loss: 0.470526784658432\n",
      "Epoch: 82 Training loss: 0.6805075407028198 Validation loss: 0.4473745822906494\n",
      "Epoch: 83 Training loss: 0.6683123111724854 Validation loss: 0.4484364986419678\n",
      "Epoch: 84 Training loss: 0.6384407877922058 Validation loss: 0.4468357563018799\n",
      "Epoch: 85 Training loss: 0.6210808157920837 Validation loss: 0.42927831411361694\n",
      "Epoch: 86 Training loss: 0.670287549495697 Validation loss: 0.42981457710266113\n",
      "Epoch: 87 Training loss: 0.6460956931114197 Validation loss: 0.44005531072616577\n",
      "Epoch: 88 Training loss: 0.6491276025772095 Validation loss: 0.43886876106262207\n",
      "Epoch: 89 Training loss: 0.6175112724304199 Validation loss: 0.4475941061973572\n",
      "Epoch: 90 Training loss: 0.5977961421012878 Validation loss: 0.44307801127433777\n",
      "Epoch: 91 Training loss: 0.6381063461303711 Validation loss: 0.4199887812137604\n",
      "Epoch: 92 Training loss: 0.631169855594635 Validation loss: 0.4165753126144409\n",
      "Epoch: 93 Training loss: 0.5382729768753052 Validation loss: 0.4265008568763733\n",
      "Epoch: 94 Training loss: 0.6253987550735474 Validation loss: 0.4241103231906891\n",
      "Epoch: 95 Training loss: 0.5894052386283875 Validation loss: 0.4207562804222107\n",
      "Epoch: 96 Training loss: 0.5902702808380127 Validation loss: 0.43253472447395325\n",
      "Epoch: 97 Training loss: 0.6256009936332703 Validation loss: 0.42646273970603943\n",
      "Epoch: 98 Training loss: 0.6151453256607056 Validation loss: 0.4084567725658417\n",
      "Epoch: 99 Training loss: 0.6419058442115784 Validation loss: 0.4134857952594757\n"
     ]
    }
   ],
   "source": [
    "# CLR, MSE\n",
    "trainConstantLR(model_CLR_MSE, optimizer_CLR_MSE, criterion2, tau,  N_EPOCHS, lossList_CLR_MSE, valList_CLR_MSE, \"MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training loss: 0.22023895382881165 Validation loss: 4.124521255493164\n",
      "Epoch: 1 Training loss: 1.6675747632980347 Validation loss: 18.8847599029541\n",
      "Epoch: 2 Training loss: 9.180298805236816 Validation loss: 16.228050231933594\n",
      "Epoch: 3 Training loss: 7.701342582702637 Validation loss: 5.474429607391357\n",
      "Epoch: 4 Training loss: 2.010469675064087 Validation loss: 5.632829189300537\n",
      "Epoch: 5 Training loss: 2.3408122062683105 Validation loss: 6.815260887145996\n",
      "Epoch: 6 Training loss: 2.5679399967193604 Validation loss: 4.681781768798828\n",
      "Epoch: 7 Training loss: 1.655900478363037 Validation loss: 3.3665854930877686\n",
      "Epoch: 8 Training loss: 1.0798392295837402 Validation loss: 2.6489226818084717\n",
      "Epoch: 9 Training loss: 0.8724318742752075 Validation loss: 1.820152997970581\n",
      "Epoch: 10 Training loss: 0.5694820880889893 Validation loss: 2.2488622665405273\n",
      "Epoch: 11 Training loss: 0.7259959578514099 Validation loss: 0.7660080194473267\n",
      "Epoch: 12 Training loss: 0.212260901927948 Validation loss: 1.6783241033554077\n",
      "Epoch: 13 Training loss: 0.5188215374946594 Validation loss: 1.4961094856262207\n",
      "Epoch: 14 Training loss: 0.4406644403934479 Validation loss: 0.8441765904426575\n",
      "Epoch: 15 Training loss: 0.23204201459884644 Validation loss: 1.4431232213974\n",
      "Epoch: 16 Training loss: 0.4136289358139038 Validation loss: 0.9769073128700256\n",
      "Epoch: 17 Training loss: 0.26031672954559326 Validation loss: 1.1796941757202148\n",
      "Epoch: 18 Training loss: 0.3295220732688904 Validation loss: 1.0749857425689697\n",
      "Epoch: 19 Training loss: 0.29446035623550415 Validation loss: 0.6108454465866089\n",
      "Epoch: 20 Training loss: 0.17755284905433655 Validation loss: 1.2650134563446045\n",
      "Epoch: 21 Training loss: 0.3700177073478699 Validation loss: 0.930968165397644\n",
      "Epoch: 22 Training loss: 0.25634878873825073 Validation loss: 0.3988458812236786\n",
      "Epoch: 23 Training loss: 0.1585996001958847 Validation loss: 0.9779952168464661\n",
      "Epoch: 24 Training loss: 0.2623046636581421 Validation loss: 1.0377771854400635\n",
      "Epoch: 25 Training loss: 0.2884705066680908 Validation loss: 0.6639605164527893\n",
      "Epoch: 26 Training loss: 0.18104685842990875 Validation loss: 0.6523007750511169\n",
      "Epoch: 27 Training loss: 0.18629471957683563 Validation loss: 0.7456393241882324\n",
      "Epoch: 28 Training loss: 0.20158281922340393 Validation loss: 0.7991253137588501\n",
      "Epoch: 29 Training loss: 0.22017651796340942 Validation loss: 0.8801601529121399\n",
      "Epoch: 30 Training loss: 0.24469991028308868 Validation loss: 0.9841379523277283\n",
      "Epoch: 31 Training loss: 0.2842986583709717 Validation loss: 0.7105411887168884\n",
      "Epoch: 32 Training loss: 0.19696056842803955 Validation loss: 0.34035807847976685\n",
      "Epoch: 33 Training loss: 0.12829215824604034 Validation loss: 0.6928766369819641\n",
      "Epoch: 34 Training loss: 0.17319715023040771 Validation loss: 0.8561016917228699\n",
      "Epoch: 35 Training loss: 0.2413051724433899 Validation loss: 0.7747945785522461\n",
      "Epoch: 36 Training loss: 0.21183140575885773 Validation loss: 0.6570450663566589\n",
      "Epoch: 37 Training loss: 0.16944430768489838 Validation loss: 0.6115015745162964\n",
      "Epoch: 38 Training loss: 0.16979053616523743 Validation loss: 0.721184253692627\n",
      "Epoch: 39 Training loss: 0.1808224618434906 Validation loss: 0.6428946852684021\n",
      "Epoch: 40 Training loss: 0.1678263396024704 Validation loss: 0.419475257396698\n",
      "Epoch: 41 Training loss: 0.12310756742954254 Validation loss: 0.4305872321128845\n",
      "Epoch: 42 Training loss: 0.13337218761444092 Validation loss: 0.6875126957893372\n",
      "Epoch: 43 Training loss: 0.17013467848300934 Validation loss: 0.9448258280754089\n",
      "Epoch: 44 Training loss: 0.24551045894622803 Validation loss: 0.9316760897636414\n",
      "Epoch: 45 Training loss: 0.23508580029010773 Validation loss: 0.6843897104263306\n",
      "Epoch: 46 Training loss: 0.1793927699327469 Validation loss: 0.5446974635124207\n",
      "Epoch: 47 Training loss: 0.14199790358543396 Validation loss: 0.6941187977790833\n",
      "Epoch: 48 Training loss: 0.18179792165756226 Validation loss: 0.9598928093910217\n",
      "Epoch: 49 Training loss: 0.23340019583702087 Validation loss: 0.9201081395149231\n",
      "Epoch: 50 Training loss: 0.23705296218395233 Validation loss: 0.7147008776664734\n",
      "Epoch: 51 Training loss: 0.1758423000574112 Validation loss: 0.7089626789093018\n",
      "Epoch: 52 Training loss: 0.17878898978233337 Validation loss: 0.6438817381858826\n",
      "Epoch: 53 Training loss: 0.15168805420398712 Validation loss: 0.5678170919418335\n",
      "Epoch: 54 Training loss: 0.1343812346458435 Validation loss: 0.6649051904678345\n",
      "Epoch: 55 Training loss: 0.16495756804943085 Validation loss: 0.7552012801170349\n",
      "Epoch: 56 Training loss: 0.16699422895908356 Validation loss: 0.6149557828903198\n",
      "Epoch: 57 Training loss: 0.14506636559963226 Validation loss: 0.4877842664718628\n",
      "Epoch: 58 Training loss: 0.11120280623435974 Validation loss: 0.41291359066963196\n",
      "Epoch: 59 Training loss: 0.0938025712966919 Validation loss: 0.5303519368171692\n",
      "Epoch: 60 Training loss: 0.12034458667039871 Validation loss: 0.8181290626525879\n",
      "Epoch: 61 Training loss: 0.18003512918949127 Validation loss: 0.9766398072242737\n",
      "Epoch: 62 Training loss: 0.23197335004806519 Validation loss: 0.8066325783729553\n",
      "Epoch: 63 Training loss: 0.16591128706932068 Validation loss: 0.6763297319412231\n",
      "Epoch: 64 Training loss: 0.14803089201450348 Validation loss: 0.7468475103378296\n",
      "Epoch: 65 Training loss: 0.1574413925409317 Validation loss: 0.7231030464172363\n",
      "Epoch: 66 Training loss: 0.15390771627426147 Validation loss: 0.7408321499824524\n",
      "Epoch: 67 Training loss: 0.1547187715768814 Validation loss: 0.8666276335716248\n",
      "Epoch: 68 Training loss: 0.19383081793785095 Validation loss: 1.1681921482086182\n",
      "Epoch: 69 Training loss: 0.2886523902416229 Validation loss: 0.908878743648529\n",
      "Epoch: 70 Training loss: 0.20630960166454315 Validation loss: 0.7183977961540222\n",
      "Epoch: 71 Training loss: 0.14786206185817719 Validation loss: 0.6928889751434326\n",
      "Epoch: 72 Training loss: 0.14076271653175354 Validation loss: 0.7304942011833191\n",
      "Epoch: 73 Training loss: 0.14388945698738098 Validation loss: 0.7580249905586243\n",
      "Epoch: 74 Training loss: 0.1600516140460968 Validation loss: 0.9468441605567932\n",
      "Epoch: 75 Training loss: 0.2142813503742218 Validation loss: 1.0771745443344116\n",
      "Epoch: 76 Training loss: 0.2623269855976105 Validation loss: 0.9830633997917175\n",
      "Epoch: 77 Training loss: 0.23084188997745514 Validation loss: 0.774560272693634\n",
      "Epoch: 78 Training loss: 0.16183821856975555 Validation loss: 0.6642752885818481\n",
      "Epoch: 79 Training loss: 0.129897341132164 Validation loss: 0.7146164178848267\n",
      "Epoch: 80 Training loss: 0.14412210881710052 Validation loss: 0.8613635301589966\n",
      "Epoch: 81 Training loss: 0.18010921776294708 Validation loss: 0.8402140140533447\n",
      "Epoch: 82 Training loss: 0.17383655905723572 Validation loss: 0.7728409171104431\n",
      "Epoch: 83 Training loss: 0.15922395884990692 Validation loss: 0.8508207201957703\n",
      "Epoch: 84 Training loss: 0.17622631788253784 Validation loss: 0.8216001391410828\n",
      "Epoch: 85 Training loss: 0.1673060655593872 Validation loss: 0.6682864427566528\n",
      "Epoch: 86 Training loss: 0.1267588585615158 Validation loss: 0.6152227520942688\n",
      "Epoch: 87 Training loss: 0.1118181049823761 Validation loss: 0.8306077122688293\n",
      "Epoch: 88 Training loss: 0.17020373046398163 Validation loss: 0.9615311622619629\n",
      "Epoch: 89 Training loss: 0.20798002183437347 Validation loss: 0.6963018178939819\n",
      "Epoch: 90 Training loss: 0.131016343832016 Validation loss: 0.651786208152771\n",
      "Epoch: 91 Training loss: 0.1218518614768982 Validation loss: 0.8209317922592163\n",
      "Epoch: 92 Training loss: 0.16407710313796997 Validation loss: 0.8288088440895081\n",
      "Epoch: 93 Training loss: 0.16021494567394257 Validation loss: 0.8735209107398987\n",
      "Epoch: 94 Training loss: 0.18169967830181122 Validation loss: 1.027284860610962\n",
      "Epoch: 95 Training loss: 0.2283221334218979 Validation loss: 0.7151948809623718\n",
      "Epoch: 96 Training loss: 0.13727274537086487 Validation loss: 0.6534528732299805\n",
      "Epoch: 97 Training loss: 0.1135181114077568 Validation loss: 0.7902340292930603\n",
      "Epoch: 98 Training loss: 0.15113425254821777 Validation loss: 0.8265563249588013\n",
      "Epoch: 99 Training loss: 0.15260303020477295 Validation loss: 0.640915036201477\n"
     ]
    }
   ],
   "source": [
    "# CLR, LC\n",
    "trainConstantLR(model_CLR_LC, optimizer_CLR_LC, criterion1, tau,  N_EPOCHS, lossList_CLR_LC, valList_CLR_LC, \"LC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb729145790>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.plot(range(100), valList_LBFGS_LC, label=\"Log-Cosh-LBFGS\")\n",
    "# plt.plot(range(100), valList_LBFGS_MSE, label=\"MSE-LBFGS\")\n",
    "# plt.plot(range(100), valList_CLR_MSE, label=\"CLR-MSE_adam\")\n",
    "# plt.plot(range(100), valList_CLR_L1, label=\"CLR-L1_adam\")\n",
    "# plt.plot(range(100), valList_CLR_LC, label=\"CLR-LC_adam\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "argv": [
    "/home/aryamanj/miniconda3/bin/python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (ipykernel)",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": {
    "debugger": true
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "name": "UCIregression.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
